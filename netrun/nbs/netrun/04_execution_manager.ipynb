{
  "cells": [
    {
      "cell_type": "code",
      "source": "#|default_exp execution_manager",
      "metadata": {},
      "id": "cell0",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|hide\nfrom nblite import nbl_export; nbl_export();",
      "metadata": {},
      "id": "cell1",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# ExecutionManager\n\nThe execution manager layer deals with the execution of functions inside the pools.\n\nIt uses a round",
      "metadata": {},
      "id": "cell2"
    },
    {
      "cell_type": "code",
      "source": "#|export\nfrom contextlib import contextmanager\nfrom typing import Any, Type\nfrom collections.abc import Callable, Awaitable\nimport datetime\nimport builtins\nfrom netrun._iutils import get_timestamp_utc\nfrom datetime import datetime\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass\nimport importlib\nimport uuid\nimport pickle\nimport random\n\nfrom netrun.rpc.base import RPCChannel\nfrom netrun.pool.thread import ThreadPool\nfrom netrun.pool.multiprocess import MultiprocessPool\nfrom netrun.pool.aio import SingleWorkerPool\nfrom netrun.pool.remote import RemotePoolClient",
      "metadata": {},
      "id": "cell3",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## `_override_builtins_print`\n\nOverrides print in a function. Used to keep the print statements in each function execution tracked and separate.",
      "metadata": {},
      "id": "cell4"
    },
    {
      "cell_type": "code",
      "source": "#|exporti\n@contextmanager\ndef _override_builtins_print(func, new_print):\n    g = func.__globals__\n    old = g.get(\"print\", builtins.print)\n    g[\"print\"] = new_print\n    try:\n        yield\n    finally:\n        # restore whatever was there before\n        if old is builtins.print and \"print\" in g:\n            # keep func globals clean if it didn't define print originally\n            del g[\"print\"]\n        else:\n            g[\"print\"] = old",
      "metadata": {},
      "id": "cell5",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Function runners\n\nHelpers for the execution manager to run functions.",
      "metadata": {},
      "id": "cell6"
    },
    {
      "cell_type": "code",
      "source": "#|exporti\nasync def _async_func_runner(\n    channel: RPCChannel,\n    func: Callable[..., Awaitable[Any]],\n    send_channel: bool,\n    print_callback: Callable[[str], None],\n    args: tuple,\n    kwargs: dict,\n) -> Any:\n    with _override_builtins_print(func, print_callback):\n        if asyncio.iscoroutinefunction(func):\n            if send_channel:\n                return await func(channel, *args, **kwargs)\n            else:\n                return await func(*args, **kwargs)\n        else:\n            if send_channel:\n                return func(channel, *args, **kwargs)\n            else:\n                return func(*args, **kwargs)",
      "metadata": {},
      "id": "cell7",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|exporti\ndef _func_runner(\n    channel: RPCChannel,\n    func: Callable[..., Any],\n    send_channel: bool,\n    print_callback: Callable[[str], None],\n    args: tuple,\n    kwargs: dict,\n    event_loop: asyncio.AbstractEventLoop,\n) -> Any:\n    with _override_builtins_print(func, print_callback):\n        if asyncio.iscoroutinefunction(func):\n            if send_channel:\n                return event_loop.run_until_complete(func(channel, *args, **kwargs))\n            else:\n                return event_loop.run_until_complete(func(*args, **kwargs))\n        else:\n            if send_channel:\n                return func(channel, *args, **kwargs)\n            else:\n                return func(*args, **kwargs)",
      "metadata": {},
      "id": "cell8",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Helpers",
      "metadata": {},
      "id": "cell9"
    },
    {
      "cell_type": "code",
      "source": "#|exporti\ndef _convert_to_str_if_not_serializable(obj: Any) -> tuple[bool, Any]:\n    \"\"\"Convert an object to string if it's not pickle-serializable.\n\n    Returns:\n        Tuple of (was_converted, result)\n    \"\"\"\n    try:\n        pickle.dumps(obj)\n        return (False, obj)\n    except (pickle.PicklingError, TypeError, AttributeError):\n        return (True, str(obj))",
      "metadata": {},
      "id": "cell10",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Workers\n\nWorker functions for the execution manager's pools",
      "metadata": {},
      "id": "cell11"
    },
    {
      "cell_type": "code",
      "source": "#|exporti\nclass ExecutionManagerProtocolKeys(Enum):\n    RUN = \"exec-manager:run\"\n    \"\"\"\n    Run a function.\n    Args: msg_id, func_import_path_or_key, run_id, send_channel, args, kwargs\n    \"\"\"\n\n    UP_RUN_STARTED = \"exec-manager-up:run-started\"\n    \"\"\"\n    Notification from the worker that a run has been submitted and started. \n    Args: msg_id, timestamp_utc_started\n    \"\"\"\n\n    UP_RUN_RESPONSE = \"exec-manager-up:run-response\"\n    \"\"\"\n    Response to RUN from the worker.\n    Args: msg_id, converted_to_str, _res\n    \"\"\"\n\n    SEND_FUNCTION = \"exec-manager:send-function\"\n    \"\"\"\n    Used to send a function object to the worker, which can then be run using RUN.\n    Args: msg_id, func_key, func\n    \"\"\"\n\n    UP_SEND_FUNCTION_RESPONSE = \"exec-manager-up:send-function-response\"\n    \"\"\"\n    Response to SEND_FUNCTION from the worker, to confirm that the function was received.\n    Args: msg_id\n    \"\"\"\n\n    FLUSH_PRINT_BUFFER = \"exec-manager:flush-print-buffer\"\n    \"\"\"\n    Flushes the print buffer of a given run_id.\n    Args: msg_id, run_id\n    \"\"\"\n\n    UP_FLUSH_PRINT_BUFFER_RESPONSE = \"exec-manager-up:flush-print-buffer-response\"\n    \"\"\"\n    Response to FLUSH_PRINT_BUFFER from the worker, with the flushed buffer.\n    Args: msg_id, run_id, _buffer\n    \"\"\"",
      "metadata": {},
      "id": "cell12",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|exporti\ndef _worker_func(is_in_main_process: bool, channel, worker_id):\n    func_print_buffer: dict[str, list[tuple[datetime, str]]] = {}\n    event_loop = asyncio.new_event_loop()\n    registered_functions: dict[str, Callable[..., Awaitable] | Callable[..., None]] = {}\n\n    while True:\n        key, data = channel.recv()\n        # RUN\n        if key == ExecutionManagerProtocolKeys.RUN.value:\n            msg_id, func_import_path_or_key, run_id, send_channel, args, kwargs = data\n            if func_import_path_or_key in registered_functions:\n                func = registered_functions[func_import_path_or_key]\n            else:\n                module_path, func_name = func_import_path_or_key.rsplit(\".\", 1)\n                module = importlib.import_module(module_path)\n                func = getattr(module, func_name)\n            func_print_buffer[run_id] = []\n            timestamp_utc_started = get_timestamp_utc()\n            channel.send(ExecutionManagerProtocolKeys.UP_RUN_STARTED.value, (msg_id, timestamp_utc_started))\n            res = _func_runner(\n                channel=channel,\n                func=func,\n                send_channel=send_channel,\n                print_callback=lambda s: func_print_buffer[run_id].append((get_timestamp_utc(), s)),\n                args=args,\n                kwargs=kwargs,\n                event_loop=event_loop,\n            )\n            timestamp_utc_completed = get_timestamp_utc()\n            if is_in_main_process:\n                converted_to_str, _res = False, res\n            else:\n                converted_to_str, _res = _convert_to_str_if_not_serializable(res)\n            _buffer = func_print_buffer.pop(run_id)\n            channel.send(ExecutionManagerProtocolKeys.UP_RUN_RESPONSE.value, (msg_id, timestamp_utc_started, timestamp_utc_completed, converted_to_str, _res, _buffer))\n        # SEND_FUNCTION\n        elif key == ExecutionManagerProtocolKeys.SEND_FUNCTION.value:\n            msg_id, func_key, func = data\n            registered_functions[func_key] = func\n            channel.send(ExecutionManagerProtocolKeys.UP_SEND_FUNCTION_RESPONSE.value, (msg_id,))\n        # FLUSH_PRINT_BUFFER\n        elif key == ExecutionManagerProtocolKeys.FLUSH_PRINT_BUFFER.value:\n            msg_id, run_id = data\n            if run_id not in func_print_buffer:\n                raise ValueError(f\"Run ID '{run_id}' not found in print buffer\")\n            _buffer = list(func_print_buffer[run_id])  # Copy the buffer\n            func_print_buffer[run_id].clear()  # Clear but keep tracking\n            channel.send(ExecutionManagerProtocolKeys.UP_FLUSH_PRINT_BUFFER_RESPONSE.value, (msg_id, run_id, _buffer))\n        else:\n            raise ValueError(f\"Unknown execution manager protocol key: '{key}'.\")\n\ndef _thread_worker_func(channel, worker_id):\n    return _worker_func(is_in_main_process=True, channel=channel, worker_id=worker_id)\n\n# If the worker is in a multiprocess pool, then the result needs to be pickleable for it to be sent back without being converted as `str(result)`.\ndef _multiprocess_worker_func(channel, worker_id):\n    return _worker_func(is_in_main_process=False, channel=channel, worker_id=worker_id)",
      "metadata": {},
      "id": "cell13",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|exporti\nasync def _async_worker_func(channel, worker_id):\n    func_print_buffer: dict[str, list[tuple[datetime, str]]] = {}\n    registered_functions: dict[str, Callable[..., Awaitable] | Callable[..., None]] = {}\n\n    while True:\n        key, data = await channel.recv()\n        # RUN\n        if key == ExecutionManagerProtocolKeys.RUN.value:\n            msg_id, func_import_path_or_key, run_id, send_channel, args, kwargs = data\n            if func_import_path_or_key in registered_functions:\n                func = registered_functions[func_import_path_or_key]\n            else:\n                module_path, func_name = func_import_path_or_key.rsplit(\".\", 1)\n                module = importlib.import_module(module_path)\n                func = getattr(module, func_name)\n            func_print_buffer[run_id] = []\n            timestamp_utc_started = get_timestamp_utc()\n            await channel.send(ExecutionManagerProtocolKeys.UP_RUN_STARTED.value, (msg_id, timestamp_utc_started))\n            res = await _async_func_runner(\n                channel=channel,\n                func=func,\n                send_channel=send_channel,\n                print_callback=lambda s: func_print_buffer[run_id].append((get_timestamp_utc(), s)),\n                args=args,\n                kwargs=kwargs,\n            )\n            timestamp_utc_completed = get_timestamp_utc()\n            converted_to_str, _res = False, res\n            _buffer = func_print_buffer.pop(run_id)\n            await channel.send(ExecutionManagerProtocolKeys.UP_RUN_RESPONSE.value, (msg_id, timestamp_utc_started, timestamp_utc_completed, converted_to_str, _res, _buffer))\n        # SEND_FUNCTION\n        elif key == ExecutionManagerProtocolKeys.SEND_FUNCTION.value:\n            msg_id, func_key, func = data\n            registered_functions[func_key] = func\n            await channel.send(ExecutionManagerProtocolKeys.UP_SEND_FUNCTION_RESPONSE.value, (msg_id,))\n        # FLUSH_PRINT_BUFFER\n        elif key == ExecutionManagerProtocolKeys.FLUSH_PRINT_BUFFER.value:\n            msg_id, run_id = data\n            if run_id not in func_print_buffer:\n                raise ValueError(f\"Run ID '{run_id}' not found in print buffer\")\n            _buffer = list(func_print_buffer[run_id])  # Copy the buffer\n            func_print_buffer[run_id].clear()  # Clear but keep tracking\n            await channel.send(ExecutionManagerProtocolKeys.UP_FLUSH_PRINT_BUFFER_RESPONSE.value, (msg_id, run_id, _buffer))\n        else:\n            raise ValueError(f\"Unknown execution manager protocol key: '{key}'.\")",
      "metadata": {},
      "id": "cell14",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## ExecutionManager",
      "metadata": {},
      "id": "cell15"
    },
    {
      "cell_type": "code",
      "source": "#|export\n@dataclass\nclass JobResult:\n    \"\"\"Result of a job execution.\"\"\"\n    timestamp_utc_submitted: datetime\n    timestamp_utc_started: datetime\n    timestamp_utc_completed: datetime\n    func_import_path_or_key: str\n    pool_id: str\n    worker_id: int\n    converted_to_str: bool\n    result: Any\n    print_buffer: list[tuple[datetime, str]]\n    \"\"\"List of (timestamp, text) tuples for each print statement captured during execution.\"\"\"\n\n@dataclass\nclass SubmittedJobInfo:\n    \"\"\"Information about a submitted job.\"\"\"\n    run_id: str\n    timestamp_utc_submitted: datetime\n    timestamp_utc_started: datetime | None\n    func_import_path_or_key: str\n    pool_id: str\n    worker_id: int\n\nclass RunAllocationMethod(Enum):\n    \"\"\"Method for allocating a job to a worker.\"\"\"\n    ROUND_ROBIN = \"round-robin\"\n    RANDOM = \"random\"\n    LEAST_BUSY = \"least-busy\"",
      "metadata": {},
      "id": "cell16",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|export\nPoolType = ThreadPool | MultiprocessPool | SingleWorkerPool | RemotePoolClient\n\nclass ExecutionManager:\n    def __init__(self, pool_configs: dict[Type[PoolType], tuple[str, dict[str, Any]]]):\n        \"\"\"\n        Create an ExecutionManager with the given pool configurations.\n\n        Args:\n            pool_configs: A dictionary mapping pool_id to (pool_type, pool_init_kwargs).\n                pool_type can be \"thread\", \"multiprocess\", \"remote\", or \"main\".\n                pool_init_kwargs are passed to the pool constructor (excluding worker_fn).\n        \"\"\"\n        self._pool_configs = pool_configs\n        self._pools: dict[str, PoolType] = {}\n        self._msg_recv_tasks: dict[str, asyncio.Task] = {}\n        self._msgs: dict[str, dict[str, asyncio.Queue]] = {}\n        self._started = False\n\n        self._worker_jobs: dict[tuple[str, str], list[SubmittedJobInfo]] = {}  # (pool_id, worker_id) -> list of SubmittedJobInfo\n        self._worker_round_robin_lst: list[tuple[str, str]] = []\n\n    async def start(self) -> None:\n        \"\"\"Start all pools and initialize the execution manager.\"\"\"\n        if self._started:\n            raise RuntimeError(\"ExecutionManager is already started.\")\n\n        for pool_id, (pool_type, pool_init_kwargs) in self._pool_configs.items():\n            if 'worker_fn' in pool_init_kwargs:\n                raise ValueError(\"The 'worker_fn' argument should not be specified in the pool config.\")\n\n            if pool_type == ThreadPool:\n                self._pools[pool_id] = ThreadPool(**pool_init_kwargs, worker_fn=_thread_worker_func)\n            elif pool_type == MultiprocessPool:\n                self._pools[pool_id] = MultiprocessPool(**pool_init_kwargs, worker_fn=_multiprocess_worker_func)\n            elif pool_type == RemotePoolClient:\n                self._pools[pool_id] = RemotePoolClient(**pool_init_kwargs)\n            elif pool_type == SingleWorkerPool:\n                self._pools[pool_id] = SingleWorkerPool(**pool_init_kwargs, worker_fn=_async_worker_func)\n            else:\n                raise ValueError(f\"Unknown pool type: '{pool_type}'.\")\n\n            self._msgs[pool_id] = {}\n\n        # Start all pools\n        for pool_id, pool in self._pools.items():\n            await pool.start()\n\n        # Initialize worker jobs tracking for each worker\n        for pool_id, pool in self._pools.items():\n            for worker_id in range(pool.num_workers):\n                self._worker_jobs[(pool_id, worker_id)] = []\n\n        # Start message receiver tasks after pools are started\n        for pool_id in self._pools:\n            self._msg_recv_tasks[pool_id] = asyncio.create_task(self._msg_recv_task_func(pool_id))\n\n        self._started = True\n\n    async def _msg_recv_task_func(self, pool_id: str):\n        pool = self._pools[pool_id]\n        while True:\n            msg = await pool.recv()\n            msg_id = msg.data[0]\n            msg.data = msg.data[1:]\n            await self._msgs[pool_id][msg_id].put(msg)\n\n    async def _send_msg(self, pool_id: str, worker_id: str, key: str, data: Any) -> str:\n        pool = self._pools[pool_id]\n        msg_id = str(uuid.uuid4())\n\n        # Check if the message receiver task for the pool is running, else propagate its exception\n        msg_recv_task = self._msg_recv_tasks.get(pool_id)\n        if msg_recv_task is not None and msg_recv_task.done():\n            exc = msg_recv_task.exception()\n            if exc is not None:\n                raise exc\n\n        await pool.send(\n            worker_id=worker_id,\n            key=key,\n            data=(msg_id, *data),\n        )\n        self._msgs[pool_id][msg_id] = asyncio.Queue()\n\n        return msg_id\n\n    async def _recv_msg(self, pool_id: str, msg_id: str, expect: ExecutionManagerProtocolKeys, close_msg_queue: bool) -> tuple[str, Any]:\n        # Check if the message receiver task for the pool is running, else propagate its exception\n        msg_recv_task = self._msg_recv_tasks.get(pool_id)\n        if msg_recv_task is not None and msg_recv_task.done():\n            exc = msg_recv_task.exception()\n            if exc is not None:\n                raise exc\n\n        msg = await self._msgs[pool_id][msg_id].get()\n        if close_msg_queue:\n            del self._msgs[pool_id][msg_id]\n\n        if msg.key != expect.value:\n            raise ValueError(f\"Expected message key '{expect.value}', got '{msg.key}'.\")\n\n        return msg\n\n    async def run(self, pool_id: str, worker_id: str, func_import_path_or_key: str, send_channel: bool, func_args, func_kwargs) -> JobResult:\n        \"\"\"\n        Run a function in a pool.\n\n        Args:\n            pool_id: The ID of the pool to run the function in.\n            worker_id: The ID of the worker to run the function on.\n            func_import_path_or_key: The import path or key of the function to run (for the latter, use send_function to register the function first)\n            send_channel: Whether to send the worker RPC channel to the function.\n            func_args: The arguments to pass to the function.\n            func_kwargs: The keyword arguments to pass to the function.\n\n        Returns:\n            The result of the function.\n        \"\"\"\n        pool = self._pools[pool_id]\n\n        run_id = str(uuid.uuid4())\n        timestamp_utc_submitted = get_timestamp_utc()\n        msg_id = await self._send_msg(\n            pool_id=pool_id,\n            worker_id=worker_id,\n            key=ExecutionManagerProtocolKeys.RUN.value,\n            data=(func_import_path_or_key, run_id, send_channel, func_args, func_kwargs),\n        )\n        job_info = SubmittedJobInfo(\n            run_id=run_id,\n            timestamp_utc_submitted=timestamp_utc_submitted,\n            timestamp_utc_started=None,\n            func_import_path_or_key=func_import_path_or_key,\n            pool_id=pool_id,\n            worker_id=worker_id,\n        )\n        self._worker_jobs[(pool_id, worker_id)].append(job_info)\n        if (pool_id, worker_id) in self._worker_round_robin_lst:\n            self._worker_round_robin_lst.remove((pool_id, worker_id))\n        self._worker_round_robin_lst.append((pool_id, worker_id))\n        started_msg = await self._recv_msg(pool_id, msg_id, expect=ExecutionManagerProtocolKeys.UP_RUN_STARTED, close_msg_queue=False)\n        job_info.timestamp_utc_started = started_msg.data[0]  # timestamp_utc_started\n        msg = await self._recv_msg(pool_id, msg_id, expect=ExecutionManagerProtocolKeys.UP_RUN_RESPONSE, close_msg_queue=True)\n        self._worker_jobs[(pool_id, worker_id)].remove(job_info)\n\n        timestamp_utc_started, timestamp_utc_completed, converted_to_str, _res, _print_buffer = msg.data\n        return JobResult(\n            timestamp_utc_submitted=job_info.timestamp_utc_submitted,\n            timestamp_utc_started=job_info.timestamp_utc_started,\n            timestamp_utc_completed=timestamp_utc_completed,\n            func_import_path_or_key=job_info.func_import_path_or_key,\n            pool_id=job_info.pool_id,\n            worker_id=job_info.worker_id,\n            converted_to_str=converted_to_str,\n            result=_res,\n            print_buffer=_print_buffer,\n        )\n\n    async def run_allocate(\n        self,\n        pool_worker_ids: list[str | tuple[str, str]],\n        allocation_method: RunAllocationMethod,\n        func_import_path_or_key: str,\n        send_channel: bool,\n        func_args,\n        func_kwargs,\n    ) -> JobResult:\n        worker_ids: list[tuple[str, int]] = []\n        # Convert pool_worker_ids to a list of (pool_id, worker_id) tuples\n        for _id in pool_worker_ids:\n            if isinstance(_id, str):\n                pool = self._pools[_id]\n                for worker_id in range(pool.num_workers):\n                    worker_ids.append((_id, worker_id))\n            else:\n                pool_id, worker_id = _id\n                worker_ids.append((pool_id, worker_id))\n\n        if not worker_ids:\n            raise ValueError(\"No workers available for allocation.\")\n\n        # Select worker based on allocation method\n        if allocation_method == RunAllocationMethod.ROUND_ROBIN:\n            round_robin_lst = [p for p in self._worker_round_robin_lst if p in worker_ids]\n            not_in_round_robin_lst = [p for p in worker_ids if p not in round_robin_lst]\n            if not_in_round_robin_lst:\n                pool_id, worker_id = not_in_round_robin_lst[0]\n            else:\n                pool_id, worker_id = round_robin_lst[0]\n\n        elif allocation_method == RunAllocationMethod.RANDOM:\n            pool_id, worker_id = random.choice(worker_ids)\n\n        elif allocation_method == RunAllocationMethod.LEAST_BUSY:\n            # Choose the worker with the fewest active jobs (jobs that have been submitted\n            # but not yet completed). A worker with no recorded jobs is preferred.\n            def active_job_count(pool_worker: tuple[str, str]) -> int:\n                key = pool_worker\n                jobs = self._worker_jobs.get(key, [])\n                # Active = submitted but not yet completed; here we treat presence in the\n                # list as active, and completion will remove the job from this structure.\n                return len(jobs)\n\n            pool_id, worker_id = min(worker_ids, key=active_job_count)\n\n        else:\n            raise ValueError(f\"Unknown allocation method: '{allocation_method}'.\")\n\n        return await self.run(\n            pool_id=pool_id,\n            worker_id=worker_id,\n            func_import_path_or_key=func_import_path_or_key,\n            send_channel=send_channel,\n            func_args=func_args,\n            func_kwargs=func_kwargs,\n        )\n\n    async def send_function(self, pool_id: str, worker_id: str, func_key: str, func: Callable[..., Any]|Callable[..., Awaitable[Any]]) -> None:\n        \"\"\"\n        Send a function to a worker in a pool, such that it can be run using the given key using `ExecutionManager.run`.\n\n        Args:\n            pool_id: The ID of the pool to send the function to.\n            worker_id: The ID of the worker to send the function to.\n            func_key: The key of the function to send. \n            func: The function to send.\n        \"\"\"\n        # If the pool is a multiprocess pool or remote pool, the function needs to be pickleable.\n        pool = self._pools[pool_id]\n        if isinstance(pool, (MultiprocessPool, RemotePoolClient)):\n            try:\n                pickle.dumps(func)\n            except pickle.PicklingError:\n                raise ValueError(f\"Function {func} (key = '{func_key}') is not pickleable. Cannot send to worker in pool '{pool_id}'.\")\n\n        msg_id = await self._send_msg(\n            pool_id=pool_id,\n            worker_id=worker_id,\n            key=ExecutionManagerProtocolKeys.SEND_FUNCTION.value,\n            data=(func_key, func),\n        )\n        await self._recv_msg(pool_id, msg_id, expect=ExecutionManagerProtocolKeys.UP_SEND_FUNCTION_RESPONSE, close_msg_queue=True)\n\n    async def send_function_to_pool(self, pool_id: str, func_key: str, func: Callable[..., Any]|Callable[..., Awaitable[Any]]) -> None:\n        \"\"\"\n        Send a function to all workers in a pool.\n\n        Args:\n            pool_id: The ID of the pool to send the function to.\n            func_key: The key of the function to send.\n            func: The function to send.\n        \"\"\"\n        tasks = [asyncio.create_task(self.send_function(pool_id, worker_id, func_key, func)) for worker_id in range(self._pools[pool_id].num_workers)]\n        await asyncio.gather(*tasks)\n\n    async def flush_print_buffer(self, pool_id: str, worker_id: int, run_id: str) -> list[tuple[datetime, str]]:\n        \"\"\"\n        Flush and retrieve the print buffer for a job.\n\n        After calling this, the print buffer for that run_id is cleared in the\n        worker but continues tracking new prints.\n\n        Note: This method can only be called when the worker is not blocked\n        executing a function. Since workers are single-threaded, they cannot\n        process this message while a synchronous function is running. This is\n        useful between jobs or for async functions that yield control.\n\n        Args:\n            pool_id: The ID of the pool where the job is running.\n            worker_id: The ID of the worker where the job is running.\n            run_id: The run ID of the job (from SubmittedJobInfo.run_id).\n\n        Returns:\n            List of (timestamp, text) tuples for each print statement captured.\n\n        Raises:\n            ValueError: If the run_id is not found in the worker's print buffer.\n        \"\"\"\n        msg_id = await self._send_msg(\n            pool_id=pool_id,\n            worker_id=worker_id,\n            key=ExecutionManagerProtocolKeys.FLUSH_PRINT_BUFFER.value,\n            data=(run_id,),\n        )\n        msg = await self._recv_msg(pool_id, msg_id, expect=ExecutionManagerProtocolKeys.UP_FLUSH_PRINT_BUFFER_RESPONSE, close_msg_queue=True)\n        _run_id, _buffer = msg.data\n        return _buffer\n\n    async def close(self):\n        \"\"\"Close the execution manager and all its pools.\"\"\"\n        # Cancel message receiver tasks FIRST to prevent them from trying to\n        # recv from closed pools (which would raise PoolNotStarted)\n        errors = []\n        for task in self._msg_recv_tasks.values():\n            task.cancel()\n        for task in self._msg_recv_tasks.values():\n            try:\n                await task\n            except asyncio.CancelledError:\n                pass\n            except Exception as e:\n                errors.append(e)\n\n        # Now close the pools\n        for pool in self._pools.values():\n            await pool.close()\n\n        self._started = False\n\n        # Propagate any errors from the recv tasks\n        if errors:\n            raise errors[0]\n\n    @property\n    def pools(self) -> list[tuple[str, Type[PoolType]]]:\n        \"\"\"Get list of pool IDs.\"\"\"\n        return [(k, type(v)) for k, v in self._pools.items()]\n\n    def get_num_workers(self, pool_id: str) -> int:\n        \"\"\"Get the number of workers in a pool.\"\"\"\n        return self._pools[pool_id].num_workers\n\n    def get_worker_ids(self, pool_id: str) -> list[str]:\n        \"\"\"Get the list of worker IDs for a pool.\"\"\"\n        return [f\"{pool_id}_{i}\" for i in range(self._pools[pool_id].num_workers)]\n\n    def get_worker_jobs(self, pool_id: str, worker_id: int) -> list[SubmittedJobInfo]:\n        \"\"\"Get the list of currently submitted jobs for a worker.\"\"\"\n        return list(self._worker_jobs.get((pool_id, worker_id), []))\n\n    async def __aenter__(self) -> \"ExecutionManager\":\n        \"\"\"Context manager entry - starts the manager.\"\"\"\n        await self.start()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n        \"\"\"Context manager exit - closes the manager.\"\"\"\n        await self.close()",
      "metadata": {},
      "id": "cell17",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Examples\n\nThe following examples demonstrate how to use the ExecutionManager.",
      "metadata": {},
      "id": "cell18"
    },
    {
      "cell_type": "markdown",
      "source": "### Example 1: Basic usage with ThreadPool\n\nThis example shows the basic workflow of running a function on a worker.",
      "metadata": {},
      "id": "cell19"
    },
    {
      "cell_type": "code",
      "source": "print(\"=\" * 50)\nprint(\"Example: Basic ExecutionManager Usage\")\nprint(\"=\" * 50)\n\ndef example_add(a: int, b: int) -> int:\n    \"\"\"A simple function that adds two numbers.\"\"\"\n    print(f\"Adding {a} + {b}\")\n    return a + b\n\n# Create an ExecutionManager with a thread pool\nmanager = ExecutionManager({\n    \"workers\": (ThreadPool, {\"num_workers\": 2}),\n})\n\nasync with manager:\n    print(f\"Pool IDs: {[pool_id for pool_id, _ in manager.pools]}\")\n    print(f\"Workers in 'workers' pool: {manager.get_num_workers('workers')}\")\n\n    # Send a function to all workers in the pool\n    await manager.send_function_to_pool(\"workers\", \"add\", example_add)\n\n    # Run the function on worker 0\n    result = await manager.run(\n        pool_id=\"workers\",\n        worker_id=0,\n        func_import_path_or_key=\"add\",\n        send_channel=False,\n        func_args=(3, 4),\n        func_kwargs={},\n    )\n\n    print(f\"\\nResult: {result.result}\")\n    print(f\"Submitted at: {result.timestamp_utc_submitted}\")\n    print(f\"Started at: {result.timestamp_utc_started}\")\n    print(f\"Completed at: {result.timestamp_utc_completed}\")\n    print(f\"Was converted to str: {result.converted_to_str}\")\n    print(f\"Print buffer: {result.print_buffer}\")",
      "metadata": {},
      "id": "cell20",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Example: Basic ExecutionManager Usage\n",
            "==================================================\n",
            "Pool IDs: ['workers']\n",
            "Workers in 'workers' pool: 2\n",
            "\n",
            "Result: 7\n",
            "Submitted at: 2026-01-18 23:59:14.422226+00:00\n",
            "Started at: 2026-01-18 23:59:14.422308+00:00\n",
            "Completed at: 2026-01-18 23:59:14.422320+00:00\n",
            "Was converted to str: False\n",
            "Print buffer: [(datetime.datetime(2026, 1, 18, 23, 59, 14, 422318, tzinfo=datetime.timezone.utc), 'Adding 3 + 4')]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Example 2: Running multiple jobs with allocation\n\nThis example shows how to use the allocation methods to distribute work.",
      "metadata": {},
      "id": "cell21"
    },
    {
      "cell_type": "code",
      "source": "def example_multiply(x: int, y: int) -> int:\n    \"\"\"Multiply two numbers.\"\"\"\n    import time\n    time.sleep(0.1)  # Simulate some work\n    print(f\"Multiplying {x} * {y}\")\n    return x * y\n\nprint(\"=\" * 50)\nprint(\"Example: Job Allocation\")\nprint(\"=\" * 50)\n\nmanager = ExecutionManager({\n    \"compute\": (ThreadPool, {\"num_workers\": 3}),\n})\n\nasync with manager:\n    # Send the multiply function to all workers\n    await manager.send_function_to_pool(\"compute\", \"multiply\", example_multiply)\n\n    # Run multiple jobs using round-robin allocation\n    print(\"\\nRunning 6 jobs with ROUND_ROBIN allocation:\")\n    tasks = []\n    for i in range(6):\n        task = asyncio.create_task(\n            manager.run_allocate(\n                pool_worker_ids=[\"compute\"],  # Use all workers in \"compute\" pool\n                allocation_method=RunAllocationMethod.ROUND_ROBIN,\n                func_import_path_or_key=\"multiply\",\n                send_channel=False,\n                func_args=(i, i + 1),\n                func_kwargs={},\n            )\n        )\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks)\n    for i, result in enumerate(results):\n        print(f\"  Job {i}: {result.result} (worker {result.worker_id})\")",
      "metadata": {},
      "id": "cell22",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Example: Job Allocation\n",
            "==================================================\n",
            "\n",
            "Running 6 jobs with ROUND_ROBIN allocation:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Job 0: 0 (worker 0)\n",
            "  Job 1: 2 (worker 0)\n",
            "  Job 2: 6 (worker 0)\n",
            "  Job 3: 12 (worker 0)\n",
            "  Job 4: 20 (worker 0)\n",
            "  Job 5: 30 (worker 0)\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "nblite_source_hash": "297fcf66de2183a30688328a124e7e4f50d587658ae4e9e8293910a0ca382b12"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
