{
  "cells": [
    {
      "cell_type": "code",
      "source": "#|default_exp execution_manager",
      "metadata": {},
      "id": "cell0",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|hide\nfrom nblite import nbl_export; nbl_export();",
      "metadata": {},
      "id": "cell1",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# ExecutionManager\n\nThe execution manager layer deals with the execution of functions inside the pools.\n\nIt uses a round",
      "metadata": {},
      "id": "cell2"
    },
    {
      "cell_type": "code",
      "source": "#|export\nfrom contextlib import contextmanager\nfrom typing import Any\nfrom collections.abc import Callable, Awaitable\nimport datetime\nimport builtins\nfrom netrun._iutils import get_timestamp_utc\nfrom datetime import datetime\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass\nimport importlib\nimport uuid\nimport pickle\nimport random\nimport functools\nimport threading\nimport io\n\nfrom netrun.rpc.base import RPCChannel\nfrom netrun.pool.thread import ThreadPool\nfrom netrun.pool.multiprocess import MultiprocessPool\nfrom netrun.pool.aio import SingleWorkerPool\nfrom netrun.pool.remote import RemotePoolClient",
      "metadata": {},
      "id": "cell3",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Thread-safe print capture\n\nThese helpers provide thread-safe print capture using thread-local storage.\nThe patch is applied to `builtins.print` and uses reference counting to support\nmultiple ExecutionManager instances.",
      "metadata": {},
      "id": "cell4"
    },
    {
      "cell_type": "code",
      "source": "#|exporti\n_print_capture_lock = threading.Lock()\n_print_capture_refcount = 0\n_original_builtins_print = None\n_print_capture_local = threading.local()\n\ndef _capturing_print(*args, **kwargs):\n    \"\"\"Replacement for builtins.print that captures output when callback is set.\"\"\"\n    callback = getattr(_print_capture_local, 'callback', None)\n    if callback is not None:\n        # Capture mode active in this thread - capture to string and call callback\n        output = io.StringIO()\n        _original_builtins_print(*args, **kwargs, file=output)\n        callback(output.getvalue())\n    else:\n        # Normal print\n        _original_builtins_print(*args, **kwargs)\n\ndef _enable_print_capture():\n    \"\"\"Enable print capture. Idempotent - uses reference counting.\"\"\"\n    global _print_capture_refcount, _original_builtins_print\n    with _print_capture_lock:\n        if _print_capture_refcount == 0:\n            _original_builtins_print = builtins.print\n            builtins.print = _capturing_print\n        _print_capture_refcount += 1\n\ndef _disable_print_capture():\n    \"\"\"Disable print capture. Only removes patch when refcount reaches 0.\"\"\"\n    global _print_capture_refcount\n    with _print_capture_lock:\n        _print_capture_refcount -= 1\n        if _print_capture_refcount == 0:\n            builtins.print = _original_builtins_print\n\n@contextmanager\ndef _capture_prints(callback):\n    \"\"\"Context manager to set the print callback for the current thread.\"\"\"\n    _print_capture_local.callback = callback\n    try:\n        yield\n    finally:\n        _print_capture_local.callback = None",
      "metadata": {},
      "id": "cell5",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Function runners\n\nHelpers for the execution manager to run functions.",
      "metadata": {},
      "id": "cell6"
    },
    {
      "cell_type": "code",
      "source": "#|exporti\nasync def _async_func_runner(\n    channel: RPCChannel,\n    func: Callable[..., Awaitable[Any]],\n    send_channel: bool,\n    print_callback: Callable[[str], None] | None,\n    args: tuple,\n    kwargs: dict,\n) -> Any:\n    if print_callback is not None:\n        with _capture_prints(print_callback):\n            if asyncio.iscoroutinefunction(func):\n                if send_channel:\n                    return await func(channel, *args, **kwargs)\n                else:\n                    return await func(*args, **kwargs)\n            else:\n                if send_channel:\n                    return func(channel, *args, **kwargs)\n                else:\n                    return func(*args, **kwargs)\n    else:\n        if asyncio.iscoroutinefunction(func):\n            if send_channel:\n                return await func(channel, *args, **kwargs)\n            else:\n                return await func(*args, **kwargs)\n        else:\n            if send_channel:\n                return func(channel, *args, **kwargs)\n            else:\n                return func(*args, **kwargs)",
      "metadata": {},
      "id": "cell7",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|exporti\ndef _func_runner(\n    channel: RPCChannel,\n    func: Callable[..., Any],\n    send_channel: bool,\n    print_callback: Callable[[str], None] | None,\n    args: tuple,\n    kwargs: dict,\n    event_loop: asyncio.AbstractEventLoop,\n) -> Any:\n    if print_callback is not None:\n        with _capture_prints(print_callback):\n            if asyncio.iscoroutinefunction(func):\n                if send_channel:\n                    return event_loop.run_until_complete(func(channel, *args, **kwargs))\n                else:\n                    return event_loop.run_until_complete(func(*args, **kwargs))\n            else:\n                if send_channel:\n                    return func(channel, *args, **kwargs)\n                else:\n                    return func(*args, **kwargs)\n    else:\n        if asyncio.iscoroutinefunction(func):\n            if send_channel:\n                return event_loop.run_until_complete(func(channel, *args, **kwargs))\n            else:\n                return event_loop.run_until_complete(func(*args, **kwargs))\n        else:\n            if send_channel:\n                return func(channel, *args, **kwargs)\n            else:\n                return func(*args, **kwargs)",
      "metadata": {},
      "id": "cell8",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Helpers",
      "metadata": {},
      "id": "cell9"
    },
    {
      "cell_type": "code",
      "source": "#|exporti\ndef _convert_to_str_if_not_serializable(obj: Any) -> tuple[bool, Any]:\n    \"\"\"Convert an object to string if it's not pickle-serializable.\n\n    Returns:\n        Tuple of (was_converted, result)\n    \"\"\"\n    try:\n        pickle.dumps(obj)\n        return (False, obj)\n    except (pickle.PicklingError, TypeError, AttributeError):\n        return (True, str(obj))",
      "metadata": {},
      "id": "cell10",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Workers\n\nWorker functions for the execution manager's pools",
      "metadata": {},
      "id": "cell11"
    },
    {
      "cell_type": "code",
      "source": "#|exporti\nclass ExecutionManagerProtocolKeys(Enum):\n    RUN = \"exec-manager:run\"\n    \"\"\"\n    Run a function.\n    Args: msg_id, func_import_path_or_key, run_id, send_channel, args, kwargs\n    \"\"\"\n\n    UP_RUN_STARTED = \"exec-manager-up:run-started\"\n    \"\"\"\n    Notification from the worker that a run has been submitted and started. \n    Args: msg_id, timestamp_utc_started\n    \"\"\"\n\n    UP_RUN_RESPONSE = \"exec-manager-up:run-response\"\n    \"\"\"\n    Response to RUN from the worker.\n    Args: msg_id, converted_to_str, _res\n    \"\"\"\n\n    SEND_FUNCTION = \"exec-manager:send-function\"\n    \"\"\"\n    Used to send a function object to the worker, which can then be run using RUN.\n    Args: msg_id, func_key, func\n    \"\"\"\n\n    UP_SEND_FUNCTION_RESPONSE = \"exec-manager-up:send-function-response\"\n    \"\"\"\n    Response to SEND_FUNCTION from the worker, to confirm that the function was received.\n    Args: msg_id\n    \"\"\"\n\n    UP_PRINT_BUFFER = \"exec-manager-up:print-buffer\"\n    \"\"\"\n    Auto-flushed print buffer from the worker during function execution.\n    Sent at regular intervals (print_flush_interval) and before UP_RUN_RESPONSE.\n    Args: msg_id, run_id, _buffer\n    \"\"\"",
      "metadata": {},
      "id": "cell12",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|exporti\ndef _worker_func(is_in_main_process: bool, channel, worker_id, print_flush_interval: float = 0.1, capture_prints: bool = True):\n    \"\"\"Worker function that handles execution manager protocol messages.\n\n    Args:\n        is_in_main_process: If True, results don't need to be serializable.\n        channel: RPC channel for communication.\n        worker_id: ID of this worker.\n        print_flush_interval: Interval in seconds between automatic print buffer flushes.\n        capture_prints: If True, capture print statements and send them back. If False, prints go to stdout normally.\n    \"\"\"\n    # Enable print capture if configured\n    if capture_prints:\n        _enable_print_capture()\n\n    event_loop = asyncio.new_event_loop()\n    registered_functions: dict[str, Callable[..., Awaitable] | Callable[..., None]] = {}\n\n    while True:\n        key, data = channel.recv()\n        # RUN\n        if key == ExecutionManagerProtocolKeys.RUN.value:\n            msg_id, func_import_path_or_key, run_id, send_channel, args, kwargs = data\n            if func_import_path_or_key in registered_functions:\n                func = registered_functions[func_import_path_or_key]\n            else:\n                module_path, func_name = func_import_path_or_key.rsplit(\".\", 1)\n                module = importlib.import_module(module_path)\n                func = getattr(module, func_name)\n\n            # Set up auto-flushing print buffer (only if capture_prints is enabled)\n            print_buffer: list[tuple[datetime, str]] = []\n            last_flush_time = get_timestamp_utc()\n\n            if capture_prints:\n                def print_callback(text: str):\n                    nonlocal last_flush_time\n                    print_buffer.append((get_timestamp_utc(), text))\n                    # Check if we should auto-flush\n                    now = get_timestamp_utc()\n                    elapsed = (now - last_flush_time).total_seconds()\n                    if elapsed >= print_flush_interval and print_buffer:\n                        channel.send(ExecutionManagerProtocolKeys.UP_PRINT_BUFFER.value, (msg_id, run_id, list(print_buffer)))\n                        print_buffer.clear()\n                        last_flush_time = now\n            else:\n                print_callback = None\n\n            timestamp_utc_started = get_timestamp_utc()\n            channel.send(ExecutionManagerProtocolKeys.UP_RUN_STARTED.value, (msg_id, timestamp_utc_started))\n            res = _func_runner(\n                channel=channel,\n                func=func,\n                send_channel=send_channel,\n                print_callback=print_callback,\n                args=args,\n                kwargs=kwargs,\n                event_loop=event_loop,\n            )\n            timestamp_utc_completed = get_timestamp_utc()\n            if is_in_main_process:\n                converted_to_str, _res = False, res\n            else:\n                converted_to_str, _res = _convert_to_str_if_not_serializable(res)\n\n            # Flush any remaining print buffer before sending response\n            if capture_prints and print_buffer:\n                channel.send(ExecutionManagerProtocolKeys.UP_PRINT_BUFFER.value, (msg_id, run_id, list(print_buffer)))\n\n            channel.send(ExecutionManagerProtocolKeys.UP_RUN_RESPONSE.value, (msg_id, timestamp_utc_started, timestamp_utc_completed, converted_to_str, _res))\n        # SEND_FUNCTION\n        elif key == ExecutionManagerProtocolKeys.SEND_FUNCTION.value:\n            msg_id, func_key, func = data\n            registered_functions[func_key] = func\n            channel.send(ExecutionManagerProtocolKeys.UP_SEND_FUNCTION_RESPONSE.value, (msg_id,))\n        else:\n            raise ValueError(f\"Unknown execution manager protocol key: '{key}'.\")\n\ndef _thread_worker_func(channel, worker_id, print_flush_interval: float = 0.1, capture_prints: bool = True):\n    return _worker_func(is_in_main_process=True, channel=channel, worker_id=worker_id, print_flush_interval=print_flush_interval, capture_prints=capture_prints)\n\n# If the worker is in a multiprocess pool, then the result needs to be pickleable for it to be sent back without being converted as `str(result)`.\ndef _multiprocess_worker_func(channel, worker_id, print_flush_interval: float = 0.1, capture_prints: bool = True):\n    return _worker_func(is_in_main_process=False, channel=channel, worker_id=worker_id, print_flush_interval=print_flush_interval, capture_prints=capture_prints)",
      "metadata": {},
      "id": "cell13",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|exporti\nasync def _async_worker_func(channel, worker_id, print_flush_interval: float = 0.1, capture_prints: bool = True):\n    \"\"\"Async worker function that handles execution manager protocol messages.\n\n    Note: For async workers, print buffer is sent all at once at the end of execution\n    since the print callback cannot be async. Interval-based flushing is only\n    supported for sync workers (thread/multiprocess pools).\n\n    Args:\n        channel: Async RPC channel for communication.\n        worker_id: ID of this worker.\n        print_flush_interval: Not used for async workers (kept for API consistency).\n        capture_prints: If True, capture print statements and send them back. If False, prints go to stdout normally.\n    \"\"\"\n    # Enable print capture if configured\n    if capture_prints:\n        _enable_print_capture()\n\n    registered_functions: dict[str, Callable[..., Awaitable] | Callable[..., None]] = {}\n\n    while True:\n        key, data = await channel.recv()\n        # RUN\n        if key == ExecutionManagerProtocolKeys.RUN.value:\n            msg_id, func_import_path_or_key, run_id, send_channel, args, kwargs = data\n            if func_import_path_or_key in registered_functions:\n                func = registered_functions[func_import_path_or_key]\n            else:\n                module_path, func_name = func_import_path_or_key.rsplit(\".\", 1)\n                module = importlib.import_module(module_path)\n                func = getattr(module, func_name)\n\n            # For async workers, we just collect prints and send at the end\n            # (interval-based flushing requires sync channel.send which isn't available)\n            print_buffer: list[tuple[datetime, str]] = []\n\n            if capture_prints:\n                print_callback = lambda s: print_buffer.append((get_timestamp_utc(), s))\n            else:\n                print_callback = None\n\n            timestamp_utc_started = get_timestamp_utc()\n            await channel.send(ExecutionManagerProtocolKeys.UP_RUN_STARTED.value, (msg_id, timestamp_utc_started))\n            res = await _async_func_runner(\n                channel=channel,\n                func=func,\n                send_channel=send_channel,\n                print_callback=print_callback,\n                args=args,\n                kwargs=kwargs,\n            )\n            timestamp_utc_completed = get_timestamp_utc()\n            converted_to_str, _res = False, res\n\n            # Send print buffer before response\n            if capture_prints and print_buffer:\n                await channel.send(ExecutionManagerProtocolKeys.UP_PRINT_BUFFER.value, (msg_id, run_id, list(print_buffer)))\n\n            await channel.send(ExecutionManagerProtocolKeys.UP_RUN_RESPONSE.value, (msg_id, timestamp_utc_started, timestamp_utc_completed, converted_to_str, _res))\n        # SEND_FUNCTION\n        elif key == ExecutionManagerProtocolKeys.SEND_FUNCTION.value:\n            msg_id, func_key, func = data\n            registered_functions[func_key] = func\n            await channel.send(ExecutionManagerProtocolKeys.UP_SEND_FUNCTION_RESPONSE.value, (msg_id,))\n        else:\n            raise ValueError(f\"Unknown execution manager protocol key: '{key}'.\")",
      "metadata": {},
      "id": "cell14",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## ExecutionManager",
      "metadata": {},
      "id": "cell15"
    },
    {
      "cell_type": "code",
      "source": "#|export\n@dataclass\nclass JobResult:\n    \"\"\"Result of a job execution.\"\"\"\n    timestamp_utc_submitted: datetime\n    timestamp_utc_started: datetime\n    timestamp_utc_completed: datetime\n    func_import_path_or_key: str\n    pool_id: str\n    worker_id: int\n    converted_to_str: bool\n    result: Any\n    print_buffer: list[tuple[datetime, str]]\n    \"\"\"List of (timestamp, text) tuples for each print statement captured during execution.\"\"\"\n\n@dataclass\nclass SubmittedJobInfo:\n    \"\"\"Information about a submitted job.\"\"\"\n    run_id: str\n    timestamp_utc_submitted: datetime\n    timestamp_utc_started: datetime | None\n    func_import_path_or_key: str\n    pool_id: str\n    worker_id: int\n\nclass RunAllocationMethod(Enum):\n    \"\"\"Method for allocating a job to a worker.\"\"\"\n    ROUND_ROBIN = \"round-robin\"\n    RANDOM = \"random\"\n    LEAST_BUSY = \"least-busy\"",
      "metadata": {},
      "id": "cell16",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|export\nPoolType = ThreadPool | MultiprocessPool | SingleWorkerPool | RemotePoolClient\n\nclass ExecutionManager:\n    def __init__(self, pool_configs: dict[type[PoolType], tuple[str, dict[str, Any]]]):\n        \"\"\"\n        Create an ExecutionManager with the given pool configurations.\n\n        Args:\n            pool_configs: A dictionary mapping pool_id to (pool_type, pool_init_kwargs).\n                pool_type can be \"thread\", \"multiprocess\", \"remote\", or \"main\".\n                pool_init_kwargs are passed to the pool constructor (excluding worker_fn).\n        \"\"\"\n        self._pool_configs = pool_configs\n        self._pools: dict[str, PoolType] = {}\n        self._msg_recv_tasks: dict[str, asyncio.Task] = {}\n        self._msgs: dict[str, dict[str, asyncio.Queue]] = {}\n        self._started = False\n\n        self._worker_jobs: dict[tuple[str, str], list[SubmittedJobInfo]] = {}  # (pool_id, worker_id) -> list of SubmittedJobInfo\n        self._worker_round_robin_lst: list[tuple[str, str]] = []\n\n    async def start(self) -> None:\n        \"\"\"Start all pools and initialize the execution manager.\"\"\"\n        if self._started:\n            raise RuntimeError(\"ExecutionManager is already started.\")\n\n        # Enable print capture in the main process\n        _enable_print_capture()\n\n        for pool_id, (pool_type, pool_init_kwargs) in self._pool_configs.items():\n            if 'worker_fn' in pool_init_kwargs:\n                raise ValueError(\"The 'worker_fn' argument should not be specified in the pool config.\")\n\n            # Extract print_flush_interval and capture_prints from kwargs\n            pool_kwargs = dict(pool_init_kwargs)\n            print_flush_interval = pool_kwargs.pop('print_flush_interval', 0.1)\n            capture_prints = pool_kwargs.pop('capture_prints', True)\n\n            if pool_type == ThreadPool:\n                worker_fn = functools.partial(_thread_worker_func, print_flush_interval=print_flush_interval, capture_prints=capture_prints)\n                self._pools[pool_id] = ThreadPool(**pool_kwargs, worker_fn=worker_fn)\n            elif pool_type == MultiprocessPool:\n                worker_fn = functools.partial(_multiprocess_worker_func, print_flush_interval=print_flush_interval, capture_prints=capture_prints)\n                self._pools[pool_id] = MultiprocessPool(**pool_kwargs, worker_fn=worker_fn)\n            elif pool_type == RemotePoolClient:\n                self._pools[pool_id] = RemotePoolClient(**pool_kwargs)\n            elif pool_type == SingleWorkerPool:\n                worker_fn = functools.partial(_async_worker_func, print_flush_interval=print_flush_interval, capture_prints=capture_prints)\n                self._pools[pool_id] = SingleWorkerPool(**pool_kwargs, worker_fn=worker_fn)\n            else:\n                raise ValueError(f\"Unknown pool type: '{pool_type}'.\")\n\n            self._msgs[pool_id] = {}\n\n        # Start all pools\n        for pool_id, pool in self._pools.items():\n            await pool.start()\n\n        # Initialize worker jobs tracking for each worker\n        for pool_id, pool in self._pools.items():\n            for worker_id in range(pool.num_workers):\n                self._worker_jobs[(pool_id, worker_id)] = []\n\n        # Start message receiver tasks after pools are started\n        for pool_id in self._pools:\n            self._msg_recv_tasks[pool_id] = asyncio.create_task(self._msg_recv_task_func(pool_id))\n\n        self._started = True\n\n    async def _msg_recv_task_func(self, pool_id: str):\n        pool = self._pools[pool_id]\n        while True:\n            msg = await pool.recv()\n            msg_id = msg.data[0]\n            msg.data = msg.data[1:]\n            await self._msgs[pool_id][msg_id].put(msg)\n\n    async def _send_msg(self, pool_id: str, worker_id: str, key: str, data: Any) -> str:\n        pool = self._pools[pool_id]\n        msg_id = str(uuid.uuid4())\n\n        # Check if the message receiver task for the pool is running, else propagate its exception\n        msg_recv_task = self._msg_recv_tasks.get(pool_id)\n        if msg_recv_task is not None and msg_recv_task.done():\n            exc = msg_recv_task.exception()\n            if exc is not None:\n                raise exc\n\n        # Create the queue BEFORE sending to avoid race condition where response\n        # arrives before the queue is created\n        self._msgs[pool_id][msg_id] = asyncio.Queue()\n\n        await pool.send(\n            worker_id=worker_id,\n            key=key,\n            data=(msg_id, *data),\n        )\n\n        return msg_id\n\n    async def _recv_msg(self, pool_id: str, msg_id: str, expect: ExecutionManagerProtocolKeys, close_msg_queue: bool) -> tuple[str, Any]:\n        # Get the queue for this message and the recv task\n        msg_queue = self._msgs[pool_id][msg_id]\n        msg_recv_task = self._msg_recv_tasks.get(pool_id)\n\n        # Create a task to wait for the message\n        get_task = asyncio.create_task(msg_queue.get())\n\n        # Wait for either the message or the recv task to complete (crash)\n        if msg_recv_task is not None:\n            done, pending = await asyncio.wait(\n                [get_task, msg_recv_task],\n                return_when=asyncio.FIRST_COMPLETED\n            )\n\n            # If the recv task completed (crashed), cancel the get and propagate exception\n            if msg_recv_task in done:\n                get_task.cancel()\n                try:\n                    await get_task\n                except asyncio.CancelledError:\n                    pass\n                if close_msg_queue:\n                    del self._msgs[pool_id][msg_id]\n                exc = msg_recv_task.exception()\n                if exc is not None:\n                    raise exc\n                # Recv task ended without exception - shouldn't happen in normal operation\n                raise RuntimeError(\"Message receiver task ended unexpectedly\")\n\n            # Otherwise, get the message result\n            msg = get_task.result()\n        else:\n            msg = await get_task\n\n        if close_msg_queue:\n            del self._msgs[pool_id][msg_id]\n\n        if msg.key != expect.value:\n            raise ValueError(f\"Expected message key '{expect.value}', got '{msg.key}'.\")\n\n        return msg\n\n    async def run(\n        self,\n        pool_id: str,\n        worker_id: str,\n        func_import_path_or_key: str,\n        send_channel: bool,\n        func_args,\n        func_kwargs,\n        on_print: Callable[[list[tuple[datetime, str]]], None] | None = None,\n    ) -> JobResult:\n        \"\"\"\n        Run a function in a pool.\n\n        Args:\n            pool_id: The ID of the pool to run the function in.\n            worker_id: The ID of the worker to run the function on.\n            func_import_path_or_key: The import path or key of the function to run (for the latter, use send_function to register the function first)\n            send_channel: Whether to send the worker RPC channel to the function.\n            func_args: The arguments to pass to the function.\n            func_kwargs: The keyword arguments to pass to the function.\n            on_print: Optional callback called when print output is received from the worker.\n                Called with a list of (timestamp, text) tuples. Called multiple times during\n                execution as print buffers are flushed (at print_flush_interval).\n\n        Returns:\n            The result of the function.\n        \"\"\"\n        pool = self._pools[pool_id]\n\n        run_id = str(uuid.uuid4())\n        timestamp_utc_submitted = get_timestamp_utc()\n        msg_id = await self._send_msg(\n            pool_id=pool_id,\n            worker_id=worker_id,\n            key=ExecutionManagerProtocolKeys.RUN.value,\n            data=(func_import_path_or_key, run_id, send_channel, func_args, func_kwargs),\n        )\n        job_info = SubmittedJobInfo(\n            run_id=run_id,\n            timestamp_utc_submitted=timestamp_utc_submitted,\n            timestamp_utc_started=None,\n            func_import_path_or_key=func_import_path_or_key,\n            pool_id=pool_id,\n            worker_id=worker_id,\n        )\n        self._worker_jobs[(pool_id, worker_id)].append(job_info)\n        if (pool_id, worker_id) in self._worker_round_robin_lst:\n            self._worker_round_robin_lst.remove((pool_id, worker_id))\n        self._worker_round_robin_lst.append((pool_id, worker_id))\n\n        # Wait for UP_RUN_STARTED\n        started_msg = await self._recv_msg(pool_id, msg_id, expect=ExecutionManagerProtocolKeys.UP_RUN_STARTED, close_msg_queue=False)\n        job_info.timestamp_utc_started = started_msg.data[0]  # timestamp_utc_started\n\n        # Accumulate print buffers and wait for UP_RUN_RESPONSE\n        accumulated_print_buffer: list[tuple[datetime, str]] = []\n        msg_queue = self._msgs[pool_id][msg_id]\n\n        while True:\n            msg_recv_task = self._msg_recv_tasks.get(pool_id)\n\n            # Create a task to wait for the next message\n            get_task = asyncio.create_task(msg_queue.get())\n\n            # Wait for either the message or the recv task to complete (crash)\n            if msg_recv_task is not None:\n                done, pending = await asyncio.wait(\n                    [get_task, msg_recv_task],\n                    return_when=asyncio.FIRST_COMPLETED\n                )\n\n                # If the recv task completed (crashed), cancel the get and propagate exception\n                if msg_recv_task in done:\n                    get_task.cancel()\n                    try:\n                        await get_task\n                    except asyncio.CancelledError:\n                        pass\n                    del self._msgs[pool_id][msg_id]\n                    exc = msg_recv_task.exception()\n                    if exc is not None:\n                        raise exc\n                    raise RuntimeError(\"Message receiver task ended unexpectedly\")\n\n                msg = get_task.result()\n            else:\n                msg = await get_task\n\n            if msg.key == ExecutionManagerProtocolKeys.UP_PRINT_BUFFER.value:\n                # Intermediate print buffer\n                _run_id, _buffer = msg.data\n                accumulated_print_buffer.extend(_buffer)\n                if on_print is not None:\n                    on_print(_buffer)\n            elif msg.key == ExecutionManagerProtocolKeys.UP_RUN_RESPONSE.value:\n                # Job completed - clean up and break\n                del self._msgs[pool_id][msg_id]\n                break\n            else:\n                raise ValueError(f\"Unexpected message key: '{msg.key}'\")\n\n        self._worker_jobs[(pool_id, worker_id)].remove(job_info)\n\n        timestamp_utc_started, timestamp_utc_completed, converted_to_str, _res = msg.data\n        return JobResult(\n            timestamp_utc_submitted=job_info.timestamp_utc_submitted,\n            timestamp_utc_started=job_info.timestamp_utc_started,\n            timestamp_utc_completed=timestamp_utc_completed,\n            func_import_path_or_key=job_info.func_import_path_or_key,\n            pool_id=job_info.pool_id,\n            worker_id=job_info.worker_id,\n            converted_to_str=converted_to_str,\n            result=_res,\n            print_buffer=accumulated_print_buffer,\n        )\n\n    async def run_allocate(\n        self,\n        pool_worker_ids: list[str | tuple[str, str]],\n        allocation_method: RunAllocationMethod,\n        func_import_path_or_key: str,\n        send_channel: bool,\n        func_args,\n        func_kwargs,\n        on_print: Callable[[list[tuple[datetime, str]]], None] | None = None,\n    ) -> JobResult:\n        worker_ids: list[tuple[str, int]] = []\n        # Convert pool_worker_ids to a list of (pool_id, worker_id) tuples\n        for _id in pool_worker_ids:\n            if isinstance(_id, str):\n                pool = self._pools[_id]\n                for worker_id in range(pool.num_workers):\n                    worker_ids.append((_id, worker_id))\n            else:\n                pool_id, worker_id = _id\n                worker_ids.append((pool_id, worker_id))\n\n        if not worker_ids:\n            raise ValueError(\"No workers available for allocation.\")\n\n        # Select worker based on allocation method\n        if allocation_method == RunAllocationMethod.ROUND_ROBIN:\n            round_robin_lst = [p for p in self._worker_round_robin_lst if p in worker_ids]\n            not_in_round_robin_lst = [p for p in worker_ids if p not in round_robin_lst]\n            if not_in_round_robin_lst:\n                pool_id, worker_id = not_in_round_robin_lst[0]\n            else:\n                pool_id, worker_id = round_robin_lst[0]\n\n        elif allocation_method == RunAllocationMethod.RANDOM:\n            pool_id, worker_id = random.choice(worker_ids)\n\n        elif allocation_method == RunAllocationMethod.LEAST_BUSY:\n            # Choose the worker with the fewest active jobs (jobs that have been submitted\n            # but not yet completed). A worker with no recorded jobs is preferred.\n            def active_job_count(pool_worker: tuple[str, str]) -> int:\n                key = pool_worker\n                jobs = self._worker_jobs.get(key, [])\n                # Active = submitted but not yet completed; here we treat presence in the\n                # list as active, and completion will remove the job from this structure.\n                return len(jobs)\n\n            pool_id, worker_id = min(worker_ids, key=active_job_count)\n\n        else:\n            raise ValueError(f\"Unknown allocation method: '{allocation_method}'.\")\n\n        return await self.run(\n            pool_id=pool_id,\n            worker_id=worker_id,\n            func_import_path_or_key=func_import_path_or_key,\n            send_channel=send_channel,\n            func_args=func_args,\n            func_kwargs=func_kwargs,\n            on_print=on_print,\n        )\n\n    async def send_function(self, pool_id: str, worker_id: str, func_key: str, func: Callable[..., Any]|Callable[..., Awaitable[Any]]) -> None:\n        \"\"\"\n        Send a function to a worker in a pool, such that it can be run using the given key using `ExecutionManager.run`.\n\n        Args:\n            pool_id: The ID of the pool to send the function to.\n            worker_id: The ID of the worker to send the function to.\n            func_key: The key of the function to send. \n            func: The function to send.\n        \"\"\"\n        # If the pool is a multiprocess pool or remote pool, the function needs to be pickleable.\n        pool = self._pools[pool_id]\n        if isinstance(pool, (MultiprocessPool, RemotePoolClient)):\n            try:\n                pickle.dumps(func)\n            except pickle.PicklingError:\n                raise ValueError(f\"Function {func} (key = '{func_key}') is not pickleable. Cannot send to worker in pool '{pool_id}'.\")\n\n        msg_id = await self._send_msg(\n            pool_id=pool_id,\n            worker_id=worker_id,\n            key=ExecutionManagerProtocolKeys.SEND_FUNCTION.value,\n            data=(func_key, func),\n        )\n        await self._recv_msg(pool_id, msg_id, expect=ExecutionManagerProtocolKeys.UP_SEND_FUNCTION_RESPONSE, close_msg_queue=True)\n\n    async def send_function_to_pool(self, pool_id: str, func_key: str, func: Callable[..., Any]|Callable[..., Awaitable[Any]]) -> None:\n        \"\"\"\n        Send a function to all workers in a pool.\n\n        Args:\n            pool_id: The ID of the pool to send the function to.\n            func_key: The key of the function to send.\n            func: The function to send.\n        \"\"\"\n        tasks = [asyncio.create_task(self.send_function(pool_id, worker_id, func_key, func)) for worker_id in range(self._pools[pool_id].num_workers)]\n        await asyncio.gather(*tasks)\n\n    async def close(self):\n        \"\"\"Close the execution manager and all its pools.\"\"\"\n        # Cancel message receiver tasks FIRST to prevent them from trying to\n        # recv from closed pools (which would raise PoolNotStarted)\n        errors = []\n        for task in self._msg_recv_tasks.values():\n            task.cancel()\n        for task in self._msg_recv_tasks.values():\n            try:\n                await task\n            except asyncio.CancelledError:\n                pass\n            except Exception as e:\n                errors.append(e)\n\n        # Now close the pools\n        for pool in self._pools.values():\n            await pool.close()\n\n        # Disable print capture in the main process\n        _disable_print_capture()\n\n        self._started = False\n\n        # Propagate any errors from the recv tasks\n        if errors:\n            raise errors[0]\n\n    @property\n    def pools(self) -> list[tuple[str, type[PoolType]]]:\n        \"\"\"Get list of pool IDs.\"\"\"\n        return [(k, type(v)) for k, v in self._pools.items()]\n\n    def get_num_workers(self, pool_id: str) -> int:\n        \"\"\"Get the number of workers in a pool.\"\"\"\n        return self._pools[pool_id].num_workers\n\n    def get_worker_ids(self, pool_id: str) -> list[str]:\n        \"\"\"Get the list of worker IDs for a pool.\"\"\"\n        return [f\"{pool_id}_{i}\" for i in range(self._pools[pool_id].num_workers)]\n\n    def get_worker_jobs(self, pool_id: str, worker_id: int) -> list[SubmittedJobInfo]:\n        \"\"\"Get the list of currently submitted jobs for a worker.\"\"\"\n        return list(self._worker_jobs.get((pool_id, worker_id), []))\n\n    def get_process_ids(self, pool_id: str) -> list[int]:\n        \"\"\"Get all process IDs for a MultiprocessPool.\n\n        Args:\n            pool_id: The ID of the pool.\n\n        Returns:\n            List of process indices (0 to num_processes-1).\n\n        Raises:\n            ValueError: If the pool is not a MultiprocessPool.\n        \"\"\"\n        pool = self._pools[pool_id]\n        if not isinstance(pool, MultiprocessPool):\n            raise ValueError(f\"Pool '{pool_id}' is not a MultiprocessPool (got {type(pool).__name__})\")\n        return list(range(pool.num_processes))\n\n    async def flush_pool_stdout(\n        self, pool_id: str, process_idx: int, timeout: float | None = None\n    ) -> list[tuple[datetime, bool, str]]:\n        \"\"\"Flush and retrieve stdout/stderr buffer from a specific process in a MultiprocessPool.\n\n        Args:\n            pool_id: The ID of the pool.\n            process_idx: Index of the process (0 to num_processes-1).\n            timeout: Maximum time to wait for response in seconds.\n\n        Returns:\n            List of (timestamp, is_stdout, text) tuples.\n            is_stdout is True for stdout, False for stderr.\n\n        Raises:\n            ValueError: If the pool is not a MultiprocessPool.\n            PoolNotStarted: If the pool is not running.\n        \"\"\"\n        pool = self._pools[pool_id]\n        if not isinstance(pool, MultiprocessPool):\n            raise ValueError(f\"Pool '{pool_id}' is not a MultiprocessPool (got {type(pool).__name__})\")\n        return await pool.flush_stdout(process_idx, timeout=timeout)\n\n    async def flush_all_pool_stdout(\n        self, pool_id: str, timeout: float | None = None\n    ) -> dict[int, list[tuple[datetime, bool, str]]]:\n        \"\"\"Flush and retrieve stdout/stderr buffers from all processes in a MultiprocessPool.\n\n        Args:\n            pool_id: The ID of the pool.\n            timeout: Maximum time to wait for each process response.\n\n        Returns:\n            Dict mapping process_idx to list of (timestamp, is_stdout, text) tuples.\n\n        Raises:\n            ValueError: If the pool is not a MultiprocessPool.\n            PoolNotStarted: If the pool is not running.\n        \"\"\"\n        pool = self._pools[pool_id]\n        if not isinstance(pool, MultiprocessPool):\n            raise ValueError(f\"Pool '{pool_id}' is not a MultiprocessPool (got {type(pool).__name__})\")\n        return await pool.flush_all_stdout(timeout=timeout)\n\n    async def __aenter__(self) -> \"ExecutionManager\":\n        \"\"\"Context manager entry - starts the manager.\"\"\"\n        await self.start()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n        \"\"\"Context manager exit - closes the manager.\"\"\"\n        await self.close()",
      "metadata": {},
      "id": "cell17",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Examples\n\nThe following examples demonstrate how to use the ExecutionManager.",
      "metadata": {},
      "id": "cell18"
    },
    {
      "cell_type": "markdown",
      "source": "### Example 1: Basic usage with ThreadPool\n\nThis example shows the basic workflow of running a function on a worker.",
      "metadata": {},
      "id": "cell19"
    },
    {
      "cell_type": "code",
      "source": "print(\"=\" * 50)\nprint(\"Example: Basic ExecutionManager Usage\")\nprint(\"=\" * 50)\n\ndef example_add(a: int, b: int) -> int:\n    \"\"\"A simple function that adds two numbers.\"\"\"\n    print(f\"Adding {a} + {b}\")\n    return a + b\n\n# Create an ExecutionManager with a thread pool\nmanager = ExecutionManager({\n    \"workers\": (ThreadPool, {\"num_workers\": 2}),\n})\n\nasync with manager:\n    print(f\"Pool IDs: {[pool_id for pool_id, _ in manager.pools]}\")\n    print(f\"Workers in 'workers' pool: {manager.get_num_workers('workers')}\")\n\n    # Send a function to all workers in the pool\n    await manager.send_function_to_pool(\"workers\", \"add\", example_add)\n\n    # Run the function on worker 0\n    result = await manager.run(\n        pool_id=\"workers\",\n        worker_id=0,\n        func_import_path_or_key=\"add\",\n        send_channel=False,\n        func_args=(3, 4),\n        func_kwargs={},\n    )\n\n    print(f\"\\nResult: {result.result}\")\n    print(f\"Submitted at: {result.timestamp_utc_submitted}\")\n    print(f\"Started at: {result.timestamp_utc_started}\")\n    print(f\"Completed at: {result.timestamp_utc_completed}\")\n    print(f\"Was converted to str: {result.converted_to_str}\")\n    print(f\"Print buffer: {result.print_buffer}\")",
      "metadata": {},
      "id": "cell20",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Example: Basic ExecutionManager Usage\n",
            "==================================================\n",
            "Pool IDs: ['workers']\n",
            "Workers in 'workers' pool: 2\n",
            "\n",
            "Result: 7\n",
            "Submitted at: 2026-01-19 11:21:43.219941+00:00\n",
            "Started at: 2026-01-19 11:21:43.219997+00:00\n",
            "Completed at: 2026-01-19 11:21:43.220022+00:00\n",
            "Was converted to str: False\n",
            "Print buffer: [(datetime.datetime(2026, 1, 19, 11, 21, 43, 220011, tzinfo=datetime.timezone.utc), 'Adding 3 + 4\\n')]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Example 2: Running multiple jobs with allocation\n\nThis example shows how to use the allocation methods to distribute work.",
      "metadata": {},
      "id": "cell21"
    },
    {
      "cell_type": "code",
      "source": "def example_multiply(x: int, y: int) -> int:\n    \"\"\"Multiply two numbers.\"\"\"\n    import time\n    time.sleep(0.1)  # Simulate some work\n    print(f\"Multiplying {x} * {y}\")\n    return x * y\n\nprint(\"=\" * 50)\nprint(\"Example: Job Allocation\")\nprint(\"=\" * 50)\n\nmanager = ExecutionManager({\n    \"compute\": (ThreadPool, {\"num_workers\": 3}),\n})\n\nasync with manager:\n    # Send the multiply function to all workers\n    await manager.send_function_to_pool(\"compute\", \"multiply\", example_multiply)\n\n    # Run multiple jobs using round-robin allocation\n    print(\"\\nRunning 6 jobs with ROUND_ROBIN allocation:\")\n    tasks = []\n    for i in range(6):\n        task = asyncio.create_task(\n            manager.run_allocate(\n                pool_worker_ids=[\"compute\"],  # Use all workers in \"compute\" pool\n                allocation_method=RunAllocationMethod.ROUND_ROBIN,\n                func_import_path_or_key=\"multiply\",\n                send_channel=False,\n                func_args=(i, i + 1),\n                func_kwargs={},\n            )\n        )\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks)\n    for i, result in enumerate(results):\n        print(f\"  Job {i}: {result.result} (worker {result.worker_id})\")",
      "metadata": {},
      "id": "cell22",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Example: Job Allocation\n",
            "==================================================\n",
            "\n",
            "Running 6 jobs with ROUND_ROBIN allocation:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Job 0: 0 (worker 0)\n",
            "  Job 1: 2 (worker 0)\n",
            "  Job 2: 6 (worker 0)\n",
            "  Job 3: 12 (worker 0)\n",
            "  Job 4: 20 (worker 0)\n",
            "  Job 5: 30 (worker 0)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Example 3: Streaming print output with on_print callback\n\nThis example shows how to use the `on_print` callback to receive print statements\nin real-time as they are flushed from the worker. The `print_flush_interval` controls\nhow often the worker sends accumulated print statements.",
      "metadata": {},
      "id": "cell23"
    },
    {
      "cell_type": "code",
      "source": "import time as _time\n\ndef example_slow_print(iterations: int, delay: float) -> str:\n    \"\"\"A function that prints multiple times with delays.\"\"\"\n    for i in range(iterations):\n        print(f\"Processing step {i + 1}/{iterations}...\")\n        _time.sleep(delay)\n    return f\"Completed {iterations} steps\"\n\nprint(\"=\" * 50)\nprint(\"Example: Streaming Print Output\")\nprint(\"=\" * 50)\n\n# Use a short flush interval (50ms) to see prints arrive in chunks\nmanager = ExecutionManager({\n    \"workers\": (ThreadPool, {\"num_workers\": 1, \"print_flush_interval\": 0.05}),\n})\n\nreceived_count = 0\n\ndef print_callback(buffer):\n    global received_count\n    received_count += 1\n    print(f\"  [Callback {received_count}] Received {len(buffer)} print(s)\")\n    for timestamp, text in buffer:\n        print(f\"    {text.strip()}\")\n\nasync with manager:\n    await manager.send_function_to_pool(\"workers\", \"slow_print\", example_slow_print)\n\n    print(\"\\nRunning function with on_print callback:\")\n    result = await manager.run(\n        pool_id=\"workers\",\n        worker_id=0,\n        func_import_path_or_key=\"slow_print\",\n        send_channel=False,\n        func_args=(5, 0.1),  # 5 iterations, 100ms delay each\n        func_kwargs={},\n        on_print=print_callback,\n    )\n\n    print(f\"\\nResult: {result.result}\")\n    print(f\"Total prints captured: {len(result.print_buffer)}\")",
      "metadata": {},
      "id": "cell24",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Example: Streaming Print Output\n",
            "==================================================\n",
            "\n",
            "Running function with on_print callback:\n",
            "  [Callback 1] Received 2 print(s)\n",
            "    Processing step 1/5...\n",
            "    Processing step 2/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Callback 2] Received 1 print(s)\n",
            "    Processing step 3/5...\n",
            "  [Callback 3] Received 1 print(s)\n",
            "    Processing step 4/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Callback 4] Received 1 print(s)\n",
            "    Processing step 5/5...\n",
            "\n",
            "Result: Completed 5 steps\n",
            "Total prints captured: 5\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "[r[1] for r in result.print_buffer]",
      "metadata": {},
      "id": "cell25",
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/plain": "['Processing step 1/5...\\n',\n 'Processing step 2/5...\\n',\n 'Processing step 3/5...\\n',\n 'Processing step 4/5...\\n',\n 'Processing step 5/5...\\n']"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "nblite_source_hash": "3da4f14d7b48b3c820e9fff2b985f525e8ff1d69d812d608045dc30b01d3a382"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
