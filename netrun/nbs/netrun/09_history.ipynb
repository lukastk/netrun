{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# History and Logging\n",
        "\n",
        "Event history recording and node-level logging for netrun."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|default_exp history"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|hide\n",
        "from nblite import nbl_export, show_doc; nbl_export();"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "import io\n",
        "import json\n",
        "import sys\n",
        "import threading\n",
        "import uuid\n",
        "from collections import deque\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Dict, List, Optional, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class HistoryEntry:\n",
        "    \"\"\"A single entry in the event history.\"\"\"\n",
        "    id: str\n",
        "    timestamp: datetime\n",
        "    entry_type: str  # \"action\" or \"event\"\n",
        "    action_type: Optional[str] = None  # For action entries\n",
        "    event_type: Optional[str] = None   # For event entries\n",
        "    action_id: Optional[str] = None    # For events, links to originating action\n",
        "    data: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        \"\"\"Convert to dictionary for serialization.\"\"\"\n",
        "        result = {\n",
        "            \"id\": self.id,\n",
        "            \"timestamp\": self.timestamp.isoformat(),\n",
        "            \"type\": self.entry_type,\n",
        "        }\n",
        "        if self.action_type:\n",
        "            result[\"action\"] = self.action_type\n",
        "        if self.event_type:\n",
        "            result[\"event\"] = self.event_type\n",
        "        if self.action_id:\n",
        "            result[\"action_id\"] = self.action_id\n",
        "        if self.data:\n",
        "            result[\"data\"] = self.data\n",
        "        return result\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, d: dict) -> \"HistoryEntry\":\n",
        "        \"\"\"Create from dictionary.\"\"\"\n",
        "        return cls(\n",
        "            id=d[\"id\"],\n",
        "            timestamp=datetime.fromisoformat(d[\"timestamp\"]),\n",
        "            entry_type=d[\"type\"],\n",
        "            action_type=d.get(\"action\"),\n",
        "            event_type=d.get(\"event\"),\n",
        "            action_id=d.get(\"action_id\"),\n",
        "            data=d.get(\"data\", {}),\n",
        "        )\n",
        "\n",
        "\n",
        "class EventHistory:\n",
        "    \"\"\"\n",
        "    Records and persists NetAction and NetEvent history.\n",
        "\n",
        "    Features:\n",
        "    - In-memory circular buffer with configurable max size\n",
        "    - Optional JSONL file persistence\n",
        "    - Chunked writes for efficiency\n",
        "    - Flush on demand or when paused\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_size: Optional[int] = None,\n",
        "        file_path: Optional[Union[str, Path]] = None,\n",
        "        chunk_size: int = 100,\n",
        "        flush_on_pause: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize event history.\n",
        "\n",
        "        Args:\n",
        "            max_size: Maximum entries in memory (None = unlimited)\n",
        "            file_path: Path to JSONL file for persistence\n",
        "            chunk_size: Number of entries to buffer before writing\n",
        "            flush_on_pause: Whether to flush when net is paused\n",
        "        \"\"\"\n",
        "        self._max_size = max_size\n",
        "        self._file_path = Path(file_path) if file_path else None\n",
        "        self._chunk_size = chunk_size\n",
        "        self._flush_on_pause = flush_on_pause\n",
        "\n",
        "        # In-memory storage\n",
        "        if max_size is not None:\n",
        "            self._entries: deque = deque(maxlen=max_size)\n",
        "        else:\n",
        "            self._entries: deque = deque()\n",
        "\n",
        "        # Pending entries for file write\n",
        "        self._pending_entries: List[HistoryEntry] = []\n",
        "\n",
        "        # Thread safety\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "        # Current action context for linking events\n",
        "        self._current_action_id: Optional[str] = None\n",
        "\n",
        "    def record_action(\n",
        "        self,\n",
        "        action_type: str,\n",
        "        data: Optional[Dict[str, Any]] = None,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Record a NetAction.\n",
        "\n",
        "        Args:\n",
        "            action_type: The action type name\n",
        "            data: Additional action data\n",
        "\n",
        "        Returns:\n",
        "            The action ID\n",
        "        \"\"\"\n",
        "        action_id = str(uuid.uuid4())\n",
        "        entry = HistoryEntry(\n",
        "            id=action_id,\n",
        "            timestamp=datetime.now(),\n",
        "            entry_type=\"action\",\n",
        "            action_type=action_type,\n",
        "            data=data or {},\n",
        "        )\n",
        "\n",
        "        with self._lock:\n",
        "            self._entries.append(entry)\n",
        "            self._pending_entries.append(entry)\n",
        "            self._current_action_id = action_id\n",
        "\n",
        "            if len(self._pending_entries) >= self._chunk_size:\n",
        "                self._flush_to_file_locked()\n",
        "\n",
        "        return action_id\n",
        "\n",
        "    def record_event(\n",
        "        self,\n",
        "        event_type: str,\n",
        "        data: Optional[Dict[str, Any]] = None,\n",
        "        action_id: Optional[str] = None,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Record a NetEvent.\n",
        "\n",
        "        Args:\n",
        "            event_type: The event type name\n",
        "            data: Additional event data\n",
        "            action_id: The action that triggered this event (defaults to current)\n",
        "\n",
        "        Returns:\n",
        "            The event ID\n",
        "        \"\"\"\n",
        "        event_id = str(uuid.uuid4())\n",
        "        entry = HistoryEntry(\n",
        "            id=event_id,\n",
        "            timestamp=datetime.now(),\n",
        "            entry_type=\"event\",\n",
        "            event_type=event_type,\n",
        "            action_id=action_id or self._current_action_id,\n",
        "            data=data or {},\n",
        "        )\n",
        "\n",
        "        with self._lock:\n",
        "            self._entries.append(entry)\n",
        "            self._pending_entries.append(entry)\n",
        "\n",
        "            if len(self._pending_entries) >= self._chunk_size:\n",
        "                self._flush_to_file_locked()\n",
        "\n",
        "        return event_id\n",
        "\n",
        "    def get_entries(\n",
        "        self,\n",
        "        entry_type: Optional[str] = None,\n",
        "        limit: Optional[int] = None,\n",
        "    ) -> List[HistoryEntry]:\n",
        "        \"\"\"\n",
        "        Get history entries.\n",
        "\n",
        "        Args:\n",
        "            entry_type: Filter by \"action\" or \"event\"\n",
        "            limit: Maximum entries to return (most recent)\n",
        "\n",
        "        Returns:\n",
        "            List of history entries\n",
        "        \"\"\"\n",
        "        with self._lock:\n",
        "            entries = list(self._entries)\n",
        "\n",
        "        if entry_type:\n",
        "            entries = [e for e in entries if e.entry_type == entry_type]\n",
        "\n",
        "        if limit:\n",
        "            entries = entries[-limit:]\n",
        "\n",
        "        return entries\n",
        "\n",
        "    def get_actions(self, limit: Optional[int] = None) -> List[HistoryEntry]:\n",
        "        \"\"\"Get action entries.\"\"\"\n",
        "        return self.get_entries(entry_type=\"action\", limit=limit)\n",
        "\n",
        "    def get_events(self, limit: Optional[int] = None) -> List[HistoryEntry]:\n",
        "        \"\"\"Get event entries.\"\"\"\n",
        "        return self.get_entries(entry_type=\"event\", limit=limit)\n",
        "\n",
        "    def get_events_for_action(self, action_id: str) -> List[HistoryEntry]:\n",
        "        \"\"\"Get all events triggered by a specific action.\"\"\"\n",
        "        with self._lock:\n",
        "            return [e for e in self._entries if e.action_id == action_id]\n",
        "\n",
        "    def flush(self) -> None:\n",
        "        \"\"\"Flush pending entries to file.\"\"\"\n",
        "        with self._lock:\n",
        "            self._flush_to_file_locked()\n",
        "\n",
        "    def _flush_to_file_locked(self) -> None:\n",
        "        \"\"\"Flush pending entries to file (must hold lock).\"\"\"\n",
        "        if not self._file_path or not self._pending_entries:\n",
        "            return\n",
        "\n",
        "        with open(self._file_path, \"a\") as f:\n",
        "            for entry in self._pending_entries:\n",
        "                f.write(json.dumps(entry.to_dict()) + \"\\n\")\n",
        "\n",
        "        self._pending_entries.clear()\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear all history (memory and file).\"\"\"\n",
        "        with self._lock:\n",
        "            self._entries.clear()\n",
        "            self._pending_entries.clear()\n",
        "\n",
        "        if self._file_path and self._file_path.exists():\n",
        "            self._file_path.unlink()\n",
        "\n",
        "    @property\n",
        "    def file_path(self) -> Optional[Path]:\n",
        "        \"\"\"The history file path.\"\"\"\n",
        "        return self._file_path\n",
        "\n",
        "    @property\n",
        "    def flush_on_pause(self) -> bool:\n",
        "        \"\"\"Whether to flush when net is paused.\"\"\"\n",
        "        return self._flush_on_pause\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Number of entries in memory.\"\"\"\n",
        "        with self._lock:\n",
        "            return len(self._entries)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "class NodeLogEntry:\n",
        "    \"\"\"A single log entry for a node.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        timestamp: Optional[datetime] = None,\n",
        "        epoch_id: Optional[str] = None,\n",
        "        level: str = \"info\",\n",
        "    ):\n",
        "        self.message = message\n",
        "        self.timestamp = timestamp or datetime.now()\n",
        "        self.epoch_id = epoch_id\n",
        "        self.level = level\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        \"\"\"Convert to dictionary.\"\"\"\n",
        "        return {\n",
        "            \"message\": self.message,\n",
        "            \"timestamp\": self.timestamp.isoformat(),\n",
        "            \"epoch_id\": self.epoch_id,\n",
        "            \"level\": self.level,\n",
        "        }\n",
        "\n",
        "\n",
        "class NodeLog:\n",
        "    \"\"\"\n",
        "    Log storage for a single node.\n",
        "\n",
        "    Captures print statements during node execution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, node_name: str, max_entries: Optional[int] = None):\n",
        "        self.node_name = node_name\n",
        "        self._max_entries = max_entries\n",
        "\n",
        "        if max_entries is not None:\n",
        "            self._entries: deque = deque(maxlen=max_entries)\n",
        "        else:\n",
        "            self._entries: deque = deque()\n",
        "\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "    def add(\n",
        "        self,\n",
        "        message: str,\n",
        "        epoch_id: Optional[str] = None,\n",
        "        level: str = \"info\",\n",
        "    ) -> None:\n",
        "        \"\"\"Add a log entry.\"\"\"\n",
        "        entry = NodeLogEntry(\n",
        "            message=message,\n",
        "            epoch_id=epoch_id,\n",
        "            level=level,\n",
        "        )\n",
        "        with self._lock:\n",
        "            self._entries.append(entry)\n",
        "\n",
        "    def get_entries(\n",
        "        self,\n",
        "        epoch_id: Optional[str] = None,\n",
        "        limit: Optional[int] = None,\n",
        "    ) -> List[NodeLogEntry]:\n",
        "        \"\"\"\n",
        "        Get log entries.\n",
        "\n",
        "        Args:\n",
        "            epoch_id: Filter by epoch ID\n",
        "            limit: Maximum entries to return\n",
        "        \"\"\"\n",
        "        with self._lock:\n",
        "            entries = list(self._entries)\n",
        "\n",
        "        if epoch_id:\n",
        "            entries = [e for e in entries if e.epoch_id == epoch_id]\n",
        "\n",
        "        if limit:\n",
        "            entries = entries[-limit:]\n",
        "\n",
        "        return entries\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear all entries.\"\"\"\n",
        "        with self._lock:\n",
        "            self._entries.clear()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        with self._lock:\n",
        "            return len(self._entries)\n",
        "\n",
        "\n",
        "class NodeLogManager:\n",
        "    \"\"\"\n",
        "    Manages logs for all nodes in a net.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_entries_per_node: Optional[int] = None):\n",
        "        self._max_entries_per_node = max_entries_per_node\n",
        "        self._logs: Dict[str, NodeLog] = {}\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "    def get_log(self, node_name: str) -> NodeLog:\n",
        "        \"\"\"Get or create log for a node.\"\"\"\n",
        "        with self._lock:\n",
        "            if node_name not in self._logs:\n",
        "                self._logs[node_name] = NodeLog(\n",
        "                    node_name,\n",
        "                    max_entries=self._max_entries_per_node,\n",
        "                )\n",
        "            return self._logs[node_name]\n",
        "\n",
        "    def get_node_log(\n",
        "        self,\n",
        "        node_name: str,\n",
        "        limit: Optional[int] = None,\n",
        "    ) -> List[NodeLogEntry]:\n",
        "        \"\"\"Get log entries for a node.\"\"\"\n",
        "        log = self.get_log(node_name)\n",
        "        return log.get_entries(limit=limit)\n",
        "\n",
        "    def get_epoch_log(\n",
        "        self,\n",
        "        node_name: str,\n",
        "        epoch_id: str,\n",
        "        limit: Optional[int] = None,\n",
        "    ) -> List[NodeLogEntry]:\n",
        "        \"\"\"Get log entries for a specific epoch.\"\"\"\n",
        "        log = self.get_log(node_name)\n",
        "        return log.get_entries(epoch_id=epoch_id, limit=limit)\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear all logs.\"\"\"\n",
        "        with self._lock:\n",
        "            for log in self._logs.values():\n",
        "                log.clear()\n",
        "            self._logs.clear()"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "class StdoutCapture:\n",
        "    \"\"\"\n",
        "    Context manager for capturing stdout during node execution.\n",
        "\n",
        "    Redirects print statements to a NodeLog while optionally echoing\n",
        "    to the original stdout.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_log: NodeLog,\n",
        "        epoch_id: str,\n",
        "        echo: bool = False,\n",
        "    ):\n",
        "        self._node_log = node_log\n",
        "        self._epoch_id = epoch_id\n",
        "        self._echo = echo\n",
        "        self._original_stdout = None\n",
        "        self._capture_buffer = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        self._capture_buffer = io.StringIO()\n",
        "\n",
        "        # Create a custom stdout that captures and optionally echoes\n",
        "        self._custom_stdout = _CaptureStdout(\n",
        "            buffer=self._capture_buffer,\n",
        "            echo_to=self._original_stdout if self._echo else None,\n",
        "        )\n",
        "        sys.stdout = self._custom_stdout\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "        # Process captured output\n",
        "        captured = self._capture_buffer.getvalue()\n",
        "        if captured:\n",
        "            # Add each line as a separate log entry\n",
        "            for line in captured.splitlines():\n",
        "                if line.strip():\n",
        "                    self._node_log.add(\n",
        "                        message=line,\n",
        "                        epoch_id=self._epoch_id,\n",
        "                        level=\"info\",\n",
        "                    )\n",
        "\n",
        "        return False\n",
        "\n",
        "\n",
        "class _CaptureStdout:\n",
        "    \"\"\"Custom stdout that captures to buffer and optionally echoes.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        buffer: io.StringIO,\n",
        "        echo_to: Optional[io.TextIOBase] = None,\n",
        "    ):\n",
        "        self._buffer = buffer\n",
        "        self._echo_to = echo_to\n",
        "\n",
        "    def write(self, s: str) -> int:\n",
        "        result = self._buffer.write(s)\n",
        "        if self._echo_to:\n",
        "            self._echo_to.write(s)\n",
        "        return result\n",
        "\n",
        "    def flush(self) -> None:\n",
        "        self._buffer.flush()\n",
        "        if self._echo_to:\n",
        "            self._echo_to.flush()\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def capture_stdout(\n",
        "    node_log: NodeLog,\n",
        "    epoch_id: str,\n",
        "    echo: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Context manager for capturing stdout.\n",
        "\n",
        "    Usage:\n",
        "        with capture_stdout(node_log, epoch_id):\n",
        "            print(\"This will be captured\")\n",
        "    \"\"\"\n",
        "    capture = StdoutCapture(node_log, epoch_id, echo)\n",
        "    with capture:\n",
        "        yield capture"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
