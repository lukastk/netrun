{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#|default_exp pool.multiprocess"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|hide\n",
        "from nblite import nbl_export; nbl_export();"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiprocess Pool\n",
        "\n",
        "A pool of worker processes, each running multiple worker threads.\n",
        "Uses `multiprocessing.Queue` for communication between the parent\n",
        "and subprocesses.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "Parent Process\n",
        "    │\n",
        "    ├── ProcessChannel ──► Subprocess 0\n",
        "    │                         ├── Thread 0 (worker_id=0)\n",
        "    │                         ├── Thread 1 (worker_id=1)\n",
        "    │                         └── Thread 2 (worker_id=2)\n",
        "    │\n",
        "    └── ProcessChannel ──► Subprocess 1\n",
        "                            ├── Thread 0 (worker_id=3)\n",
        "                            ├── Thread 1 (worker_id=4)\n",
        "                            └── Thread 2 (worker_id=5)\n",
        "```\n",
        "\n",
        "Worker IDs are flat integers: `worker_id = process_idx * threads_per_process + thread_idx`\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from netrun.pool.multiprocess import MultiprocessPool\n",
        "\n",
        "# Worker function must be importable (defined at module level)\n",
        "def my_worker(channel, worker_id):\n",
        "    while True:\n",
        "        key, data = channel.recv()\n",
        "        channel.send(\"result\", data * 2)\n",
        "\n",
        "pool = MultiprocessPool(\n",
        "    worker_fn=my_worker,\n",
        "    num_processes=2,\n",
        "    threads_per_process=3,\n",
        ")\n",
        "await pool.start()\n",
        "await pool.send(worker_id=0, key=\"task\", data=10)\n",
        "msg = await pool.recv()\n",
        "await pool.close()\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "import asyncio\n",
        "import datetime\n",
        "import queue\n",
        "import sys\n",
        "import threading\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "from typing import Any\n",
        "from collections.abc import Callable\n",
        "\n",
        "from netrun.rpc.base import ChannelClosed, RecvTimeout, RPC_KEY_SHUTDOWN\n",
        "from netrun.rpc.multiprocess import (\n",
        "    ProcessChannel,\n",
        "    SyncProcessChannel,\n",
        "    create_queue_pair,\n",
        ")\n",
        "from netrun.pool.base import (\n",
        "    WorkerId,\n",
        "    WorkerFn,\n",
        "    WorkerMessage,\n",
        "    PoolNotStarted,\n",
        "    PoolAlreadyStarted,\n",
        "    POOL_UP_ERROR_EXCEPTION,\n",
        "    POOL_UP_ERROR_CRASHED,\n",
        "    _check_error_and_raise,\n",
        ")\n",
        "from netrun._iutils import get_timestamp_utc"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiprocessPool Keys\n",
        "\n",
        "Keys for internal routing in multiprocess pools.\n",
        "Downstream: parent → subprocess, Upstream: subprocess → parent"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "MP_DOWN_SHUTDOWN = \"__pool-mp-down:shutdown\"\n",
        "\"\"\"Signal subprocess to shut down gracefully. Data: None.\n",
        "\n",
        "Note: This is different from RPC_KEY_SHUTDOWN which triggers immediate channel close.\n",
        "This key allows the subprocess to complete its shutdown sequence (including final\n",
        "flush and SHUTDOWN_COMPLETE signal) before the channel is closed.\"\"\""
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "MP_DOWN_DISPATCH = \"__pool-mp-down:dispatch\"\n",
        "\"\"\"Route message to specific worker thread. Data: (thread_idx, key, data)\"\"\""
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "MP_DOWN_BROADCAST = \"__pool-mp-down:broadcast\"\n",
        "\"\"\"Broadcast to all workers in subprocess. Data: (key, data)\"\"\""
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "MP_UP_RESPONSE = \"__pool-mp-up:response\"\n",
        "\"\"\"Wrapper for worker responses. Data: (worker_id, key, data)\"\"\""
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "MP_DOWN_FLUSH_STDOUT = \"__pool-mp-down:flush-stdout\"\n",
        "\"\"\"Request to flush stdout buffer and send it back. Data: None\"\"\""
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "MP_UP_STDOUT_BUFFER = \"__pool-mp-up:stdout-buffer\"\n",
        "\"\"\"Stdout buffer contents. Data: list[tuple[datetime, bool, str]]\"\"\""
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "MP_UP_SHUTDOWN_COMPLETE = \"__pool-mp-up:shutdown-complete\"\n",
        "\"\"\"Signal from subprocess that shutdown is complete and final buffer has been sent. Data: None\"\"\""
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "OutputBuffer = list[tuple[datetime.datetime, bool, str]]\n",
        "\"\"\"Type alias for stdout/stderr buffer. Each entry is (timestamp, is_stdout, text).\"\"\""
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Capture\n",
        "\n",
        "A file-like object that captures writes and stores them in a shared buffer\n",
        "with timestamps. Used to redirect stdout/stderr in subprocesses."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "class _OutputCapture:\n",
        "    \"\"\"Captures writes to stdout/stderr with timestamps.\n",
        "\n",
        "    Args:\n",
        "        buffer: Shared list to append captured output to\n",
        "        is_stdout: True for stdout, False for stderr\n",
        "        buffer_output: If False, discard output instead of buffering\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        buffer: OutputBuffer,\n",
        "        is_stdout: bool,\n",
        "        buffer_output: bool = True,\n",
        "    ):\n",
        "        self._buffer = buffer\n",
        "        self._is_stdout = is_stdout\n",
        "        self._buffer_output = buffer_output\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "    def write(self, text: str) -> int:\n",
        "        \"\"\"Capture a write with timestamp.\"\"\"\n",
        "        if text and self._buffer_output:\n",
        "            with self._lock:\n",
        "                self._buffer.append((get_timestamp_utc(), self._is_stdout, text))\n",
        "        return len(text)\n",
        "\n",
        "    def flush(self) -> None:\n",
        "        \"\"\"No-op for compatibility.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def fileno(self) -> int:\n",
        "        \"\"\"Return invalid fd - not connected to real file.\"\"\"\n",
        "        raise OSError(\"_OutputCapture does not have a file descriptor\")\n",
        "\n",
        "    def isatty(self) -> bool:\n",
        "        \"\"\"Not a TTY.\"\"\"\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subprocess Worker Entry Point\n",
        "\n",
        "This function runs in each subprocess. It creates worker threads and\n",
        "routes messages between the parent and workers."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "def _subprocess_main(\n",
        "    parent_send_q: mp.Queue,\n",
        "    parent_recv_q: mp.Queue,\n",
        "    worker_fn: WorkerFn,\n",
        "    num_threads: int,\n",
        "    process_idx: int,\n",
        "    threads_per_process: int,\n",
        "    redirect_output: bool = True,\n",
        "    buffer_output: bool = True,\n",
        "    output_flush_interval: float = 0.1,\n",
        "):\n",
        "    \"\"\"Entry point for subprocess. Routes messages to/from worker threads.\n",
        "\n",
        "    Args:\n",
        "        parent_send_q: Queue for sending to parent\n",
        "        parent_recv_q: Queue for receiving from parent\n",
        "        worker_fn: Worker function to run in threads\n",
        "        num_threads: Number of worker threads to create\n",
        "        process_idx: Index of this process\n",
        "        threads_per_process: Total threads per process (for worker_id calculation)\n",
        "        redirect_output: If True, capture stdout/stderr\n",
        "        buffer_output: If True, buffer captured output; if False, discard it\n",
        "        output_flush_interval: Interval in seconds between automatic output buffer flushes\n",
        "    \"\"\"\n",
        "    # Set up stdout/stderr capture if requested\n",
        "    output_buffer: OutputBuffer = []\n",
        "    output_buffer_lock = threading.Lock()\n",
        "\n",
        "    if redirect_output:\n",
        "        stdout_capture = _OutputCapture(output_buffer, is_stdout=True, buffer_output=buffer_output)\n",
        "        stderr_capture = _OutputCapture(output_buffer, is_stdout=False, buffer_output=buffer_output)\n",
        "        sys.stdout = stdout_capture  # type: ignore\n",
        "        sys.stderr = stderr_capture  # type: ignore\n",
        "\n",
        "    # Create channel to parent\n",
        "    parent_channel = SyncProcessChannel(parent_send_q, parent_recv_q)\n",
        "\n",
        "    # Create thread-safe queues for each worker\n",
        "    # worker_queues[thread_idx] = (send_to_worker, recv_from_worker)\n",
        "    worker_send_queues: list[queue.Queue] = [queue.Queue() for _ in range(num_threads)]\n",
        "    response_queue: queue.Queue = queue.Queue()  # All workers send responses here\n",
        "\n",
        "    # Start worker threads\n",
        "    threads = []\n",
        "    for thread_idx in range(num_threads):\n",
        "        worker_id = process_idx * threads_per_process + thread_idx\n",
        "        t = threading.Thread(\n",
        "            target=_thread_worker,\n",
        "            args=(worker_fn, worker_send_queues[thread_idx], response_queue, worker_id),\n",
        "            daemon=True,\n",
        "        )\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "\n",
        "    # Router loop: handle messages from parent and responses from workers\n",
        "    shutdown = False\n",
        "    shutdown_event = threading.Event()\n",
        "\n",
        "    def response_forwarder():\n",
        "        \"\"\"Forward responses from workers to parent.\"\"\"\n",
        "        while not shutdown:\n",
        "            try:\n",
        "                msg = response_queue.get(timeout=None)\n",
        "                if msg is None:\n",
        "                    break\n",
        "                worker_id, key, data = msg\n",
        "                parent_channel.send(MP_UP_RESPONSE, (worker_id, key, data))\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "            except Exception:\n",
        "                break\n",
        "\n",
        "        # Drain any remaining messages before exiting\n",
        "        while True:\n",
        "            try:\n",
        "                msg = response_queue.get_nowait()\n",
        "                if msg is None:\n",
        "                    break\n",
        "                worker_id, key, data = msg\n",
        "                parent_channel.send(MP_UP_RESPONSE, (worker_id, key, data))\n",
        "            except queue.Empty:\n",
        "                break\n",
        "            except Exception:\n",
        "                break\n",
        "\n",
        "    def output_flusher():\n",
        "        \"\"\"Periodically flush output buffer to parent.\"\"\"\n",
        "        while not shutdown:\n",
        "            # Wait for shutdown_event or timeout - allows fast wakeup on shutdown\n",
        "            shutdown_event.wait(timeout=output_flush_interval)\n",
        "            if shutdown:\n",
        "                break\n",
        "            with output_buffer_lock:\n",
        "                if output_buffer:\n",
        "                    buffer_copy = list(output_buffer)\n",
        "                    output_buffer.clear()\n",
        "                    try:\n",
        "                        parent_channel.send(MP_UP_STDOUT_BUFFER, buffer_copy)\n",
        "                    except Exception:\n",
        "                        # Channel might be closed during shutdown\n",
        "                        break\n",
        "\n",
        "    # Start response forwarder thread\n",
        "    forwarder = threading.Thread(target=response_forwarder, daemon=True)\n",
        "    forwarder.start()\n",
        "\n",
        "    # Start output flusher thread if output is being captured and buffered\n",
        "    flusher = None\n",
        "    if redirect_output and buffer_output:\n",
        "        flusher = threading.Thread(target=output_flusher, daemon=True)\n",
        "        flusher.start()\n",
        "\n",
        "    # Main loop: receive from parent and dispatch to workers\n",
        "    try:\n",
        "        while True:\n",
        "            key, data = parent_channel.recv()\n",
        "\n",
        "            if key == MP_DOWN_DISPATCH:\n",
        "                thread_idx, msg_key, msg_data = data\n",
        "                worker_send_queues[thread_idx].put((msg_key, msg_data))\n",
        "            elif key == MP_DOWN_BROADCAST:\n",
        "                msg_key, msg_data = data\n",
        "                for q in worker_send_queues:\n",
        "                    q.put((msg_key, msg_data))\n",
        "            elif key == MP_DOWN_FLUSH_STDOUT:\n",
        "                # Flush and send back the output buffer\n",
        "                with output_buffer_lock:\n",
        "                    buffer_copy = list(output_buffer)\n",
        "                    output_buffer.clear()\n",
        "                parent_channel.send(MP_UP_STDOUT_BUFFER, buffer_copy)\n",
        "            elif key == MP_DOWN_SHUTDOWN:\n",
        "                # Graceful shutdown - allows us to send SHUTDOWN_COMPLETE before channel closes\n",
        "                break\n",
        "    except ChannelClosed:\n",
        "        pass\n",
        "    finally:\n",
        "        # Signal shutdown to background threads\n",
        "        shutdown = True\n",
        "        shutdown_event.set()  # Wake up flusher immediately\n",
        "\n",
        "        # Signal workers to stop\n",
        "        for q in worker_send_queues:\n",
        "            q.put((RPC_KEY_SHUTDOWN, None))\n",
        "\n",
        "        # Wait for worker threads to finish sending their responses\n",
        "        for t in threads:\n",
        "            t.join()\n",
        "\n",
        "        # Now signal forwarder to stop (after workers are done)\n",
        "        response_queue.put(None)\n",
        "\n",
        "        # Wait for forwarder to finish draining\n",
        "        forwarder.join()\n",
        "\n",
        "        # Flusher will exit quickly due to shutdown_event being set\n",
        "        if flusher is not None:\n",
        "            flusher.join(timeout=1.0)  # Short timeout since we signaled shutdown_event\n",
        "\n",
        "        # Final flush of any remaining output\n",
        "        if redirect_output and buffer_output:\n",
        "            with output_buffer_lock:\n",
        "                if output_buffer:\n",
        "                    try:\n",
        "                        parent_channel.send(MP_UP_STDOUT_BUFFER, list(output_buffer))\n",
        "                        output_buffer.clear()\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "        # Signal that shutdown is complete\n",
        "        try:\n",
        "            parent_channel.send(MP_UP_SHUTDOWN_COMPLETE, None)\n",
        "        except Exception:\n",
        "            pass"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "def _thread_worker(\n",
        "    worker_fn: WorkerFn,\n",
        "    recv_queue: queue.Queue,\n",
        "    response_queue: queue.Queue,\n",
        "    worker_id: WorkerId,\n",
        "):\n",
        "    \"\"\"Run worker function in a thread within subprocess.\"\"\"\n",
        "\n",
        "    class _WorkerChannel:\n",
        "        \"\"\"Adapter that looks like SyncRPCChannel for the worker.\"\"\"\n",
        "        def __init__(self):\n",
        "            self._closed = False\n",
        "\n",
        "        def send(self, key: str, data: Any) -> None:\n",
        "            if self._closed:\n",
        "                raise ChannelClosed(\"Channel is closed\")\n",
        "            response_queue.put((worker_id, key, data))\n",
        "\n",
        "        def recv(self, timeout: float | None = None) -> tuple[str, Any]:\n",
        "            if self._closed:\n",
        "                raise ChannelClosed(\"Channel is closed\")\n",
        "            try:\n",
        "                key, data = recv_queue.get(timeout=timeout)\n",
        "                if key == RPC_KEY_SHUTDOWN:\n",
        "                    self._closed = True\n",
        "                    raise ChannelClosed(\"Channel was shut down\")\n",
        "                return key, data\n",
        "            except queue.Empty:\n",
        "                from netrun.rpc.base import RecvTimeout\n",
        "                raise RecvTimeout(f\"Receive timed out after {timeout}s\")\n",
        "\n",
        "        def try_recv(self) -> tuple[str, Any] | None:\n",
        "            if self._closed:\n",
        "                raise ChannelClosed(\"Channel is closed\")\n",
        "            try:\n",
        "                key, data = recv_queue.get_nowait()\n",
        "                if key == RPC_KEY_SHUTDOWN:\n",
        "                    self._closed = True\n",
        "                    raise ChannelClosed(\"Channel was shut down\")\n",
        "                return key, data\n",
        "            except queue.Empty:\n",
        "                return None\n",
        "\n",
        "        def close(self) -> None:\n",
        "            self._closed = True\n",
        "\n",
        "        @property\n",
        "        def is_closed(self) -> bool:\n",
        "            return self._closed\n",
        "\n",
        "    channel = _WorkerChannel()\n",
        "    try:\n",
        "        worker_fn(channel, worker_id)\n",
        "    except ChannelClosed:\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        # Send error back - try exception object first, fallback to dict if unpickleable\n",
        "        import pickle\n",
        "        import traceback\n",
        "        try:\n",
        "            pickle.dumps(e)  # Test if pickleable\n",
        "            response_queue.put((worker_id, POOL_UP_ERROR_EXCEPTION, e))\n",
        "        except Exception:\n",
        "            # Fallback to dict with error info\n",
        "            response_queue.put((worker_id, POOL_UP_ERROR_EXCEPTION, {\n",
        "                \"type\": type(e).__name__,\n",
        "                \"message\": str(e),\n",
        "                \"traceback\": traceback.format_exc(),\n",
        "            }))"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiprocessPool"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "class MultiprocessPool:\n",
        "    \"\"\"A pool of worker processes, each running multiple threads.\n",
        "\n",
        "    Messages are routed to specific workers via their worker_id.\n",
        "    Worker IDs are flat integers from 0 to (num_processes * threads_per_process - 1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        worker_fn: WorkerFn,\n",
        "        num_processes: int,\n",
        "        threads_per_process: int = 1,\n",
        "        redirect_output: bool = True,\n",
        "        buffer_output: bool = True,\n",
        "        output_flush_interval: float = 0.1,\n",
        "        on_output: Callable[[int, OutputBuffer], None] | None = None,\n",
        "    ):\n",
        "        \"\"\"Create a multiprocess pool.\n",
        "\n",
        "        Args:\n",
        "            worker_fn: Function to run in each worker thread (must be importable)\n",
        "            num_processes: Number of subprocesses to create\n",
        "            threads_per_process: Number of worker threads per subprocess\n",
        "            redirect_output: If True, capture stdout/stderr from subprocesses\n",
        "            buffer_output: If True, buffer captured output for retrieval.\n",
        "                          If False, discard captured output (silent mode).\n",
        "                          Only applies when redirect_output=True.\n",
        "            output_flush_interval: Interval in seconds between automatic output buffer\n",
        "                          flushes from subprocesses (default 0.1 = 100ms).\n",
        "                          Only applies when redirect_output=True and buffer_output=True.\n",
        "            on_output: Optional callback called when output buffer is received from a\n",
        "                          subprocess. Called with (process_idx, buffer) where buffer is\n",
        "                          a list of (timestamp, is_stdout, text) tuples.\n",
        "        \"\"\"\n",
        "        if num_processes < 1:\n",
        "            raise ValueError(\"num_processes must be at least 1\")\n",
        "        if threads_per_process < 1:\n",
        "            raise ValueError(\"threads_per_process must be at least 1\")\n",
        "\n",
        "        self._worker_fn = worker_fn\n",
        "        self._num_processes = num_processes\n",
        "        self._threads_per_process = threads_per_process\n",
        "        self._num_workers = num_processes * threads_per_process\n",
        "        self._redirect_output = redirect_output\n",
        "        self._buffer_output = buffer_output\n",
        "        self._output_flush_interval = output_flush_interval\n",
        "        self._on_output = on_output\n",
        "        self._running = False\n",
        "\n",
        "        # Will be populated on start()\n",
        "        self._channels: list[ProcessChannel] = []\n",
        "        self._processes: list[mp.Process] = []\n",
        "        self._recv_queue: asyncio.Queue = asyncio.Queue()\n",
        "        self._recv_tasks: list[asyncio.Task] = []\n",
        "        self._monitor_task: asyncio.Task | None = None\n",
        "        self._dead_processes: set[int] = set()  # Track processes we've already reported as dead\n",
        "        self._stdout_buffers: dict[int, OutputBuffer] = {}  # process_idx -> buffer\n",
        "        self._flush_events: dict[int, asyncio.Event] = {}  # process_idx -> event for flush response\n",
        "        self._shutdown_complete_events: dict[int, asyncio.Event] = {}  # process_idx -> event for shutdown complete\n",
        "\n",
        "    @property\n",
        "    def num_workers(self) -> int:\n",
        "        \"\"\"Total number of workers in the pool.\"\"\"\n",
        "        return self._num_workers\n",
        "\n",
        "    @property\n",
        "    def num_processes(self) -> int:\n",
        "        \"\"\"Number of subprocesses.\"\"\"\n",
        "        return self._num_processes\n",
        "\n",
        "    @property\n",
        "    def threads_per_process(self) -> int:\n",
        "        \"\"\"Number of threads per subprocess.\"\"\"\n",
        "        return self._threads_per_process\n",
        "\n",
        "    @property\n",
        "    def is_running(self) -> bool:\n",
        "        \"\"\"Whether the pool has been started.\"\"\"\n",
        "        return self._running\n",
        "\n",
        "    def _worker_id_to_process_thread(self, worker_id: WorkerId) -> tuple[int, int]:\n",
        "        \"\"\"Convert flat worker_id to (process_idx, thread_idx).\"\"\"\n",
        "        process_idx = worker_id // self._threads_per_process\n",
        "        thread_idx = worker_id % self._threads_per_process\n",
        "        return process_idx, thread_idx\n",
        "\n",
        "    async def start(self) -> None:\n",
        "        \"\"\"Start all processes and workers.\"\"\"\n",
        "        if self._running:\n",
        "            raise PoolAlreadyStarted(\"Pool is already running\")\n",
        "\n",
        "        ctx = mp.get_context(\"spawn\")\n",
        "        self._channels = []\n",
        "        self._processes = []\n",
        "        self._stdout_buffers = {}\n",
        "        self._flush_events = {}\n",
        "\n",
        "        for process_idx in range(self._num_processes):\n",
        "            # Create channel pair\n",
        "            parent_channel, child_queues = create_queue_pair(ctx)\n",
        "            self._channels.append(parent_channel)\n",
        "\n",
        "            # Initialize stdout buffer for this process\n",
        "            self._stdout_buffers[process_idx] = []\n",
        "\n",
        "            # Create and start subprocess\n",
        "            proc = ctx.Process(\n",
        "                target=_subprocess_main,\n",
        "                args=(\n",
        "                    child_queues[0],  # send_q\n",
        "                    child_queues[1],  # recv_q\n",
        "                    self._worker_fn,\n",
        "                    self._threads_per_process,\n",
        "                    process_idx,\n",
        "                    self._threads_per_process,\n",
        "                    self._redirect_output,\n",
        "                    self._buffer_output,\n",
        "                    self._output_flush_interval,\n",
        "                ),\n",
        "            )\n",
        "            proc.start()\n",
        "            self._processes.append(proc)\n",
        "\n",
        "        self._running = True\n",
        "        self._dead_processes = set()\n",
        "        self._monitor_task = asyncio.create_task(self._monitor_processes())\n",
        "\n",
        "    async def _monitor_processes(self) -> None:\n",
        "        \"\"\"Background task to detect dead subprocesses.\"\"\"\n",
        "        while self._running:\n",
        "            for proc_idx, proc in enumerate(self._processes):\n",
        "                if proc_idx not in self._dead_processes and proc.exitcode is not None:\n",
        "                    # Process died\n",
        "                    self._dead_processes.add(proc_idx)\n",
        "                    exit_info = {\n",
        "                        \"exit_code\": proc.exitcode,\n",
        "                        \"reason\": f\"Process exited with code {proc.exitcode}\",\n",
        "                    }\n",
        "                    # Signal death for all workers in this process\n",
        "                    for thread_idx in range(self._threads_per_process):\n",
        "                        worker_id = proc_idx * self._threads_per_process + thread_idx\n",
        "                        await self._recv_queue.put(WorkerMessage(\n",
        "                            worker_id=worker_id,\n",
        "                            key=POOL_UP_ERROR_CRASHED,\n",
        "                            data=exit_info\n",
        "                        ))\n",
        "            await asyncio.sleep(0.5)  # Check every 500ms\n",
        "\n",
        "    async def close(self, timeout: float | None = None) -> None:\n",
        "        \"\"\"Shut down all processes and clean up resources.\n",
        "\n",
        "        This method ensures that all pending output from subprocesses is received\n",
        "        before shutting down. It works by:\n",
        "        1. Sending RPC_KEY_SHUTDOWN to each subprocess\n",
        "        2. Waiting for each subprocess to send SHUTDOWN_COMPLETE (with final flush)\n",
        "        3. Then closing channels and cleaning up\n",
        "\n",
        "        Args:\n",
        "            timeout: Max seconds to wait for each process to finish gracefully.\n",
        "                     If None, wait indefinitely. If timeout expires, processes\n",
        "                     are forcefully terminated.\n",
        "        \"\"\"\n",
        "        if not self._running:\n",
        "            return\n",
        "\n",
        "        # Ensure recv tasks are running BEFORE we set _running = False\n",
        "        # This ensures we can receive SHUTDOWN_COMPLETE messages\n",
        "        self._start_recv_tasks()\n",
        "\n",
        "        self._running = False\n",
        "\n",
        "        # Cancel monitor task\n",
        "        if self._monitor_task and not self._monitor_task.done():\n",
        "            self._monitor_task.cancel()\n",
        "            try:\n",
        "                await self._monitor_task\n",
        "            except asyncio.CancelledError:\n",
        "                pass\n",
        "\n",
        "        # Create shutdown complete events for each process\n",
        "        for process_idx in range(self._num_processes):\n",
        "            self._shutdown_complete_events[process_idx] = asyncio.Event()\n",
        "\n",
        "        # Send shutdown signals to each subprocess using MP_DOWN_SHUTDOWN (NOT RPC_KEY_SHUTDOWN)\n",
        "        # MP_DOWN_SHUTDOWN allows the subprocess to complete its shutdown sequence\n",
        "        # (including final flush and SHUTDOWN_COMPLETE) before the channel is closed\n",
        "        for channel in self._channels:\n",
        "            try:\n",
        "                await channel.send(MP_DOWN_SHUTDOWN, None)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Wait for SHUTDOWN_COMPLETE from all processes\n",
        "        # The recv loop will set the events when it receives SHUTDOWN_COMPLETE\n",
        "        shutdown_timeout = timeout if timeout is not None else 30.0  # Default 30s\n",
        "        try:\n",
        "            await asyncio.wait_for(\n",
        "                asyncio.gather(*[\n",
        "                    self._shutdown_complete_events[i].wait()\n",
        "                    for i in range(self._num_processes)\n",
        "                ]),\n",
        "                timeout=shutdown_timeout\n",
        "            )\n",
        "        except asyncio.TimeoutError:\n",
        "            pass  # Continue with cleanup even if some processes didn't respond\n",
        "\n",
        "        # Now close channels - this is safe since we've received all messages\n",
        "        for channel in self._channels:\n",
        "            await channel.close()\n",
        "\n",
        "        # Cancel recv tasks (they should already be done since they break after SHUTDOWN_COMPLETE)\n",
        "        for task in self._recv_tasks:\n",
        "            if not task.done():\n",
        "                task.cancel()\n",
        "                try:\n",
        "                    await task\n",
        "                except asyncio.CancelledError:\n",
        "                    pass\n",
        "\n",
        "        # Wait for processes to finish\n",
        "        for proc in self._processes:\n",
        "            proc.join(timeout=timeout)\n",
        "            if proc.is_alive():\n",
        "                proc.terminate()\n",
        "\n",
        "        self._channels = []\n",
        "        self._processes = []\n",
        "        self._recv_queue = asyncio.Queue()\n",
        "        self._recv_tasks = []\n",
        "        self._monitor_task = None\n",
        "        self._dead_processes = set()\n",
        "        self._stdout_buffers = {}\n",
        "        self._flush_events = {}\n",
        "        self._shutdown_complete_events = {}\n",
        "\n",
        "    async def send(self, worker_id: WorkerId, key: str, data: Any) -> None:\n",
        "        \"\"\"Send a message to a specific worker.\"\"\"\n",
        "        if not self._running:\n",
        "            raise PoolNotStarted(\"Pool has not been started\")\n",
        "\n",
        "        if worker_id < 0 or worker_id >= self._num_workers:\n",
        "            raise ValueError(f\"worker_id {worker_id} out of range [0, {self._num_workers})\")\n",
        "\n",
        "        process_idx, thread_idx = self._worker_id_to_process_thread(worker_id)\n",
        "        await self._channels[process_idx].send(MP_DOWN_DISPATCH, (thread_idx, key, data))\n",
        "\n",
        "    def _start_recv_tasks(self) -> None:\n",
        "        \"\"\"Start background tasks that forward messages to the queue.\"\"\"\n",
        "        if self._recv_tasks:\n",
        "            return\n",
        "\n",
        "        async def recv_loop(process_idx: int, channel: ProcessChannel):\n",
        "            try:\n",
        "                # Keep receiving until shutdown complete or channel closed\n",
        "                while True:\n",
        "                    key, data = await channel.recv()\n",
        "                    if key == MP_UP_RESPONSE:\n",
        "                        worker_id, msg_key, msg_data = data\n",
        "                        msg = WorkerMessage(worker_id=worker_id, key=msg_key, data=msg_data)\n",
        "                        await self._recv_queue.put(msg)\n",
        "                    elif key == MP_UP_STDOUT_BUFFER:\n",
        "                        # Append received buffer to our local buffer for this process\n",
        "                        self._stdout_buffers[process_idx].extend(data)\n",
        "                        # Call on_output callback if provided\n",
        "                        if self._on_output is not None and data:\n",
        "                            self._on_output(process_idx, data)\n",
        "                        # Signal that flush response was received\n",
        "                        if process_idx in self._flush_events:\n",
        "                            self._flush_events[process_idx].set()\n",
        "                    elif key == MP_UP_SHUTDOWN_COMPLETE:\n",
        "                        # Signal that this subprocess has finished shutdown\n",
        "                        if process_idx in self._shutdown_complete_events:\n",
        "                            self._shutdown_complete_events[process_idx].set()\n",
        "                        # Exit the loop after receiving shutdown complete\n",
        "                        break\n",
        "            except (ChannelClosed, asyncio.CancelledError):\n",
        "                pass\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        for process_idx, channel in enumerate(self._channels):\n",
        "            task = asyncio.create_task(recv_loop(process_idx, channel))\n",
        "            self._recv_tasks.append(task)\n",
        "\n",
        "    async def recv(self, timeout: float | None = None) -> WorkerMessage:\n",
        "        \"\"\"Receive a message from any worker.\n",
        "\n",
        "        Raises:\n",
        "            WorkerException: If the worker raised an exception\n",
        "            WorkerCrashed: If the worker died unexpectedly\n",
        "            WorkerTimeout: If the worker timed out\n",
        "            RecvTimeout: If this recv() call times out\n",
        "        \"\"\"\n",
        "        if not self._running:\n",
        "            raise PoolNotStarted(\"Pool has not been started\")\n",
        "\n",
        "        self._start_recv_tasks()\n",
        "\n",
        "        try:\n",
        "            if timeout is None:\n",
        "                msg = await self._recv_queue.get()\n",
        "            else:\n",
        "                msg = await asyncio.wait_for(\n",
        "                    self._recv_queue.get(),\n",
        "                    timeout=timeout,\n",
        "                )\n",
        "        except TimeoutError:\n",
        "            raise RecvTimeout(f\"Receive timed out after {timeout}s\")\n",
        "\n",
        "        _check_error_and_raise(msg)\n",
        "        return msg\n",
        "\n",
        "    async def try_recv(self) -> WorkerMessage | None:\n",
        "        \"\"\"Non-blocking receive from any worker.\n",
        "\n",
        "        Raises:\n",
        "            WorkerException: If the worker raised an exception\n",
        "            WorkerCrashed: If the worker died unexpectedly\n",
        "            WorkerTimeout: If the worker timed out\n",
        "        \"\"\"\n",
        "        if not self._running:\n",
        "            raise PoolNotStarted(\"Pool has not been started\")\n",
        "\n",
        "        # If recv tasks are running, check the queue first\n",
        "        if self._recv_tasks:\n",
        "            try:\n",
        "                msg = self._recv_queue.get_nowait()\n",
        "                _check_error_and_raise(msg)\n",
        "                return msg\n",
        "            except asyncio.QueueEmpty:\n",
        "                return None\n",
        "\n",
        "        # Otherwise, read directly from channels\n",
        "        for process_idx, channel in enumerate(self._channels):\n",
        "            result = await channel.try_recv()\n",
        "            if result is not None:\n",
        "                key, data = result\n",
        "                if key == MP_UP_RESPONSE:\n",
        "                    worker_id, msg_key, msg_data = data\n",
        "                    msg = WorkerMessage(worker_id=worker_id, key=msg_key, data=msg_data)\n",
        "                    _check_error_and_raise(msg)\n",
        "                    return msg\n",
        "\n",
        "        return None\n",
        "\n",
        "    async def broadcast(self, key: str, data: Any) -> None:\n",
        "        \"\"\"Send a message to all workers.\"\"\"\n",
        "        if not self._running:\n",
        "            raise PoolNotStarted(\"Pool has not been started\")\n",
        "\n",
        "        for channel in self._channels:\n",
        "            await channel.send(MP_DOWN_BROADCAST, (key, data))\n",
        "\n",
        "    async def flush_stdout(self, process_idx: int, timeout: float|None = None) -> OutputBuffer:\n",
        "        \"\"\"Flush and retrieve stdout/stderr buffer from a specific process.\n",
        "\n",
        "        Sends a flush request to the subprocess, waits for the response,\n",
        "        and returns the buffer contents. The subprocess buffer is cleared.\n",
        "\n",
        "        Args:\n",
        "            process_idx: Index of the process (0 to num_processes-1)\n",
        "            timeout: Maximum time to wait for response in seconds\n",
        "\n",
        "        Returns:\n",
        "            List of (timestamp, is_stdout, text) tuples.\n",
        "            is_stdout is True for stdout, False for stderr.\n",
        "\n",
        "        Raises:\n",
        "            PoolNotStarted: If the pool is not running\n",
        "            ValueError: If process_idx is out of range\n",
        "            RecvTimeout: If the subprocess doesn't respond in time\n",
        "        \"\"\"\n",
        "        if not self._running:\n",
        "            raise PoolNotStarted(\"Pool has not been started\")\n",
        "\n",
        "        if process_idx < 0 or process_idx >= self._num_processes:\n",
        "            raise ValueError(f\"process_idx {process_idx} out of range [0, {self._num_processes})\")\n",
        "\n",
        "        # Ensure recv tasks are running to capture the response\n",
        "        self._start_recv_tasks()\n",
        "\n",
        "        # Create an event to wait for the flush response\n",
        "        event = asyncio.Event()\n",
        "        self._flush_events[process_idx] = event\n",
        "\n",
        "        # Send flush request\n",
        "        await self._channels[process_idx].send(MP_DOWN_FLUSH_STDOUT, None)\n",
        "\n",
        "        # Wait for response\n",
        "        try:\n",
        "            await asyncio.wait_for(event.wait(), timeout=timeout)\n",
        "        except TimeoutError:\n",
        "            raise RecvTimeout(f\"flush_stdout timed out after {timeout}s\")\n",
        "        finally:\n",
        "            # Clean up event\n",
        "            self._flush_events.pop(process_idx, None)\n",
        "\n",
        "        # Extract buffer and clear it\n",
        "        result = self._stdout_buffers[process_idx]\n",
        "        self._stdout_buffers[process_idx] = []\n",
        "        return result\n",
        "\n",
        "    async def flush_all_stdout(self, timeout: float|None = None) -> dict[int, OutputBuffer]:\n",
        "        \"\"\"Flush and retrieve stdout/stderr buffers from all processes.\n",
        "\n",
        "        Args:\n",
        "            timeout: Maximum time to wait for each process response\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping process_idx to buffer contents.\n",
        "            Each buffer is a list of (timestamp, is_stdout, text) tuples.\n",
        "\n",
        "        Raises:\n",
        "            PoolNotStarted: If the pool is not running\n",
        "        \"\"\"\n",
        "        if not self._running:\n",
        "            raise PoolNotStarted(\"Pool has not been started\")\n",
        "\n",
        "        # Flush all processes concurrently\n",
        "        tasks = [\n",
        "            self.flush_stdout(process_idx, timeout=timeout)\n",
        "            for process_idx in range(self._num_processes)\n",
        "        ]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "        return {i: result for i, result in enumerate(results)}\n",
        "\n",
        "    async def __aenter__(self) -> \"MultiprocessPool\":\n",
        "        \"\"\"Context manager entry - starts the pool.\"\"\"\n",
        "        await self.start()\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n",
        "        \"\"\"Context manager exit - closes the pool.\"\"\"\n",
        "        await self.close()"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n",
        "\n",
        "Note: For multiprocessing with spawn context, the worker function must be\n",
        "defined at module level (importable). We'll create a temp module for the example."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Create temp module with worker function\n",
        "_temp_dir = tempfile.mkdtemp(prefix=\"pool_example_\")\n",
        "_worker_code = '''\n",
        "\"\"\"Worker functions for multiprocess pool example.\"\"\"\n",
        "from netrun.rpc.base import ChannelClosed\n",
        "\n",
        "def echo_worker(channel, worker_id):\n",
        "    \"\"\"Echo worker that runs in subprocess.\"\"\"\n",
        "    import os\n",
        "    print(f\"[Worker {worker_id}] Started process\")\n",
        "    try:\n",
        "        while True:\n",
        "            key, data = channel.recv()\n",
        "            print(f\"[Worker {worker_id}] Received: {key}={data}\")\n",
        "            channel.send(f\"echo:{key}\", {\"worker_id\": worker_id, \"data\": data})\n",
        "    except ChannelClosed:\n",
        "        print(f\"[Worker {worker_id}] Stopping\")\n",
        "\n",
        "def compute_worker(channel, worker_id):\n",
        "    \"\"\"Compute worker that runs in subprocess.\"\"\"\n",
        "    try:\n",
        "        while True:\n",
        "            key, data = channel.recv()\n",
        "            if key == \"square\":\n",
        "                result = data * data\n",
        "            elif key == \"double\":\n",
        "                result = data * 2\n",
        "            else:\n",
        "                result = f\"unknown: {key}\"\n",
        "            channel.send(\"result\", {\"worker_id\": worker_id, \"result\": result})\n",
        "    except ChannelClosed:\n",
        "        pass\n",
        "'''\n",
        "\n",
        "_worker_path = Path(_temp_dir) / \"mp_workers.py\"\n",
        "_worker_path.write_text(_worker_code)\n",
        "print(f\"Created worker module at: {_worker_path}\")\n",
        "\n",
        "if _temp_dir not in sys.path:\n",
        "    sys.path.insert(0, _temp_dir)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from mp_workers import echo_worker, compute_worker\n",
        "from netrun.pool.multiprocess import MultiprocessPool as _MultiprocessPool\n",
        "\n",
        "async def example_multiprocess_pool():\n",
        "    \"\"\"Example: multiprocess pool with 2 processes, 2 threads each.\"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Example 1: Multiprocess Pool\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    async with _MultiprocessPool(echo_worker, num_processes=2, threads_per_process=2) as pool:\n",
        "        print(f\"Pool has {pool.num_workers} workers across {pool.num_processes} processes\")\n",
        "\n",
        "        # Send to each worker\n",
        "        for worker_id in range(pool.num_workers):\n",
        "            await pool.send(worker_id, \"hello\", f\"message-{worker_id}\")\n",
        "\n",
        "        # Receive all responses\n",
        "        for _ in range(pool.num_workers):\n",
        "            msg = await pool.recv(timeout=5.0)\n",
        "            print(f\"[Main] Got from worker {msg.worker_id}: {msg.key}={msg.data}\")\n",
        "\n",
        "    print(\"Done!\\n\")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "await example_multiprocess_pool()"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "async def example_compute_multiprocess():\n",
        "    \"\"\"Example: compute tasks distributed across processes.\"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Example 2: Distributed Compute\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    async with _MultiprocessPool(compute_worker, num_processes=2, threads_per_process=2) as pool:\n",
        "        # Distribute tasks\n",
        "        tasks = [(\"square\", 5), (\"double\", 10), (\"square\", 7), (\"double\", 3)]\n",
        "\n",
        "        for i, (key, data) in enumerate(tasks):\n",
        "            worker_id = i % pool.num_workers\n",
        "            await pool.send(worker_id, key, data)\n",
        "\n",
        "        # Collect results\n",
        "        for _ in range(len(tasks)):\n",
        "            msg = await pool.recv(timeout=5.0)\n",
        "            print(f\"[Main] Worker {msg.worker_id}: {msg.data}\")\n",
        "\n",
        "    print(\"Done!\\n\")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "await example_compute_multiprocess()"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up temp module\n",
        "import shutil\n",
        "shutil.rmtree(_temp_dir, ignore_errors=True)\n",
        "print(f\"Cleaned up: {_temp_dir}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
