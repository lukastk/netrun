{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#|default_exp pool.multiprocess"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|hide\n",
        "from nblite import nbl_export; nbl_export();"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiprocess Pool\n",
        "\n",
        "A pool of worker processes, each running multiple worker threads.\n",
        "Uses `multiprocessing.Queue` for communication between the parent\n",
        "and subprocesses.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "Parent Process\n",
        "    │\n",
        "    ├── ProcessChannel ──► Subprocess 0\n",
        "    │                         ├── Thread 0 (worker_id=0)\n",
        "    │                         ├── Thread 1 (worker_id=1)\n",
        "    │                         └── Thread 2 (worker_id=2)\n",
        "    │\n",
        "    └── ProcessChannel ──► Subprocess 1\n",
        "                            ├── Thread 0 (worker_id=3)\n",
        "                            ├── Thread 1 (worker_id=4)\n",
        "                            └── Thread 2 (worker_id=5)\n",
        "```\n",
        "\n",
        "Worker IDs are flat integers: `worker_id = process_idx * threads_per_process + thread_idx`\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from netrun.pool.multiprocess import MultiprocessPool\n",
        "\n",
        "# Worker function must be importable (defined at module level)\n",
        "def my_worker(channel, worker_id):\n",
        "    while True:\n",
        "        key, data = channel.recv()\n",
        "        channel.send(\"result\", data * 2)\n",
        "\n",
        "pool = MultiprocessPool(\n",
        "    worker_fn=my_worker,\n",
        "    num_processes=2,\n",
        "    threads_per_process=3,\n",
        ")\n",
        "await pool.start()\n",
        "await pool.send(worker_id=0, key=\"task\", data=10)\n",
        "msg = await pool.recv()\n",
        "await pool.close()\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "import asyncio\n",
        "import queue\n",
        "import threading\n",
        "import multiprocessing as mp\n",
        "from typing import Any\n",
        "\n",
        "from netrun.rpc.base import ChannelClosed, RecvTimeout, SHUTDOWN_KEY\n",
        "from netrun.rpc.process import (\n",
        "    ProcessChannel,\n",
        "    SyncProcessChannel,\n",
        "    create_queue_pair,\n",
        ")\n",
        "from netrun.pool.base import (\n",
        "    WorkerId,\n",
        "    WorkerFn,\n",
        "    WorkerMessage,\n",
        "    PoolNotStarted,\n",
        "    PoolAlreadyStarted,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subprocess Worker Entry Point\n",
        "\n",
        "This function runs in each subprocess. It creates worker threads and\n",
        "routes messages between the parent and workers."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "def _subprocess_main(\n",
        "    parent_send_q: mp.Queue,\n",
        "    parent_recv_q: mp.Queue,\n",
        "    worker_fn: WorkerFn,\n",
        "    num_threads: int,\n",
        "    process_idx: int,\n",
        "    threads_per_process: int,\n",
        "):\n",
        "    \"\"\"Entry point for subprocess. Routes messages to/from worker threads.\"\"\"\n",
        "    # Create channel to parent\n",
        "    parent_channel = SyncProcessChannel(parent_send_q, parent_recv_q)\n",
        "\n",
        "    # Create thread-safe queues for each worker\n",
        "    # worker_queues[thread_idx] = (send_to_worker, recv_from_worker)\n",
        "    worker_send_queues: list[queue.Queue] = [queue.Queue() for _ in range(num_threads)]\n",
        "    response_queue: queue.Queue = queue.Queue()  # All workers send responses here\n",
        "\n",
        "    # Start worker threads\n",
        "    threads = []\n",
        "    for thread_idx in range(num_threads):\n",
        "        worker_id = process_idx * threads_per_process + thread_idx\n",
        "        t = threading.Thread(\n",
        "            target=_thread_worker,\n",
        "            args=(worker_fn, worker_send_queues[thread_idx], response_queue, worker_id),\n",
        "            daemon=True,\n",
        "        )\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "\n",
        "    # Router loop: handle messages from parent and responses from workers\n",
        "    shutdown = False\n",
        "\n",
        "    def response_forwarder():\n",
        "        \"\"\"Forward responses from workers to parent.\"\"\"\n",
        "        while not shutdown:\n",
        "            try:\n",
        "                msg = response_queue.get(timeout=0.1)\n",
        "                if msg is None:\n",
        "                    break\n",
        "                worker_id, key, data = msg\n",
        "                parent_channel.send(\"response\", (worker_id, key, data))\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "            except Exception:\n",
        "                break\n",
        "\n",
        "    # Start response forwarder thread\n",
        "    forwarder = threading.Thread(target=response_forwarder, daemon=True)\n",
        "    forwarder.start()\n",
        "\n",
        "    # Main loop: receive from parent and dispatch to workers\n",
        "    try:\n",
        "        while True:\n",
        "            key, data = parent_channel.recv()\n",
        "\n",
        "            if key == \"__dispatch__\":\n",
        "                thread_idx, msg_key, msg_data = data\n",
        "                worker_send_queues[thread_idx].put((msg_key, msg_data))\n",
        "            elif key == \"__broadcast__\":\n",
        "                msg_key, msg_data = data\n",
        "                for q in worker_send_queues:\n",
        "                    q.put((msg_key, msg_data))\n",
        "            elif key == SHUTDOWN_KEY:\n",
        "                break\n",
        "    except ChannelClosed:\n",
        "        pass\n",
        "    finally:\n",
        "        shutdown = True\n",
        "        # Signal workers to stop\n",
        "        for q in worker_send_queues:\n",
        "            q.put((SHUTDOWN_KEY, None))\n",
        "        # Signal forwarder\n",
        "        response_queue.put(None)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "def _thread_worker(\n",
        "    worker_fn: WorkerFn,\n",
        "    recv_queue: queue.Queue,\n",
        "    response_queue: queue.Queue,\n",
        "    worker_id: WorkerId,\n",
        "):\n",
        "    \"\"\"Run worker function in a thread within subprocess.\"\"\"\n",
        "\n",
        "    class _WorkerChannel:\n",
        "        \"\"\"Adapter that looks like SyncRPCChannel for the worker.\"\"\"\n",
        "        def __init__(self):\n",
        "            self._closed = False\n",
        "\n",
        "        def send(self, key: str, data: Any) -> None:\n",
        "            if self._closed:\n",
        "                raise ChannelClosed(\"Channel is closed\")\n",
        "            response_queue.put((worker_id, key, data))\n",
        "\n",
        "        def recv(self, timeout: float | None = None) -> tuple[str, Any]:\n",
        "            if self._closed:\n",
        "                raise ChannelClosed(\"Channel is closed\")\n",
        "            try:\n",
        "                key, data = recv_queue.get(timeout=timeout)\n",
        "                if key == SHUTDOWN_KEY:\n",
        "                    self._closed = True\n",
        "                    raise ChannelClosed(\"Channel was shut down\")\n",
        "                return key, data\n",
        "            except queue.Empty:\n",
        "                from netrun.rpc.base import RecvTimeout\n",
        "                raise RecvTimeout(f\"Receive timed out after {timeout}s\")\n",
        "\n",
        "        def try_recv(self) -> tuple[str, Any] | None:\n",
        "            if self._closed:\n",
        "                raise ChannelClosed(\"Channel is closed\")\n",
        "            try:\n",
        "                key, data = recv_queue.get_nowait()\n",
        "                if key == SHUTDOWN_KEY:\n",
        "                    self._closed = True\n",
        "                    raise ChannelClosed(\"Channel was shut down\")\n",
        "                return key, data\n",
        "            except queue.Empty:\n",
        "                return None\n",
        "\n",
        "        def close(self) -> None:\n",
        "            self._closed = True\n",
        "\n",
        "        @property\n",
        "        def is_closed(self) -> bool:\n",
        "            return self._closed\n",
        "\n",
        "    channel = _WorkerChannel()\n",
        "    try:\n",
        "        worker_fn(channel, worker_id)\n",
        "    except ChannelClosed:\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        # Send error back\n",
        "        try:\n",
        "            response_queue.put((worker_id, \"__error__\", str(e)))\n",
        "        except Exception:\n",
        "            pass"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiprocessPool"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "class MultiprocessPool:\n",
        "    \"\"\"A pool of worker processes, each running multiple threads.\n",
        "\n",
        "    Messages are routed to specific workers via their worker_id.\n",
        "    Worker IDs are flat integers from 0 to (num_processes * threads_per_process - 1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        worker_fn: WorkerFn,\n",
        "        num_processes: int,\n",
        "        threads_per_process: int = 1,\n",
        "    ):\n",
        "        \"\"\"Create a multiprocess pool.\n",
        "\n",
        "        Args:\n",
        "            worker_fn: Function to run in each worker thread (must be importable)\n",
        "            num_processes: Number of subprocesses to create\n",
        "            threads_per_process: Number of worker threads per subprocess\n",
        "        \"\"\"\n",
        "        if num_processes < 1:\n",
        "            raise ValueError(\"num_processes must be at least 1\")\n",
        "        if threads_per_process < 1:\n",
        "            raise ValueError(\"threads_per_process must be at least 1\")\n",
        "\n",
        "        self._worker_fn = worker_fn\n",
        "        self._num_processes = num_processes\n",
        "        self._threads_per_process = threads_per_process\n",
        "        self._num_workers = num_processes * threads_per_process\n",
        "        self._running = False\n",
        "\n",
        "        # Will be populated on start()\n",
        "        self._channels: list[ProcessChannel] = []\n",
        "        self._processes: list[mp.Process] = []\n",
        "        self._recv_queue: asyncio.Queue = asyncio.Queue()\n",
        "        self._recv_tasks: list[asyncio.Task] = []\n",
        "\n",
        "    @property\n",
        "    def num_workers(self) -> int:\n",
        "        \"\"\"Total number of workers in the pool.\"\"\"\n",
        "        return self._num_workers\n",
        "\n",
        "    @property\n",
        "    def num_processes(self) -> int:\n",
        "        \"\"\"Number of subprocesses.\"\"\"\n",
        "        return self._num_processes\n",
        "\n",
        "    @property\n",
        "    def threads_per_process(self) -> int:\n",
        "        \"\"\"Number of threads per subprocess.\"\"\"\n",
        "        return self._threads_per_process\n",
        "\n",
        "    @property\n",
        "    def is_running(self) -> bool:\n",
        "        \"\"\"Whether the pool has been started.\"\"\"\n",
        "        return self._running\n",
        "\n",
        "    def _worker_id_to_process_thread(self, worker_id: WorkerId) -> tuple[int, int]:\n",
        "        \"\"\"Convert flat worker_id to (process_idx, thread_idx).\"\"\"\n",
        "        process_idx = worker_id // self._threads_per_process\n",
        "        thread_idx = worker_id % self._threads_per_process\n",
        "        return process_idx, thread_idx\n",
        "\n",
        "    async def start(self) -> None:\n",
        "        \"\"\"Start all processes and workers.\"\"\"\n",
        "        if self._running:\n",
        "            raise PoolAlreadyStarted(\"Pool is already running\")\n",
        "\n",
        "        ctx = mp.get_context(\"spawn\")\n",
        "        self._channels = []\n",
        "        self._processes = []\n",
        "\n",
        "        for process_idx in range(self._num_processes):\n",
        "            # Create channel pair\n",
        "            parent_channel, child_queues = create_queue_pair(ctx)\n",
        "            self._channels.append(parent_channel)\n",
        "\n",
        "            # Create and start subprocess\n",
        "            proc = ctx.Process(\n",
        "                target=_subprocess_main,\n",
        "                args=(\n",
        "                    child_queues[0],  # send_q\n",
        "                    child_queues[1],  # recv_q\n",
        "                    self._worker_fn,\n",
        "                    self._threads_per_process,\n",
        "                    process_idx,\n",
        "                    self._threads_per_process,\n",
        "                ),\n",
        "            )\n",
        "            proc.start()\n",
        "            self._processes.append(proc)\n",
        "\n",
        "        self._running = True\n",
        "\n",
        "    async def close(self) -> None:\n",
        "        \"\"\"Shut down all processes and clean up resources.\"\"\"\n",
        "        if not self._running:\n",
        "            return\n",
        "\n",
        "        self._running = False\n",
        "\n",
        "        # Cancel recv tasks first\n",
        "        for task in self._recv_tasks:\n",
        "            if not task.done():\n",
        "                task.cancel()\n",
        "                try:\n",
        "                    await task\n",
        "                except asyncio.CancelledError:\n",
        "                    pass\n",
        "\n",
        "        # Close all channels\n",
        "        for channel in self._channels:\n",
        "            await channel.close()\n",
        "\n",
        "        # Wait for processes to finish\n",
        "        for proc in self._processes:\n",
        "            proc.join(timeout=2.0)\n",
        "            if proc.is_alive():\n",
        "                proc.terminate()\n",
        "\n",
        "        self._channels = []\n",
        "        self._processes = []\n",
        "        self._recv_queue = asyncio.Queue()\n",
        "        self._recv_tasks = []\n",
        "\n",
        "    async def send(self, worker_id: WorkerId, key: str, data: Any) -> None:\n",
        "        \"\"\"Send a message to a specific worker.\"\"\"\n",
        "        if not self._running:\n",
        "            raise PoolNotStarted(\"Pool has not been started\")\n",
        "\n",
        "        if worker_id < 0 or worker_id >= self._num_workers:\n",
        "            raise ValueError(f\"worker_id {worker_id} out of range [0, {self._num_workers})\")\n",
        "\n",
        "        process_idx, thread_idx = self._worker_id_to_process_thread(worker_id)\n",
        "        await self._channels[process_idx].send(\"__dispatch__\", (thread_idx, key, data))\n",
        "\n",
        "    def _start_recv_tasks(self) -> None:\n",
        "        \"\"\"Start background tasks that forward messages to the queue.\"\"\"\n",
        "        if self._recv_tasks:\n",
        "            return\n",
        "\n",
        "        async def recv_loop(process_idx: int, channel: ProcessChannel):\n",
        "            try:\n",
        "                while self._running:\n",
        "                    key, data = await channel.recv()\n",
        "                    if key == \"response\":\n",
        "                        worker_id, msg_key, msg_data = data\n",
        "                        msg = WorkerMessage(worker_id=worker_id, key=msg_key, data=msg_data)\n",
        "                        await self._recv_queue.put(msg)\n",
        "                    elif key == \"__error__\":\n",
        "                        # Put error as a special message\n",
        "                        msg = WorkerMessage(worker_id=-1, key=\"__error__\", data=data)\n",
        "                        await self._recv_queue.put(msg)\n",
        "            except (ChannelClosed, asyncio.CancelledError):\n",
        "                pass\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        for process_idx, channel in enumerate(self._channels):\n",
        "            task = asyncio.create_task(recv_loop(process_idx, channel))\n",
        "            self._recv_tasks.append(task)\n",
        "\n",
        "    async def recv(self, timeout: float | None = None) -> WorkerMessage:\n",
        "        \"\"\"Receive a message from any worker.\"\"\"\n",
        "        if not self._running:\n",
        "            raise PoolNotStarted(\"Pool has not been started\")\n",
        "\n",
        "        self._start_recv_tasks()\n",
        "\n",
        "        try:\n",
        "            if timeout is None:\n",
        "                return await self._recv_queue.get()\n",
        "            else:\n",
        "                return await asyncio.wait_for(\n",
        "                    self._recv_queue.get(),\n",
        "                    timeout=timeout,\n",
        "                )\n",
        "        except TimeoutError:\n",
        "            raise RecvTimeout(f\"Receive timed out after {timeout}s\")\n",
        "\n",
        "    async def try_recv(self) -> WorkerMessage | None:\n",
        "        \"\"\"Non-blocking receive from any worker.\"\"\"\n",
        "        if not self._running:\n",
        "            raise PoolNotStarted(\"Pool has not been started\")\n",
        "\n",
        "        for process_idx, channel in enumerate(self._channels):\n",
        "            result = await channel.try_recv()\n",
        "            if result is not None:\n",
        "                key, data = result\n",
        "                if key == \"response\":\n",
        "                    worker_id, msg_key, msg_data = data\n",
        "                    return WorkerMessage(worker_id=worker_id, key=msg_key, data=msg_data)\n",
        "\n",
        "        return None\n",
        "\n",
        "    async def broadcast(self, key: str, data: Any) -> None:\n",
        "        \"\"\"Send a message to all workers.\"\"\"\n",
        "        if not self._running:\n",
        "            raise PoolNotStarted(\"Pool has not been started\")\n",
        "\n",
        "        for channel in self._channels:\n",
        "            await channel.send(\"__broadcast__\", (key, data))\n",
        "\n",
        "    async def __aenter__(self) -> \"MultiprocessPool\":\n",
        "        \"\"\"Context manager entry - starts the pool.\"\"\"\n",
        "        await self.start()\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n",
        "        \"\"\"Context manager exit - closes the pool.\"\"\"\n",
        "        await self.close()"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n",
        "\n",
        "Note: For multiprocessing with spawn context, the worker function must be\n",
        "defined at module level (importable). We'll create a temp module for the example."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Create temp module with worker function\n",
        "_temp_dir = tempfile.mkdtemp(prefix=\"pool_example_\")\n",
        "_worker_code = '''\n",
        "\"\"\"Worker functions for multiprocess pool example.\"\"\"\n",
        "from netrun.rpc.base import ChannelClosed\n",
        "\n",
        "def echo_worker(channel, worker_id):\n",
        "    \"\"\"Echo worker that runs in subprocess.\"\"\"\n",
        "    import os\n",
        "    print(f\"[Worker {worker_id}] Started in process {os.getpid()}\")\n",
        "    try:\n",
        "        while True:\n",
        "            key, data = channel.recv()\n",
        "            print(f\"[Worker {worker_id}] Received: {key}={data}\")\n",
        "            channel.send(f\"echo:{key}\", {\"worker_id\": worker_id, \"data\": data})\n",
        "    except ChannelClosed:\n",
        "        print(f\"[Worker {worker_id}] Stopping\")\n",
        "\n",
        "def compute_worker(channel, worker_id):\n",
        "    \"\"\"Compute worker that runs in subprocess.\"\"\"\n",
        "    try:\n",
        "        while True:\n",
        "            key, data = channel.recv()\n",
        "            if key == \"square\":\n",
        "                result = data * data\n",
        "            elif key == \"double\":\n",
        "                result = data * 2\n",
        "            else:\n",
        "                result = f\"unknown: {key}\"\n",
        "            channel.send(\"result\", {\"worker_id\": worker_id, \"result\": result})\n",
        "    except ChannelClosed:\n",
        "        pass\n",
        "'''\n",
        "\n",
        "_worker_path = Path(_temp_dir) / \"mp_workers.py\"\n",
        "_worker_path.write_text(_worker_code)\n",
        "print(f\"Created worker module at: {_worker_path}\")\n",
        "\n",
        "if _temp_dir not in sys.path:\n",
        "    sys.path.insert(0, _temp_dir)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from mp_workers import echo_worker, compute_worker\n",
        "from netrun.pool.multiprocess import MultiprocessPool as _MultiprocessPool\n",
        "\n",
        "async def example_multiprocess_pool():\n",
        "    \"\"\"Example: multiprocess pool with 2 processes, 2 threads each.\"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Example 1: Multiprocess Pool\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    async with _MultiprocessPool(echo_worker, num_processes=2, threads_per_process=2) as pool:\n",
        "        print(f\"Pool has {pool.num_workers} workers across {pool.num_processes} processes\")\n",
        "\n",
        "        # Send to each worker\n",
        "        for worker_id in range(pool.num_workers):\n",
        "            await pool.send(worker_id, \"hello\", f\"message-{worker_id}\")\n",
        "\n",
        "        # Receive all responses\n",
        "        for _ in range(pool.num_workers):\n",
        "            msg = await pool.recv(timeout=5.0)\n",
        "            print(f\"[Main] Got from worker {msg.worker_id}: {msg.key}={msg.data}\")\n",
        "\n",
        "    print(\"Done!\\n\")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "await example_multiprocess_pool()"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "async def example_compute_multiprocess():\n",
        "    \"\"\"Example: compute tasks distributed across processes.\"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Example 2: Distributed Compute\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    async with _MultiprocessPool(compute_worker, num_processes=2, threads_per_process=2) as pool:\n",
        "        # Distribute tasks\n",
        "        tasks = [(\"square\", 5), (\"double\", 10), (\"square\", 7), (\"double\", 3)]\n",
        "\n",
        "        for i, (key, data) in enumerate(tasks):\n",
        "            worker_id = i % pool.num_workers\n",
        "            await pool.send(worker_id, key, data)\n",
        "\n",
        "        # Collect results\n",
        "        for _ in range(len(tasks)):\n",
        "            msg = await pool.recv(timeout=5.0)\n",
        "            print(f\"[Main] Worker {msg.worker_id}: {msg.data}\")\n",
        "\n",
        "    print(\"Done!\\n\")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "await example_compute_multiprocess()"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up temp module\n",
        "import shutil\n",
        "shutil.rmtree(_temp_dir, ignore_errors=True)\n",
        "print(f\"Cleaned up: {_temp_dir}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
