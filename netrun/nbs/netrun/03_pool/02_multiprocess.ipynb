{
  "cells": [
    {
      "cell_type": "code",
      "source": "#|default_exp pool.multiprocess",
      "metadata": {},
      "id": "cell0",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|hide\nfrom nblite import nbl_export; nbl_export();",
      "metadata": {},
      "id": "cell1",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Multiprocess Pool\n\nA pool of worker processes, each running multiple worker threads.\nUses `multiprocessing.Queue` for communication between the parent\nand subprocesses.\n\n## Architecture\n\n```\nParent Process\n    \u2502\n    \u251c\u2500\u2500 ProcessChannel \u2500\u2500\u25ba Subprocess 0\n    \u2502                         \u251c\u2500\u2500 Thread 0 (worker_id=0)\n    \u2502                         \u251c\u2500\u2500 Thread 1 (worker_id=1)\n    \u2502                         \u2514\u2500\u2500 Thread 2 (worker_id=2)\n    \u2502\n    \u2514\u2500\u2500 ProcessChannel \u2500\u2500\u25ba Subprocess 1\n                            \u251c\u2500\u2500 Thread 0 (worker_id=3)\n                            \u251c\u2500\u2500 Thread 1 (worker_id=4)\n                            \u2514\u2500\u2500 Thread 2 (worker_id=5)\n```\n\nWorker IDs are flat integers: `worker_id = process_idx * threads_per_process + thread_idx`\n\n## Usage\n\n```python\nfrom netrun.pool.multiprocess import MultiprocessPool\n\n# Worker function must be importable (defined at module level)\ndef my_worker(channel, worker_id):\n    while True:\n        key, data = channel.recv()\n        channel.send(\"result\", data * 2)\n\npool = MultiprocessPool(\n    worker_fn=my_worker,\n    num_processes=2,\n    threads_per_process=3,\n)\nawait pool.start()\nawait pool.send(worker_id=0, key=\"task\", data=10)\nmsg = await pool.recv()\nawait pool.close()\n```",
      "metadata": {},
      "id": "cell2"
    },
    {
      "cell_type": "code",
      "source": "#|export\nimport asyncio\nimport queue\nimport threading\nimport multiprocessing as mp\nfrom typing import Any\n\nfrom netrun.rpc.base import ChannelClosed, RecvTimeout, SHUTDOWN_KEY\nfrom netrun.rpc.process import (\n    ProcessChannel,\n    SyncProcessChannel,\n    create_queue_pair,\n)\nfrom netrun.pool.base import (\n    WorkerId,\n    WorkerFn,\n    WorkerMessage,\n    PoolNotStarted,\n    PoolAlreadyStarted,\n)",
      "metadata": {},
      "id": "cell3",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Subprocess Worker Entry Point\n\nThis function runs in each subprocess. It creates worker threads and\nroutes messages between the parent and workers.",
      "metadata": {},
      "id": "cell4"
    },
    {
      "cell_type": "code",
      "source": "#|export\ndef _subprocess_main(\n    parent_send_q: mp.Queue,\n    parent_recv_q: mp.Queue,\n    worker_fn: WorkerFn,\n    num_threads: int,\n    process_idx: int,\n    threads_per_process: int,\n):\n    \"\"\"Entry point for subprocess. Routes messages to/from worker threads.\"\"\"\n    # Create channel to parent\n    parent_channel = SyncProcessChannel(parent_send_q, parent_recv_q)\n\n    # Create thread-safe queues for each worker\n    # worker_queues[thread_idx] = (send_to_worker, recv_from_worker)\n    worker_send_queues: list[queue.Queue] = [queue.Queue() for _ in range(num_threads)]\n    response_queue: queue.Queue = queue.Queue()  # All workers send responses here\n\n    # Start worker threads\n    threads = []\n    for thread_idx in range(num_threads):\n        worker_id = process_idx * threads_per_process + thread_idx\n        t = threading.Thread(\n            target=_thread_worker,\n            args=(worker_fn, worker_send_queues[thread_idx], response_queue, worker_id),\n            daemon=True,\n        )\n        t.start()\n        threads.append(t)\n\n    # Router loop: handle messages from parent and responses from workers\n    shutdown = False\n\n    def response_forwarder():\n        \"\"\"Forward responses from workers to parent.\"\"\"\n        while not shutdown:\n            try:\n                msg = response_queue.get(timeout=0.1)\n                if msg is None:\n                    break\n                worker_id, key, data = msg\n                parent_channel.send(\"response\", (worker_id, key, data))\n            except queue.Empty:\n                continue\n            except Exception:\n                break\n\n        # Drain any remaining messages before exiting\n        while True:\n            try:\n                msg = response_queue.get_nowait()\n                if msg is None:\n                    break\n                worker_id, key, data = msg\n                parent_channel.send(\"response\", (worker_id, key, data))\n            except queue.Empty:\n                break\n            except Exception:\n                break\n\n    # Start response forwarder thread\n    forwarder = threading.Thread(target=response_forwarder, daemon=True)\n    forwarder.start()\n\n    # Main loop: receive from parent and dispatch to workers\n    try:\n        while True:\n            key, data = parent_channel.recv()\n\n            if key == \"__dispatch__\":\n                thread_idx, msg_key, msg_data = data\n                worker_send_queues[thread_idx].put((msg_key, msg_data))\n            elif key == \"__broadcast__\":\n                msg_key, msg_data = data\n                for q in worker_send_queues:\n                    q.put((msg_key, msg_data))\n            elif key == SHUTDOWN_KEY:\n                break\n    except ChannelClosed:\n        pass\n    finally:\n        # Signal workers to stop\n        for q in worker_send_queues:\n            q.put((SHUTDOWN_KEY, None))\n\n        # Wait for worker threads to finish sending their responses\n        for t in threads:\n            t.join(timeout=2.0)\n\n        # Now signal forwarder to stop (after workers are done)\n        shutdown = True\n        response_queue.put(None)\n\n        # Wait for forwarder to finish draining\n        forwarder.join(timeout=2.0)",
      "metadata": {},
      "id": "cell5",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#|export\ndef _thread_worker(\n    worker_fn: WorkerFn,\n    recv_queue: queue.Queue,\n    response_queue: queue.Queue,\n    worker_id: WorkerId,\n):\n    \"\"\"Run worker function in a thread within subprocess.\"\"\"\n\n    class _WorkerChannel:\n        \"\"\"Adapter that looks like SyncRPCChannel for the worker.\"\"\"\n        def __init__(self):\n            self._closed = False\n\n        def send(self, key: str, data: Any) -> None:\n            if self._closed:\n                raise ChannelClosed(\"Channel is closed\")\n            response_queue.put((worker_id, key, data))\n\n        def recv(self, timeout: float | None = None) -> tuple[str, Any]:\n            if self._closed:\n                raise ChannelClosed(\"Channel is closed\")\n            try:\n                key, data = recv_queue.get(timeout=timeout)\n                if key == SHUTDOWN_KEY:\n                    self._closed = True\n                    raise ChannelClosed(\"Channel was shut down\")\n                return key, data\n            except queue.Empty:\n                from netrun.rpc.base import RecvTimeout\n                raise RecvTimeout(f\"Receive timed out after {timeout}s\")\n\n        def try_recv(self) -> tuple[str, Any] | None:\n            if self._closed:\n                raise ChannelClosed(\"Channel is closed\")\n            try:\n                key, data = recv_queue.get_nowait()\n                if key == SHUTDOWN_KEY:\n                    self._closed = True\n                    raise ChannelClosed(\"Channel was shut down\")\n                return key, data\n            except queue.Empty:\n                return None\n\n        def close(self) -> None:\n            self._closed = True\n\n        @property\n        def is_closed(self) -> bool:\n            return self._closed\n\n    channel = _WorkerChannel()\n    try:\n        worker_fn(channel, worker_id)\n    except ChannelClosed:\n        pass\n    except Exception as e:\n        # Send error back\n        try:\n            response_queue.put((worker_id, \"__error__\", str(e)))\n        except Exception:\n            pass",
      "metadata": {},
      "id": "cell6",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## MultiprocessPool",
      "metadata": {},
      "id": "cell7"
    },
    {
      "cell_type": "code",
      "source": "#|export\nclass MultiprocessPool:\n    \"\"\"A pool of worker processes, each running multiple threads.\n\n    Messages are routed to specific workers via their worker_id.\n    Worker IDs are flat integers from 0 to (num_processes * threads_per_process - 1).\n    \"\"\"\n\n    def __init__(\n        self,\n        worker_fn: WorkerFn,\n        num_processes: int,\n        threads_per_process: int = 1,\n    ):\n        \"\"\"Create a multiprocess pool.\n\n        Args:\n            worker_fn: Function to run in each worker thread (must be importable)\n            num_processes: Number of subprocesses to create\n            threads_per_process: Number of worker threads per subprocess\n        \"\"\"\n        if num_processes < 1:\n            raise ValueError(\"num_processes must be at least 1\")\n        if threads_per_process < 1:\n            raise ValueError(\"threads_per_process must be at least 1\")\n\n        self._worker_fn = worker_fn\n        self._num_processes = num_processes\n        self._threads_per_process = threads_per_process\n        self._num_workers = num_processes * threads_per_process\n        self._running = False\n\n        # Will be populated on start()\n        self._channels: list[ProcessChannel] = []\n        self._processes: list[mp.Process] = []\n        self._recv_queue: asyncio.Queue = asyncio.Queue()\n        self._recv_tasks: list[asyncio.Task] = []\n\n    @property\n    def num_workers(self) -> int:\n        \"\"\"Total number of workers in the pool.\"\"\"\n        return self._num_workers\n\n    @property\n    def num_processes(self) -> int:\n        \"\"\"Number of subprocesses.\"\"\"\n        return self._num_processes\n\n    @property\n    def threads_per_process(self) -> int:\n        \"\"\"Number of threads per subprocess.\"\"\"\n        return self._threads_per_process\n\n    @property\n    def is_running(self) -> bool:\n        \"\"\"Whether the pool has been started.\"\"\"\n        return self._running\n\n    def _worker_id_to_process_thread(self, worker_id: WorkerId) -> tuple[int, int]:\n        \"\"\"Convert flat worker_id to (process_idx, thread_idx).\"\"\"\n        process_idx = worker_id // self._threads_per_process\n        thread_idx = worker_id % self._threads_per_process\n        return process_idx, thread_idx\n\n    async def start(self) -> None:\n        \"\"\"Start all processes and workers.\"\"\"\n        if self._running:\n            raise PoolAlreadyStarted(\"Pool is already running\")\n\n        ctx = mp.get_context(\"spawn\")\n        self._channels = []\n        self._processes = []\n\n        for process_idx in range(self._num_processes):\n            # Create channel pair\n            parent_channel, child_queues = create_queue_pair(ctx)\n            self._channels.append(parent_channel)\n\n            # Create and start subprocess\n            proc = ctx.Process(\n                target=_subprocess_main,\n                args=(\n                    child_queues[0],  # send_q\n                    child_queues[1],  # recv_q\n                    self._worker_fn,\n                    self._threads_per_process,\n                    process_idx,\n                    self._threads_per_process,\n                ),\n            )\n            proc.start()\n            self._processes.append(proc)\n\n        self._running = True\n\n    async def close(self) -> None:\n        \"\"\"Shut down all processes and clean up resources.\"\"\"\n        if not self._running:\n            return\n\n        self._running = False\n\n        # Close channels first - this unblocks any recv() calls\n        for channel in self._channels:\n            await channel.close()\n\n        # Now cancel recv tasks (they should exit quickly since channels are closed)\n        for task in self._recv_tasks:\n            if not task.done():\n                task.cancel()\n                try:\n                    await task\n                except asyncio.CancelledError:\n                    pass\n\n        # Wait for processes to finish\n        for proc in self._processes:\n            proc.join(timeout=2.0)\n            if proc.is_alive():\n                proc.terminate()\n\n        self._channels = []\n        self._processes = []\n        self._recv_queue = asyncio.Queue()\n        self._recv_tasks = []\n\n    async def send(self, worker_id: WorkerId, key: str, data: Any) -> None:\n        \"\"\"Send a message to a specific worker.\"\"\"\n        if not self._running:\n            raise PoolNotStarted(\"Pool has not been started\")\n\n        if worker_id < 0 or worker_id >= self._num_workers:\n            raise ValueError(f\"worker_id {worker_id} out of range [0, {self._num_workers})\")\n\n        process_idx, thread_idx = self._worker_id_to_process_thread(worker_id)\n        await self._channels[process_idx].send(\"__dispatch__\", (thread_idx, key, data))\n\n    def _start_recv_tasks(self) -> None:\n        \"\"\"Start background tasks that forward messages to the queue.\"\"\"\n        if self._recv_tasks:\n            return\n\n        async def recv_loop(process_idx: int, channel: ProcessChannel):\n            try:\n                while self._running:\n                    key, data = await channel.recv()\n                    if key == \"response\":\n                        worker_id, msg_key, msg_data = data\n                        msg = WorkerMessage(worker_id=worker_id, key=msg_key, data=msg_data)\n                        await self._recv_queue.put(msg)\n                    elif key == \"__error__\":\n                        # Put error as a special message\n                        msg = WorkerMessage(worker_id=-1, key=\"__error__\", data=data)\n                        await self._recv_queue.put(msg)\n            except (ChannelClosed, asyncio.CancelledError):\n                pass\n            except Exception:\n                pass\n\n        for process_idx, channel in enumerate(self._channels):\n            task = asyncio.create_task(recv_loop(process_idx, channel))\n            self._recv_tasks.append(task)\n\n    async def recv(self, timeout: float | None = None) -> WorkerMessage:\n        \"\"\"Receive a message from any worker.\"\"\"\n        if not self._running:\n            raise PoolNotStarted(\"Pool has not been started\")\n\n        self._start_recv_tasks()\n\n        try:\n            if timeout is None:\n                return await self._recv_queue.get()\n            else:\n                return await asyncio.wait_for(\n                    self._recv_queue.get(),\n                    timeout=timeout,\n                )\n        except TimeoutError:\n            raise RecvTimeout(f\"Receive timed out after {timeout}s\")\n\n    async def try_recv(self) -> WorkerMessage | None:\n        \"\"\"Non-blocking receive from any worker.\"\"\"\n        if not self._running:\n            raise PoolNotStarted(\"Pool has not been started\")\n\n        for process_idx, channel in enumerate(self._channels):\n            result = await channel.try_recv()\n            if result is not None:\n                key, data = result\n                if key == \"response\":\n                    worker_id, msg_key, msg_data = data\n                    return WorkerMessage(worker_id=worker_id, key=msg_key, data=msg_data)\n\n        return None\n\n    async def broadcast(self, key: str, data: Any) -> None:\n        \"\"\"Send a message to all workers.\"\"\"\n        if not self._running:\n            raise PoolNotStarted(\"Pool has not been started\")\n\n        for channel in self._channels:\n            await channel.send(\"__broadcast__\", (key, data))\n\n    async def __aenter__(self) -> \"MultiprocessPool\":\n        \"\"\"Context manager entry - starts the pool.\"\"\"\n        await self.start()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n        \"\"\"Context manager exit - closes the pool.\"\"\"\n        await self.close()",
      "metadata": {},
      "id": "cell8",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Example\n\nNote: For multiprocessing with spawn context, the worker function must be\ndefined at module level (importable). We'll create a temp module for the example.",
      "metadata": {},
      "id": "cell9"
    },
    {
      "cell_type": "code",
      "source": "import tempfile\nimport sys\nfrom pathlib import Path\n\n# Create temp module with worker function\n_temp_dir = tempfile.mkdtemp(prefix=\"pool_example_\")\n_worker_code = '''\n\"\"\"Worker functions for multiprocess pool example.\"\"\"\nfrom netrun.rpc.base import ChannelClosed\n\ndef echo_worker(channel, worker_id):\n    \"\"\"Echo worker that runs in subprocess.\"\"\"\n    import os\n    print(f\"[Worker {worker_id}] Started process\")\n    try:\n        while True:\n            key, data = channel.recv()\n            print(f\"[Worker {worker_id}] Received: {key}={data}\")\n            channel.send(f\"echo:{key}\", {\"worker_id\": worker_id, \"data\": data})\n    except ChannelClosed:\n        print(f\"[Worker {worker_id}] Stopping\")\n\ndef compute_worker(channel, worker_id):\n    \"\"\"Compute worker that runs in subprocess.\"\"\"\n    try:\n        while True:\n            key, data = channel.recv()\n            if key == \"square\":\n                result = data * data\n            elif key == \"double\":\n                result = data * 2\n            else:\n                result = f\"unknown: {key}\"\n            channel.send(\"result\", {\"worker_id\": worker_id, \"result\": result})\n    except ChannelClosed:\n        pass\n'''\n\n_worker_path = Path(_temp_dir) / \"mp_workers.py\"\n_worker_path.write_text(_worker_code)\nprint(f\"Created worker module at: {_worker_path}\")\n\nif _temp_dir not in sys.path:\n    sys.path.insert(0, _temp_dir)",
      "metadata": {},
      "id": "cell10",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created worker module at: /var/folders/gc/wx3wyw4538vdtn_n2dyjn80m0000gn/T/pool_example_pf1nf2tm/mp_workers.py\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from mp_workers import echo_worker, compute_worker\nfrom netrun.pool.multiprocess import MultiprocessPool as _MultiprocessPool\n\nasync def example_multiprocess_pool():\n    \"\"\"Example: multiprocess pool with 2 processes, 2 threads each.\"\"\"\n    print(\"=\" * 50)\n    print(\"Example 1: Multiprocess Pool\")\n    print(\"=\" * 50)\n\n    async with _MultiprocessPool(echo_worker, num_processes=2, threads_per_process=2) as pool:\n        print(f\"Pool has {pool.num_workers} workers across {pool.num_processes} processes\")\n\n        # Send to each worker\n        for worker_id in range(pool.num_workers):\n            await pool.send(worker_id, \"hello\", f\"message-{worker_id}\")\n\n        # Receive all responses\n        for _ in range(pool.num_workers):\n            msg = await pool.recv(timeout=5.0)\n            print(f\"[Main] Got from worker {msg.worker_id}: {msg.key}={msg.data}\")\n\n    print(\"Done!\\n\")",
      "metadata": {},
      "id": "cell11",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "await example_multiprocess_pool()",
      "metadata": {},
      "id": "cell12",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Example 1: Multiprocess Pool\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pool has 4 workers across 2 processes\n",
            "[Main] Got from worker 0: echo:hello={'worker_id': 0, 'data': 'message-0'}\n",
            "[Main] Got from worker 1: echo:hello={'worker_id': 1, 'data': 'message-1'}\n",
            "[Main] Got from worker 2: echo:hello={'worker_id': 2, 'data': 'message-2'}\n",
            "[Main] Got from worker 3: echo:hello={'worker_id': 3, 'data': 'message-3'}\n",
            "[Worker 2] Started process\n",
            "[Worker 3] Started process\n",
            "[Worker 2] Received: hello=message-2\n",
            "[Worker 3] Received: hello=message-3\n",
            "[Worker 3] Stopping\n",
            "[Worker 2] Stopping\n",
            "[Worker 0] Started process\n",
            "[Worker 1] Started process\n",
            "[Worker 0] Received: hello=message-0\n",
            "[Worker 1] Received: hello=message-1\n",
            "[Worker 1] Stopping\n",
            "[Worker 0] Stopping\n",
            "Done!\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "async def example_compute_multiprocess():\n    \"\"\"Example: compute tasks distributed across processes.\"\"\"\n    print(\"=\" * 50)\n    print(\"Example 2: Distributed Compute\")\n    print(\"=\" * 50)\n\n    async with _MultiprocessPool(compute_worker, num_processes=2, threads_per_process=2) as pool:\n        # Distribute tasks\n        tasks = [(\"square\", 5), (\"double\", 10), (\"square\", 7), (\"double\", 3)]\n\n        for i, (key, data) in enumerate(tasks):\n            worker_id = i % pool.num_workers\n            await pool.send(worker_id, key, data)\n\n        # Collect results\n        for _ in range(len(tasks)):\n            msg = await pool.recv(timeout=5.0)\n            print(f\"[Main] Worker {msg.worker_id}: {msg.data}\")\n\n    print(\"Done!\\n\")",
      "metadata": {},
      "id": "cell13",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "await example_compute_multiprocess()",
      "metadata": {},
      "id": "cell14",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Example 2: Distributed Compute\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Main] Worker 0: {'worker_id': 0, 'result': 25}\n",
            "[Main] Worker 1: {'worker_id': 1, 'result': 20}\n",
            "[Main] Worker 2: {'worker_id': 2, 'result': 49}\n",
            "[Main] Worker 3: {'worker_id': 3, 'result': 6}\n",
            "Done!\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Clean up temp module\nimport shutil\nshutil.rmtree(_temp_dir, ignore_errors=True)\nprint(f\"Cleaned up: {_temp_dir}\")",
      "metadata": {},
      "id": "cell15",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned up: /var/folders/gc/wx3wyw4538vdtn_n2dyjn80m0000gn/T/pool_example_pf1nf2tm\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "nblite_source_hash": "22b3ee14f69486473bdc8d213dbd974c9b5103384ca6b9cefd0478ec51d38f39"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
