{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Example 11: Complete Application - Data Processing Pipeline\n",
        "\n",
        "This example demonstrates a complete, real-world data processing pipeline that\n",
        "combines all features from the previous milestones:\n",
        "\n",
        "- **TOML DSL**: Define the network structure in a human-readable format\n",
        "- **Thread Pools**: Parallel processing with configurable pools\n",
        "- **Rate Limiting**: Control processing rate to avoid overwhelming downstream systems\n",
        "- **Error Handling**: Retries with dead letter queue for failed items\n",
        "- **Data Types**: Structured data with dataclasses\n",
        "- **History/Logging**: Track all operations for debugging\n",
        "- **Checkpointing**: Save and resume pipeline state\n",
        "\n",
        "## The Application: ETL Data Pipeline\n",
        "\n",
        "We'll build an Extract-Transform-Load (ETL) pipeline that:\n",
        "1. **Extracts** data records from a source\n",
        "2. **Validates** records (routing invalid ones to error handling)\n",
        "3. **Transforms** valid records (enrichment, normalization)\n",
        "4. **Loads** transformed records to a destination\n",
        "\n",
        "```\n",
        "                   /--> Transformer --> Loader\n",
        "Extractor --> Validator\n",
        "                   \\--> ErrorHandler\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|default_exp 11_complete_application"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "import tempfile\n",
        "import time\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from netrun import (\n",
        "    # Graph building\n",
        "    Graph,\n",
        "    Node,\n",
        "    Edge,\n",
        "    Port,\n",
        "    PortType,\n",
        "    PortRef,\n",
        "    PortState,\n",
        "    PortSlotSpec,\n",
        "    MaxSalvos,\n",
        "    SalvoCondition,\n",
        "    SalvoConditionTerm,\n",
        "    # Net and configuration\n",
        "    Net,\n",
        "    NetState,\n",
        "    NodeConfig,\n",
        "    # DSL\n",
        "    parse_toml_string,\n",
        "    net_config_to_toml,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Define the Pipeline Using TOML DSL\n",
        "\n",
        "We define our ETL pipeline using the TOML DSL format for clarity and portability."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "ETL_PIPELINE_TOML = '''\n",
        "[net]\n",
        "on_error = \"continue\"\n",
        "\n",
        "[net.thread_pools.workers]\n",
        "size = 4\n",
        "\n",
        "[net.thread_pools.io]\n",
        "size = 2\n",
        "\n",
        "# =============================================================================\n",
        "# Extractor Node - Generates data records\n",
        "# =============================================================================\n",
        "[nodes.Extractor]\n",
        "out_ports = { out = {} }\n",
        "\n",
        "[nodes.Extractor.out_salvo_conditions.send]\n",
        "max_salvos = \"infinite\"\n",
        "ports = \"out\"\n",
        "when = \"nonempty(out)\"\n",
        "\n",
        "# =============================================================================\n",
        "# Validator Node - Validates records, routes to transform or error\n",
        "# =============================================================================\n",
        "[nodes.Validator]\n",
        "in_ports = { in = {} }\n",
        "out_ports = { valid = {}, invalid = {} }\n",
        "\n",
        "[nodes.Validator.in_salvo_conditions.receive]\n",
        "max_salvos = 1\n",
        "ports = \"in\"\n",
        "when = \"nonempty(in)\"\n",
        "\n",
        "[nodes.Validator.out_salvo_conditions.send_valid]\n",
        "max_salvos = \"infinite\"\n",
        "ports = \"valid\"\n",
        "when = \"nonempty(valid)\"\n",
        "\n",
        "[nodes.Validator.out_salvo_conditions.send_invalid]\n",
        "max_salvos = \"infinite\"\n",
        "ports = \"invalid\"\n",
        "when = \"nonempty(invalid)\"\n",
        "\n",
        "[nodes.Validator.options]\n",
        "pool = \"workers\"\n",
        "\n",
        "# =============================================================================\n",
        "# Transformer Node - Enriches and normalizes data\n",
        "# =============================================================================\n",
        "[nodes.Transformer]\n",
        "in_ports = { in = { slots = 10 } }\n",
        "out_ports = { out = {} }\n",
        "\n",
        "[nodes.Transformer.in_salvo_conditions.receive]\n",
        "max_salvos = 1\n",
        "ports = \"in\"\n",
        "when = \"nonempty(in)\"\n",
        "\n",
        "[nodes.Transformer.out_salvo_conditions.send]\n",
        "max_salvos = \"infinite\"\n",
        "ports = \"out\"\n",
        "when = \"nonempty(out)\"\n",
        "\n",
        "[nodes.Transformer.options]\n",
        "pool = \"workers\"\n",
        "retries = 3\n",
        "defer_net_actions = true\n",
        "\n",
        "# =============================================================================\n",
        "# Loader Node - Loads data to destination\n",
        "# =============================================================================\n",
        "[nodes.Loader]\n",
        "in_ports = { in = {} }\n",
        "\n",
        "[nodes.Loader.in_salvo_conditions.receive]\n",
        "max_salvos = 1\n",
        "ports = \"in\"\n",
        "when = \"nonempty(in)\"\n",
        "\n",
        "[nodes.Loader.options]\n",
        "pool = \"io\"\n",
        "rate_limit_per_second = 100\n",
        "\n",
        "# =============================================================================\n",
        "# ErrorHandler Node - Handles invalid/failed records\n",
        "# =============================================================================\n",
        "[nodes.ErrorHandler]\n",
        "in_ports = { in = {} }\n",
        "\n",
        "[nodes.ErrorHandler.in_salvo_conditions.receive]\n",
        "max_salvos = 1\n",
        "ports = \"in\"\n",
        "when = \"nonempty(in)\"\n",
        "\n",
        "# =============================================================================\n",
        "# Edges\n",
        "# =============================================================================\n",
        "[[edges]]\n",
        "from = \"Extractor.out\"\n",
        "to = \"Validator.in\"\n",
        "\n",
        "[[edges]]\n",
        "from = \"Validator.valid\"\n",
        "to = \"Transformer.in\"\n",
        "\n",
        "[[edges]]\n",
        "from = \"Validator.invalid\"\n",
        "to = \"ErrorHandler.in\"\n",
        "\n",
        "[[edges]]\n",
        "from = \"Transformer.out\"\n",
        "to = \"Loader.in\"\n",
        "'''\n",
        "\n",
        "# Parse the TOML definition\n",
        "config = parse_toml_string(ETL_PIPELINE_TOML)\n",
        "print(\"Pipeline configuration loaded from TOML:\")\n",
        "print(f\"  Nodes: {list(config.graph.nodes().keys())}\")\n",
        "print(f\"  Edges: {len(config.graph.edges())}\")\n",
        "print(f\"  Thread pools: {list(config.thread_pools.keys())}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Define Data Types\n",
        "\n",
        "We use dataclasses to represent our data records with type checking."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "@dataclass\n",
        "class RawRecord:\n",
        "    \"\"\"A raw data record from the source.\"\"\"\n",
        "    id: int\n",
        "    name: str\n",
        "    value: float\n",
        "    category: Optional[str] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ValidatedRecord:\n",
        "    \"\"\"A validated record ready for transformation.\"\"\"\n",
        "    id: int\n",
        "    name: str\n",
        "    value: float\n",
        "    category: str\n",
        "    validation_timestamp: float\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TransformedRecord:\n",
        "    \"\"\"A fully transformed record ready for loading.\"\"\"\n",
        "    id: int\n",
        "    name: str\n",
        "    normalized_value: float\n",
        "    category: str\n",
        "    enriched_data: Dict[str, Any]\n",
        "    transform_timestamp: float\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ErrorRecord:\n",
        "    \"\"\"A record that failed processing.\"\"\"\n",
        "    original_record: Any\n",
        "    error_type: str\n",
        "    error_message: str\n",
        "    timestamp: float"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Create the Net and Define Execution Functions\n",
        "\n",
        "We'll create execution functions for each node demonstrating different features."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "# Create a temporary directory for history and checkpoints\n",
        "temp_dir = tempfile.mkdtemp(prefix=\"etl_pipeline_\")\n",
        "history_file = Path(temp_dir) / \"history.jsonl\"\n",
        "checkpoint_dir = Path(temp_dir) / \"checkpoint\"\n",
        "\n",
        "print(f\"Working directory: {temp_dir}\")\n",
        "\n",
        "# Create the Net from the parsed config\n",
        "net = Net(\n",
        "    config.graph,\n",
        "    on_error=config.on_error,\n",
        "    thread_pools=config.thread_pools,\n",
        "    history_file=str(history_file),\n",
        "    consumed_packet_storage=True,\n",
        "    consumed_packet_storage_limit=1000,\n",
        ")\n",
        "\n",
        "# Apply node configs from TOML\n",
        "for node_name, node_config in config.node_configs.items():\n",
        "    net.set_node_config(node_name, **node_config)\n",
        "\n",
        "# Storage for results and metrics\n",
        "loaded_records: List[TransformedRecord] = []\n",
        "error_records: List[ErrorRecord] = []\n",
        "metrics = {\n",
        "    \"extracted\": 0,\n",
        "    \"validated\": 0,\n",
        "    \"invalid\": 0,\n",
        "    \"transformed\": 0,\n",
        "    \"loaded\": 0,\n",
        "    \"errors\": 0,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extractor Node\n",
        "Generates sample data records for the pipeline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "def extractor_exec(ctx, packets):\n",
        "    \"\"\"Extract data records from source.\"\"\"\n",
        "    # Simulate extracting records from a data source\n",
        "    sample_records = [\n",
        "        RawRecord(1, \"Alice\", 100.5, \"A\"),\n",
        "        RawRecord(2, \"Bob\", 200.0, \"B\"),\n",
        "        RawRecord(3, \"Charlie\", -50.0, None),  # Invalid: negative value\n",
        "        RawRecord(4, \"Diana\", 150.75, \"A\"),\n",
        "        RawRecord(5, \"Eve\", 0.0, \"\"),  # Invalid: empty category\n",
        "        RawRecord(6, \"Frank\", 300.25, \"C\"),\n",
        "        RawRecord(7, \"Grace\", 175.0, \"B\"),\n",
        "        RawRecord(8, \"Henry\", 999.99, \"A\"),\n",
        "    ]\n",
        "\n",
        "    print(f\"[Extractor] Extracting {len(sample_records)} records...\")\n",
        "\n",
        "    for record in sample_records:\n",
        "        pkt = ctx.create_packet(record)\n",
        "        ctx.load_output_port(\"out\", pkt)\n",
        "        ctx.send_output_salvo(\"send\")\n",
        "        metrics[\"extracted\"] += 1\n",
        "\n",
        "    print(f\"[Extractor] Sent {len(sample_records)} records to validation\")\n",
        "\n",
        "net.set_node_exec(\"Extractor\", extractor_exec)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validator Node\n",
        "Validates records and routes them to either transformation or error handling."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "def validator_exec(ctx, packets):\n",
        "    \"\"\"Validate incoming records.\"\"\"\n",
        "    for port_name, pkts in packets.items():\n",
        "        for pkt in pkts:\n",
        "            record: RawRecord = ctx.consume_packet(pkt)\n",
        "\n",
        "            # Validation rules\n",
        "            errors = []\n",
        "            if record.value < 0:\n",
        "                errors.append(\"Value must be non-negative\")\n",
        "            if not record.category:\n",
        "                errors.append(\"Category is required\")\n",
        "\n",
        "            if errors:\n",
        "                # Invalid record - send to error handler\n",
        "                error_record = ErrorRecord(\n",
        "                    original_record=record,\n",
        "                    error_type=\"validation_error\",\n",
        "                    error_message=\"; \".join(errors),\n",
        "                    timestamp=time.time(),\n",
        "                )\n",
        "                out_pkt = ctx.create_packet(error_record)\n",
        "                ctx.load_output_port(\"invalid\", out_pkt)\n",
        "                ctx.send_output_salvo(\"send_invalid\")\n",
        "                metrics[\"invalid\"] += 1\n",
        "                print(f\"[Validator] Record {record.id} invalid: {errors}\")\n",
        "            else:\n",
        "                # Valid record - send to transformer\n",
        "                validated = ValidatedRecord(\n",
        "                    id=record.id,\n",
        "                    name=record.name,\n",
        "                    value=record.value,\n",
        "                    category=record.category,\n",
        "                    validation_timestamp=time.time(),\n",
        "                )\n",
        "                out_pkt = ctx.create_packet(validated)\n",
        "                ctx.load_output_port(\"valid\", out_pkt)\n",
        "                ctx.send_output_salvo(\"send_valid\")\n",
        "                metrics[\"validated\"] += 1\n",
        "                print(f\"[Validator] Record {record.id} validated\")\n",
        "\n",
        "net.set_node_exec(\"Validator\", validator_exec)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Node\n",
        "Transforms records with enrichment and normalization."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "def transformer_exec(ctx, packets):\n",
        "    \"\"\"Transform and enrich validated records.\"\"\"\n",
        "    for port_name, pkts in packets.items():\n",
        "        for pkt in pkts:\n",
        "            record: ValidatedRecord = ctx.consume_packet(pkt)\n",
        "\n",
        "            # Normalize value (scale to 0-1 range, assuming max of 1000)\n",
        "            normalized_value = min(record.value / 1000.0, 1.0)\n",
        "\n",
        "            # Enrich with additional computed data\n",
        "            enriched_data = {\n",
        "                \"value_tier\": \"high\" if record.value > 200 else \"medium\" if record.value > 100 else \"low\",\n",
        "                \"category_code\": ord(record.category[0]) if record.category else 0,\n",
        "                \"processing_node\": ctx.node_name,\n",
        "            }\n",
        "\n",
        "            transformed = TransformedRecord(\n",
        "                id=record.id,\n",
        "                name=record.name,\n",
        "                normalized_value=normalized_value,\n",
        "                category=record.category,\n",
        "                enriched_data=enriched_data,\n",
        "                transform_timestamp=time.time(),\n",
        "            )\n",
        "\n",
        "            out_pkt = ctx.create_packet(transformed)\n",
        "            ctx.load_output_port(\"out\", out_pkt)\n",
        "            ctx.send_output_salvo(\"send\")\n",
        "            metrics[\"transformed\"] += 1\n",
        "            print(f\"[Transformer] Record {record.id} transformed (tier: {enriched_data['value_tier']})\")\n",
        "\n",
        "net.set_node_exec(\"Transformer\", transformer_exec)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loader Node\n",
        "Loads transformed records to the destination."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "def loader_exec(ctx, packets):\n",
        "    \"\"\"Load transformed records to destination.\"\"\"\n",
        "    for port_name, pkts in packets.items():\n",
        "        for pkt in pkts:\n",
        "            record: TransformedRecord = ctx.consume_packet(pkt)\n",
        "            loaded_records.append(record)\n",
        "            metrics[\"loaded\"] += 1\n",
        "            print(f\"[Loader] Record {record.id} loaded to destination\")\n",
        "\n",
        "net.set_node_exec(\"Loader\", loader_exec)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error Handler Node\n",
        "Handles invalid and failed records."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "def error_handler_exec(ctx, packets):\n",
        "    \"\"\"Handle error records.\"\"\"\n",
        "    for port_name, pkts in packets.items():\n",
        "        for pkt in pkts:\n",
        "            record: ErrorRecord = ctx.consume_packet(pkt)\n",
        "            error_records.append(record)\n",
        "            metrics[\"errors\"] += 1\n",
        "            print(f\"[ErrorHandler] Captured error for record: {record.error_message}\")\n",
        "\n",
        "net.set_node_exec(\"ErrorHandler\", error_handler_exec)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Run the Pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Starting ETL Pipeline\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Inject the extractor epoch to start the pipeline\n",
        "net.inject_source_epoch(\"Extractor\")\n",
        "\n",
        "# Run the pipeline\n",
        "start_time = time.time()\n",
        "net.start()\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Pipeline Complete\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Review Results"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "print(\"\\n--- Pipeline Metrics ---\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"  {metric}: {value}\")\n",
        "print(f\"  elapsed_time: {elapsed:.3f}s\")\n",
        "\n",
        "print(\"\\n--- Successfully Loaded Records ---\")\n",
        "for record in loaded_records:\n",
        "    print(f\"  ID={record.id}, Name={record.name}, \"\n",
        "          f\"NormValue={record.normalized_value:.3f}, \"\n",
        "          f\"Tier={record.enriched_data['value_tier']}\")\n",
        "\n",
        "print(\"\\n--- Error Records ---\")\n",
        "for record in error_records:\n",
        "    orig = record.original_record\n",
        "    print(f\"  ID={orig.id}, Error: {record.error_message}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Demonstrate Checkpointing\n",
        "\n",
        "We can save the pipeline state and configuration for later resumption."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "print(\"\\n--- Demonstrating Checkpointing ---\")\n",
        "\n",
        "# Pause the net before saving checkpoint\n",
        "net.pause()\n",
        "print(f\"Net paused. State: {net.state}\")\n",
        "\n",
        "# Save the net definition (without runtime state)\n",
        "definition_path = Path(temp_dir) / \"pipeline_definition.toml\"\n",
        "net.save_definition(definition_path)\n",
        "print(f\"Definition saved to: {definition_path}\")\n",
        "\n",
        "# Read back and display a snippet\n",
        "with open(definition_path) as f:\n",
        "    toml_content = f.read()\n",
        "print(f\"\\nSaved TOML definition ({len(toml_content)} chars):\")\n",
        "print(toml_content[:500] + \"...\" if len(toml_content) > 500 else toml_content)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Verify History Recording\n",
        "\n",
        "The pipeline records all events for debugging and auditing."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "print(\"\\n--- Event History ---\")\n",
        "history = net._event_history\n",
        "print(f\"Total entries in memory: {len(history)}\")\n",
        "\n",
        "# Show recent entries\n",
        "entries = history.get_entries(limit=5)\n",
        "if entries:\n",
        "    print(\"Recent entries:\")\n",
        "    for entry in entries:\n",
        "        print(f\"  [{entry.entry_type}] {entry.event_type or entry.action_type}: {entry.id[:8]}...\")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This complete application demonstrated:\n",
        "\n",
        "1. **TOML DSL**: Defined the entire pipeline structure in a readable TOML format\n",
        "2. **Thread Pools**: Used separate pools for workers (CPU-bound) and I/O operations\n",
        "3. **Rate Limiting**: Controlled the Loader node's throughput\n",
        "4. **Error Handling**: Validation errors routed to a dedicated error handler\n",
        "5. **Retries**: Transformer configured with automatic retries on failure\n",
        "6. **Data Types**: Used dataclasses with type annotations\n",
        "7. **History**: Tracked all operations for debugging\n",
        "8. **Checkpointing**: Saved pipeline definition for portability\n",
        "\n",
        "Key patterns demonstrated:\n",
        "- Separation of valid/invalid data flows\n",
        "- Parallel processing with thread pools\n",
        "- Configuration-driven pipeline design\n",
        "- Graceful error handling without pipeline failure\n",
        "- ETL pipeline architecture"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "# Cleanup (optional - comment out to inspect files)\n",
        "import shutil\n",
        "# shutil.rmtree(temp_dir)\n",
        "print(f\"\\nTemp files available at: {temp_dir}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
