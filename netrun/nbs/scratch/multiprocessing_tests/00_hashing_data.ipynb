{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78210df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.11 environment at: /Users/lukastk/dev/20260113_w3pmcj__netrun2/netrun/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m3 packages\u001b[0m \u001b[2min 50ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install joblib numpy diskcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c70f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling numpy.square...\n",
      "square(array([[0., 0., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [4., 2., 1.]]))\n",
      "___________________________________________________________square - 0.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "# from joblib import Memory\n",
    "# location = 'your_cache_dir_goes_here'\n",
    "# mem = Memory(location, verbose=1)\n",
    "# import numpy as np\n",
    "# a = np.vander(np.arange(3)).astype(float)\n",
    "# square = mem.cache(np.square)\n",
    "# b = square(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694de2de",
   "metadata": {},
   "source": [
    "> The default diskcache.Disk serialization uses pickling for both keys and values. **Unfortunately, pickling produces inconsistencies sometimes when applied to container data types like tuples. Two equal tuples may serialize to different bytes objects using pickle.** The likelihood of differences is reduced by using pickletools.optimize but still inconsistencies occur (#54). The inconsistent serialized pickle values is particularly problematic when applied to the key in the cache. **Consider using an alternative Disk type, like JSONDisk, for consistent serialization of keys.**\n",
    "\n",
    "[Source](https://grantjenks.com/docs/diskcache/tutorial.html#caveats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ebc472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diskcache\n",
    "import tempfile\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "cache = diskcache.Cache(disk=diskcache.JSONDisk, disk_compress_level=6, directory=temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db3a6ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type MyModel is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m     bar: \u001b[38;5;28mdict\u001b[39m\n\u001b[32m      7\u001b[39m my_data = MyModel(foo=\u001b[33m'\u001b[39m\u001b[33mbar\u001b[39m\u001b[33m'\u001b[39m, bar={\u001b[33m'\u001b[39m\u001b[33mbaz\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mqux\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mquux\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mobject\u001b[39m()})\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmy_data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = my_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/20260113_w3pmcj__netrun2/netrun/.venv/lib/python3.11/site-packages/diskcache/core.py:823\u001b[39m, in \u001b[36mCache.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[32m    815\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Set corresponding `value` for `key` in cache.\u001b[39;00m\n\u001b[32m    816\u001b[39m \n\u001b[32m    817\u001b[39m \u001b[33;03m    :param key: key for item\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    821\u001b[39m \n\u001b[32m    822\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/20260113_w3pmcj__netrun2/netrun/.venv/lib/python3.11/site-packages/diskcache/core.py:772\u001b[39m, in \u001b[36mCache.set\u001b[39m\u001b[34m(self, key, value, expire, read, tag, retry)\u001b[39m\n\u001b[32m    770\u001b[39m db_key, raw = \u001b[38;5;28mself\u001b[39m._disk.put(key)\n\u001b[32m    771\u001b[39m expire_time = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m expire \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m now + expire\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m size, mode, filename, db_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_disk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    773\u001b[39m columns = (expire_time, tag, size, mode, filename, db_value)\n\u001b[32m    775\u001b[39m \u001b[38;5;66;03m# The order of SELECT, UPDATE, and INSERT is important below.\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    777\u001b[39m \u001b[38;5;66;03m# Typical cache usage pattern is:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    793\u001b[39m \u001b[38;5;66;03m# INSERT OR REPLACE aka UPSERT is not used because the old filename may\u001b[39;00m\n\u001b[32m    794\u001b[39m \u001b[38;5;66;03m# need cleanup.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/20260113_w3pmcj__netrun2/netrun/.venv/lib/python3.11/site-packages/diskcache/core.py:364\u001b[39m, in \u001b[36mJSONDisk.store\u001b[39m\u001b[34m(self, value, read, key)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstore\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, read, key=UNKNOWN):\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m read:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m         json_bytes = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m.encode(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    365\u001b[39m         value = zlib.compress(json_bytes, \u001b[38;5;28mself\u001b[39m.compress_level)\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().store(value, read, key=key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/json/__init__.py:231\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    228\u001b[39m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    230\u001b[39m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/json/encoder.py:200\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    202\u001b[39m     chunks = \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/json/encoder.py:258\u001b[39m, in \u001b[36mJSONEncoder.iterencode\u001b[39m\u001b[34m(self, o, _one_shot)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    254\u001b[39m     _iterencode = _make_iterencode(\n\u001b[32m    255\u001b[39m         markers, \u001b[38;5;28mself\u001b[39m.default, _encoder, \u001b[38;5;28mself\u001b[39m.indent, floatstr,\n\u001b[32m    256\u001b[39m         \u001b[38;5;28mself\u001b[39m.key_separator, \u001b[38;5;28mself\u001b[39m.item_separator, \u001b[38;5;28mself\u001b[39m.sort_keys,\n\u001b[32m    257\u001b[39m         \u001b[38;5;28mself\u001b[39m.skipkeys, _one_shot)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type MyModel is not JSON serializable"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    foo: str\n",
    "    bar: dict\n",
    "\n",
    "my_data = MyModel(foo='bar', bar={'baz': 'qux', 'quux': object()})\n",
    "\n",
    "cache['my_data'] = my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82256937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diskcache\n",
    "import tempfile\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "cache = diskcache.Cache(directory=temp_dir)\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    foo: str\n",
    "    bar: dict\n",
    "\n",
    "my_data = MyModel(foo='bar', bar={'baz': 'qux', 'quux': object()})\n",
    "\n",
    "cache['my_data'] = my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7db4f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[(my_data, MyModel)] = 123\n",
    "\n",
    "cache[(my_data, MyModel)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98afabf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.6 μs ± 468 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (my_data, MyModel) in cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8bf07ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_con',\n",
       " '_cull',\n",
       " '_iter',\n",
       " '_row_insert',\n",
       " '_row_update',\n",
       " '_select_delete',\n",
       " '_sql',\n",
       " '_sql_retry',\n",
       " '_transact',\n",
       " 'add',\n",
       " 'check',\n",
       " 'clear',\n",
       " 'close',\n",
       " 'create_tag_index',\n",
       " 'cull',\n",
       " 'decr',\n",
       " 'delete',\n",
       " 'directory',\n",
       " 'disk',\n",
       " 'drop_tag_index',\n",
       " 'evict',\n",
       " 'expire',\n",
       " 'get',\n",
       " 'incr',\n",
       " 'iterkeys',\n",
       " 'memoize',\n",
       " 'peek',\n",
       " 'peekitem',\n",
       " 'pop',\n",
       " 'pull',\n",
       " 'push',\n",
       " 'read',\n",
       " 'reset',\n",
       " 'set',\n",
       " 'stats',\n",
       " 'timeout',\n",
       " 'touch',\n",
       " 'transact',\n",
       " 'volume']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(diskcache.Cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "390fbf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diskcache import Index\n",
    "\n",
    "ind = Index()\n",
    "\n",
    "ind[(my_data, MyModel)] = 123\n",
    "\n",
    "ind[(my_data, MyModel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "485a7c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3745006623"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.disk.hash((my_data, MyModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "70392ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Disk.hash of <diskcache.core.Disk object at 0x113055c10>>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.disk.hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a24efc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diskcache.core.Cache"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diskcache.Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "93e7c952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asd'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disk_key, _ = cache.disk.put(\"asd\")\n",
    "disk_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5360e4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<memory at 0x1135b8580>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_disk = diskcache.JSONDisk(temp_dir)\n",
    "\n",
    "json_disk.put(\"asd\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "355aab6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asd'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.disk.put(\"asd\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b5ba92a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3745006623"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disk_key, _ = cache.disk.put((my_data, MyModel))\n",
    "\n",
    "import zlib\n",
    "mask = 0xFFFFFFFF\n",
    "zlib.adler32(disk_key) & mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "da710395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329057424"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import struct\n",
    "\n",
    "zlib.adler32(struct.pack('!d', 0.1)) & mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9fb4569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "def hash(self, key, disk: diskcache.Disk):\n",
    "    \"\"\"Compute portable hash for `key`.\n",
    "\n",
    "    :param key: key to hash\n",
    "    :return: hash value\n",
    "\n",
    "    \"\"\"\n",
    "    mask = 0xFFFFFFFF\n",
    "    disk_key, _ = disk.put(key)\n",
    "    type_disk_key = type(disk_key)\n",
    "\n",
    "    if type_disk_key is sqlite3.Binary:\n",
    "        return zlib.adler32(disk_key) & mask\n",
    "    elif type_disk_key is str:\n",
    "        return zlib.adler32(disk_key.encode('utf-8')) & mask  # noqa\n",
    "    elif type_disk_key is int:\n",
    "        return disk_key % mask\n",
    "    else:\n",
    "        assert type_disk_key is float\n",
    "        return zlib.adler32(struct.pack('!d', disk_key)) & mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
