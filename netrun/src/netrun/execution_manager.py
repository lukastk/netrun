# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/netrun/04_execution_manager.ipynb

__all__ = ['ExecutionManager', 'JobResult', 'PoolType', 'RunAllocationMethod', 'SubmittedJobInfo']

# %% nbs/netrun/04_execution_manager.ipynb 3
from contextlib import contextmanager
from typing import Any, Type
from collections.abc import Callable, Awaitable
import datetime
import builtins
from ._iutils import get_timestamp_utc
from datetime import datetime
import asyncio
from enum import Enum
from dataclasses import dataclass
import importlib
import uuid
import pickle
import random
import functools

from .rpc.base import RPCChannel
from .pool.thread import ThreadPool
from .pool.multiprocess import MultiprocessPool
from .pool.aio import SingleWorkerPool
from .pool.remote import RemotePoolClient

# %% nbs/netrun/04_execution_manager.ipynb 5
@contextmanager
def _override_builtins_print(func, new_print):
    g = func.__globals__
    old = g.get("print", builtins.print)
    g["print"] = new_print
    try:
        yield
    finally:
        # restore whatever was there before
        if old is builtins.print and "print" in g:
            # keep func globals clean if it didn't define print originally
            del g["print"]
        else:
            g["print"] = old

# %% nbs/netrun/04_execution_manager.ipynb 7
async def _async_func_runner(
    channel: RPCChannel,
    func: Callable[..., Awaitable[Any]],
    send_channel: bool,
    print_callback: Callable[[str], None],
    args: tuple,
    kwargs: dict,
) -> Any:
    with _override_builtins_print(func, print_callback):
        if asyncio.iscoroutinefunction(func):
            if send_channel:
                return await func(channel, *args, **kwargs)
            else:
                return await func(*args, **kwargs)
        else:
            if send_channel:
                return func(channel, *args, **kwargs)
            else:
                return func(*args, **kwargs)

# %% nbs/netrun/04_execution_manager.ipynb 8
def _func_runner(
    channel: RPCChannel,
    func: Callable[..., Any],
    send_channel: bool,
    print_callback: Callable[[str], None],
    args: tuple,
    kwargs: dict,
    event_loop: asyncio.AbstractEventLoop,
) -> Any:
    with _override_builtins_print(func, print_callback):
        if asyncio.iscoroutinefunction(func):
            if send_channel:
                return event_loop.run_until_complete(func(channel, *args, **kwargs))
            else:
                return event_loop.run_until_complete(func(*args, **kwargs))
        else:
            if send_channel:
                return func(channel, *args, **kwargs)
            else:
                return func(*args, **kwargs)

# %% nbs/netrun/04_execution_manager.ipynb 10
def _convert_to_str_if_not_serializable(obj: Any) -> tuple[bool, Any]:
    """Convert an object to string if it's not pickle-serializable.

    Returns:
        Tuple of (was_converted, result)
    """
    try:
        pickle.dumps(obj)
        return (False, obj)
    except (pickle.PicklingError, TypeError, AttributeError):
        return (True, str(obj))

# %% nbs/netrun/04_execution_manager.ipynb 12
class ExecutionManagerProtocolKeys(Enum):
    RUN = "exec-manager:run"
    """
    Run a function.
    Args: msg_id, func_import_path_or_key, run_id, send_channel, args, kwargs
    """

    UP_RUN_STARTED = "exec-manager-up:run-started"
    """
    Notification from the worker that a run has been submitted and started. 
    Args: msg_id, timestamp_utc_started
    """

    UP_RUN_RESPONSE = "exec-manager-up:run-response"
    """
    Response to RUN from the worker.
    Args: msg_id, converted_to_str, _res
    """

    SEND_FUNCTION = "exec-manager:send-function"
    """
    Used to send a function object to the worker, which can then be run using RUN.
    Args: msg_id, func_key, func
    """

    UP_SEND_FUNCTION_RESPONSE = "exec-manager-up:send-function-response"
    """
    Response to SEND_FUNCTION from the worker, to confirm that the function was received.
    Args: msg_id
    """

    UP_PRINT_BUFFER = "exec-manager-up:print-buffer"
    """
    Auto-flushed print buffer from the worker during function execution.
    Sent at regular intervals (print_flush_interval) and before UP_RUN_RESPONSE.
    Args: msg_id, run_id, _buffer
    """

# %% nbs/netrun/04_execution_manager.ipynb 13
def _worker_func(is_in_main_process: bool, channel, worker_id, print_flush_interval: float = 0.1):
    """Worker function that handles execution manager protocol messages.

    Args:
        is_in_main_process: If True, results don't need to be serializable.
        channel: RPC channel for communication.
        worker_id: ID of this worker.
        print_flush_interval: Interval in seconds between automatic print buffer flushes.
    """
    event_loop = asyncio.new_event_loop()
    registered_functions: dict[str, Callable[..., Awaitable] | Callable[..., None]] = {}

    while True:
        key, data = channel.recv()
        # RUN
        if key == ExecutionManagerProtocolKeys.RUN.value:
            msg_id, func_import_path_or_key, run_id, send_channel, args, kwargs = data
            if func_import_path_or_key in registered_functions:
                func = registered_functions[func_import_path_or_key]
            else:
                module_path, func_name = func_import_path_or_key.rsplit(".", 1)
                module = importlib.import_module(module_path)
                func = getattr(module, func_name)

            # Set up auto-flushing print buffer
            print_buffer: list[tuple[datetime, str]] = []
            last_flush_time = get_timestamp_utc()

            def print_callback(text: str):
                nonlocal last_flush_time
                print_buffer.append((get_timestamp_utc(), text))
                # Check if we should auto-flush
                now = get_timestamp_utc()
                elapsed = (now - last_flush_time).total_seconds()
                if elapsed >= print_flush_interval and print_buffer:
                    channel.send(ExecutionManagerProtocolKeys.UP_PRINT_BUFFER.value, (msg_id, run_id, list(print_buffer)))
                    print_buffer.clear()
                    last_flush_time = now

            timestamp_utc_started = get_timestamp_utc()
            channel.send(ExecutionManagerProtocolKeys.UP_RUN_STARTED.value, (msg_id, timestamp_utc_started))
            res = _func_runner(
                channel=channel,
                func=func,
                send_channel=send_channel,
                print_callback=print_callback,
                args=args,
                kwargs=kwargs,
                event_loop=event_loop,
            )
            timestamp_utc_completed = get_timestamp_utc()
            if is_in_main_process:
                converted_to_str, _res = False, res
            else:
                converted_to_str, _res = _convert_to_str_if_not_serializable(res)

            # Flush any remaining print buffer before sending response
            if print_buffer:
                channel.send(ExecutionManagerProtocolKeys.UP_PRINT_BUFFER.value, (msg_id, run_id, list(print_buffer)))

            channel.send(ExecutionManagerProtocolKeys.UP_RUN_RESPONSE.value, (msg_id, timestamp_utc_started, timestamp_utc_completed, converted_to_str, _res))
        # SEND_FUNCTION
        elif key == ExecutionManagerProtocolKeys.SEND_FUNCTION.value:
            msg_id, func_key, func = data
            registered_functions[func_key] = func
            channel.send(ExecutionManagerProtocolKeys.UP_SEND_FUNCTION_RESPONSE.value, (msg_id,))
        else:
            raise ValueError(f"Unknown execution manager protocol key: '{key}'.")

def _thread_worker_func(channel, worker_id, print_flush_interval: float = 0.1):
    return _worker_func(is_in_main_process=True, channel=channel, worker_id=worker_id, print_flush_interval=print_flush_interval)

# If the worker is in a multiprocess pool, then the result needs to be pickleable for it to be sent back without being converted as `str(result)`.
def _multiprocess_worker_func(channel, worker_id, print_flush_interval: float = 0.1):
    return _worker_func(is_in_main_process=False, channel=channel, worker_id=worker_id, print_flush_interval=print_flush_interval)

# %% nbs/netrun/04_execution_manager.ipynb 14
async def _async_worker_func(channel, worker_id, print_flush_interval: float = 0.1):
    """Async worker function that handles execution manager protocol messages.

    Note: For async workers, print buffer is sent all at once at the end of execution
    since the print callback cannot be async. Interval-based flushing is only
    supported for sync workers (thread/multiprocess pools).

    Args:
        channel: Async RPC channel for communication.
        worker_id: ID of this worker.
        print_flush_interval: Not used for async workers (kept for API consistency).
    """
    registered_functions: dict[str, Callable[..., Awaitable] | Callable[..., None]] = {}

    while True:
        key, data = await channel.recv()
        # RUN
        if key == ExecutionManagerProtocolKeys.RUN.value:
            msg_id, func_import_path_or_key, run_id, send_channel, args, kwargs = data
            if func_import_path_or_key in registered_functions:
                func = registered_functions[func_import_path_or_key]
            else:
                module_path, func_name = func_import_path_or_key.rsplit(".", 1)
                module = importlib.import_module(module_path)
                func = getattr(module, func_name)

            # For async workers, we just collect prints and send at the end
            # (interval-based flushing requires sync channel.send which isn't available)
            print_buffer: list[tuple[datetime, str]] = []

            timestamp_utc_started = get_timestamp_utc()
            await channel.send(ExecutionManagerProtocolKeys.UP_RUN_STARTED.value, (msg_id, timestamp_utc_started))
            res = await _async_func_runner(
                channel=channel,
                func=func,
                send_channel=send_channel,
                print_callback=lambda s: print_buffer.append((get_timestamp_utc(), s)),
                args=args,
                kwargs=kwargs,
            )
            timestamp_utc_completed = get_timestamp_utc()
            converted_to_str, _res = False, res

            # Send print buffer before response
            if print_buffer:
                await channel.send(ExecutionManagerProtocolKeys.UP_PRINT_BUFFER.value, (msg_id, run_id, list(print_buffer)))

            await channel.send(ExecutionManagerProtocolKeys.UP_RUN_RESPONSE.value, (msg_id, timestamp_utc_started, timestamp_utc_completed, converted_to_str, _res))
        # SEND_FUNCTION
        elif key == ExecutionManagerProtocolKeys.SEND_FUNCTION.value:
            msg_id, func_key, func = data
            registered_functions[func_key] = func
            await channel.send(ExecutionManagerProtocolKeys.UP_SEND_FUNCTION_RESPONSE.value, (msg_id,))
        else:
            raise ValueError(f"Unknown execution manager protocol key: '{key}'.")

# %% nbs/netrun/04_execution_manager.ipynb 16
@dataclass
class JobResult:
    """Result of a job execution."""
    timestamp_utc_submitted: datetime
    timestamp_utc_started: datetime
    timestamp_utc_completed: datetime
    func_import_path_or_key: str
    pool_id: str
    worker_id: int
    converted_to_str: bool
    result: Any
    print_buffer: list[tuple[datetime, str]]
    """List of (timestamp, text) tuples for each print statement captured during execution."""

@dataclass
class SubmittedJobInfo:
    """Information about a submitted job."""
    run_id: str
    timestamp_utc_submitted: datetime
    timestamp_utc_started: datetime | None
    func_import_path_or_key: str
    pool_id: str
    worker_id: int

class RunAllocationMethod(Enum):
    """Method for allocating a job to a worker."""
    ROUND_ROBIN = "round-robin"
    RANDOM = "random"
    LEAST_BUSY = "least-busy"

# %% nbs/netrun/04_execution_manager.ipynb 17
PoolType = ThreadPool | MultiprocessPool | SingleWorkerPool | RemotePoolClient

class ExecutionManager:
    def __init__(self, pool_configs: dict[Type[PoolType], tuple[str, dict[str, Any]]]):
        """
        Create an ExecutionManager with the given pool configurations.

        Args:
            pool_configs: A dictionary mapping pool_id to (pool_type, pool_init_kwargs).
                pool_type can be "thread", "multiprocess", "remote", or "main".
                pool_init_kwargs are passed to the pool constructor (excluding worker_fn).
        """
        self._pool_configs = pool_configs
        self._pools: dict[str, PoolType] = {}
        self._msg_recv_tasks: dict[str, asyncio.Task] = {}
        self._msgs: dict[str, dict[str, asyncio.Queue]] = {}
        self._started = False

        self._worker_jobs: dict[tuple[str, str], list[SubmittedJobInfo]] = {}  # (pool_id, worker_id) -> list of SubmittedJobInfo
        self._worker_round_robin_lst: list[tuple[str, str]] = []

    async def start(self) -> None:
        """Start all pools and initialize the execution manager."""
        if self._started:
            raise RuntimeError("ExecutionManager is already started.")

        for pool_id, (pool_type, pool_init_kwargs) in self._pool_configs.items():
            if 'worker_fn' in pool_init_kwargs:
                raise ValueError("The 'worker_fn' argument should not be specified in the pool config.")

            # Extract print_flush_interval from kwargs (default 0.1 = 100ms)
            pool_kwargs = dict(pool_init_kwargs)
            print_flush_interval = pool_kwargs.pop('print_flush_interval', 0.1)

            if pool_type == ThreadPool:
                worker_fn = functools.partial(_thread_worker_func, print_flush_interval=print_flush_interval)
                self._pools[pool_id] = ThreadPool(**pool_kwargs, worker_fn=worker_fn)
            elif pool_type == MultiprocessPool:
                worker_fn = functools.partial(_multiprocess_worker_func, print_flush_interval=print_flush_interval)
                self._pools[pool_id] = MultiprocessPool(**pool_kwargs, worker_fn=worker_fn)
            elif pool_type == RemotePoolClient:
                self._pools[pool_id] = RemotePoolClient(**pool_kwargs)
            elif pool_type == SingleWorkerPool:
                worker_fn = functools.partial(_async_worker_func, print_flush_interval=print_flush_interval)
                self._pools[pool_id] = SingleWorkerPool(**pool_kwargs, worker_fn=worker_fn)
            else:
                raise ValueError(f"Unknown pool type: '{pool_type}'.")

            self._msgs[pool_id] = {}

        # Start all pools
        for pool_id, pool in self._pools.items():
            await pool.start()

        # Initialize worker jobs tracking for each worker
        for pool_id, pool in self._pools.items():
            for worker_id in range(pool.num_workers):
                self._worker_jobs[(pool_id, worker_id)] = []

        # Start message receiver tasks after pools are started
        for pool_id in self._pools:
            self._msg_recv_tasks[pool_id] = asyncio.create_task(self._msg_recv_task_func(pool_id))

        self._started = True

    async def _msg_recv_task_func(self, pool_id: str):
        pool = self._pools[pool_id]
        while True:
            msg = await pool.recv()
            msg_id = msg.data[0]
            msg.data = msg.data[1:]
            await self._msgs[pool_id][msg_id].put(msg)

    async def _send_msg(self, pool_id: str, worker_id: str, key: str, data: Any) -> str:
        pool = self._pools[pool_id]
        msg_id = str(uuid.uuid4())

        # Check if the message receiver task for the pool is running, else propagate its exception
        msg_recv_task = self._msg_recv_tasks.get(pool_id)
        if msg_recv_task is not None and msg_recv_task.done():
            exc = msg_recv_task.exception()
            if exc is not None:
                raise exc

        # Create the queue BEFORE sending to avoid race condition where response
        # arrives before the queue is created
        self._msgs[pool_id][msg_id] = asyncio.Queue()

        await pool.send(
            worker_id=worker_id,
            key=key,
            data=(msg_id, *data),
        )

        return msg_id

    async def _recv_msg(self, pool_id: str, msg_id: str, expect: ExecutionManagerProtocolKeys, close_msg_queue: bool) -> tuple[str, Any]:
        # Check if the message receiver task for the pool is running, else propagate its exception
        msg_recv_task = self._msg_recv_tasks.get(pool_id)
        if msg_recv_task is not None and msg_recv_task.done():
            exc = msg_recv_task.exception()
            if exc is not None:
                raise exc

        msg = await self._msgs[pool_id][msg_id].get()
        if close_msg_queue:
            del self._msgs[pool_id][msg_id]

        if msg.key != expect.value:
            raise ValueError(f"Expected message key '{expect.value}', got '{msg.key}'.")

        return msg

    async def run(
        self,
        pool_id: str,
        worker_id: str,
        func_import_path_or_key: str,
        send_channel: bool,
        func_args,
        func_kwargs,
        on_print: Callable[[list[tuple[datetime, str]]], None] | None = None,
    ) -> JobResult:
        """
        Run a function in a pool.

        Args:
            pool_id: The ID of the pool to run the function in.
            worker_id: The ID of the worker to run the function on.
            func_import_path_or_key: The import path or key of the function to run (for the latter, use send_function to register the function first)
            send_channel: Whether to send the worker RPC channel to the function.
            func_args: The arguments to pass to the function.
            func_kwargs: The keyword arguments to pass to the function.
            on_print: Optional callback called when print output is received from the worker.
                Called with a list of (timestamp, text) tuples. Called multiple times during
                execution as print buffers are flushed (at print_flush_interval).

        Returns:
            The result of the function.
        """
        pool = self._pools[pool_id]

        run_id = str(uuid.uuid4())
        timestamp_utc_submitted = get_timestamp_utc()
        msg_id = await self._send_msg(
            pool_id=pool_id,
            worker_id=worker_id,
            key=ExecutionManagerProtocolKeys.RUN.value,
            data=(func_import_path_or_key, run_id, send_channel, func_args, func_kwargs),
        )
        job_info = SubmittedJobInfo(
            run_id=run_id,
            timestamp_utc_submitted=timestamp_utc_submitted,
            timestamp_utc_started=None,
            func_import_path_or_key=func_import_path_or_key,
            pool_id=pool_id,
            worker_id=worker_id,
        )
        self._worker_jobs[(pool_id, worker_id)].append(job_info)
        if (pool_id, worker_id) in self._worker_round_robin_lst:
            self._worker_round_robin_lst.remove((pool_id, worker_id))
        self._worker_round_robin_lst.append((pool_id, worker_id))

        # Wait for UP_RUN_STARTED
        started_msg = await self._recv_msg(pool_id, msg_id, expect=ExecutionManagerProtocolKeys.UP_RUN_STARTED, close_msg_queue=False)
        job_info.timestamp_utc_started = started_msg.data[0]  # timestamp_utc_started

        # Accumulate print buffers and wait for UP_RUN_RESPONSE
        accumulated_print_buffer: list[tuple[datetime, str]] = []

        while True:
            # Check if the message receiver task is still running
            msg_recv_task = self._msg_recv_tasks.get(pool_id)
            if msg_recv_task is not None and msg_recv_task.done():
                exc = msg_recv_task.exception()
                if exc is not None:
                    raise exc

            msg = await self._msgs[pool_id][msg_id].get()

            if msg.key == ExecutionManagerProtocolKeys.UP_PRINT_BUFFER.value:
                # Intermediate print buffer
                _run_id, _buffer = msg.data
                accumulated_print_buffer.extend(_buffer)
                if on_print is not None:
                    on_print(_buffer)
            elif msg.key == ExecutionManagerProtocolKeys.UP_RUN_RESPONSE.value:
                # Job completed - clean up and break
                del self._msgs[pool_id][msg_id]
                break
            else:
                raise ValueError(f"Unexpected message key: '{msg.key}'")

        self._worker_jobs[(pool_id, worker_id)].remove(job_info)

        timestamp_utc_started, timestamp_utc_completed, converted_to_str, _res = msg.data
        return JobResult(
            timestamp_utc_submitted=job_info.timestamp_utc_submitted,
            timestamp_utc_started=job_info.timestamp_utc_started,
            timestamp_utc_completed=timestamp_utc_completed,
            func_import_path_or_key=job_info.func_import_path_or_key,
            pool_id=job_info.pool_id,
            worker_id=job_info.worker_id,
            converted_to_str=converted_to_str,
            result=_res,
            print_buffer=accumulated_print_buffer,
        )

    async def run_allocate(
        self,
        pool_worker_ids: list[str | tuple[str, str]],
        allocation_method: RunAllocationMethod,
        func_import_path_or_key: str,
        send_channel: bool,
        func_args,
        func_kwargs,
        on_print: Callable[[list[tuple[datetime, str]]], None] | None = None,
    ) -> JobResult:
        worker_ids: list[tuple[str, int]] = []
        # Convert pool_worker_ids to a list of (pool_id, worker_id) tuples
        for _id in pool_worker_ids:
            if isinstance(_id, str):
                pool = self._pools[_id]
                for worker_id in range(pool.num_workers):
                    worker_ids.append((_id, worker_id))
            else:
                pool_id, worker_id = _id
                worker_ids.append((pool_id, worker_id))

        if not worker_ids:
            raise ValueError("No workers available for allocation.")

        # Select worker based on allocation method
        if allocation_method == RunAllocationMethod.ROUND_ROBIN:
            round_robin_lst = [p for p in self._worker_round_robin_lst if p in worker_ids]
            not_in_round_robin_lst = [p for p in worker_ids if p not in round_robin_lst]
            if not_in_round_robin_lst:
                pool_id, worker_id = not_in_round_robin_lst[0]
            else:
                pool_id, worker_id = round_robin_lst[0]

        elif allocation_method == RunAllocationMethod.RANDOM:
            pool_id, worker_id = random.choice(worker_ids)

        elif allocation_method == RunAllocationMethod.LEAST_BUSY:
            # Choose the worker with the fewest active jobs (jobs that have been submitted
            # but not yet completed). A worker with no recorded jobs is preferred.
            def active_job_count(pool_worker: tuple[str, str]) -> int:
                key = pool_worker
                jobs = self._worker_jobs.get(key, [])
                # Active = submitted but not yet completed; here we treat presence in the
                # list as active, and completion will remove the job from this structure.
                return len(jobs)

            pool_id, worker_id = min(worker_ids, key=active_job_count)

        else:
            raise ValueError(f"Unknown allocation method: '{allocation_method}'.")

        return await self.run(
            pool_id=pool_id,
            worker_id=worker_id,
            func_import_path_or_key=func_import_path_or_key,
            send_channel=send_channel,
            func_args=func_args,
            func_kwargs=func_kwargs,
            on_print=on_print,
        )

    async def send_function(self, pool_id: str, worker_id: str, func_key: str, func: Callable[..., Any]|Callable[..., Awaitable[Any]]) -> None:
        """
        Send a function to a worker in a pool, such that it can be run using the given key using `ExecutionManager.run`.

        Args:
            pool_id: The ID of the pool to send the function to.
            worker_id: The ID of the worker to send the function to.
            func_key: The key of the function to send. 
            func: The function to send.
        """
        # If the pool is a multiprocess pool or remote pool, the function needs to be pickleable.
        pool = self._pools[pool_id]
        if isinstance(pool, (MultiprocessPool, RemotePoolClient)):
            try:
                pickle.dumps(func)
            except pickle.PicklingError:
                raise ValueError(f"Function {func} (key = '{func_key}') is not pickleable. Cannot send to worker in pool '{pool_id}'.")

        msg_id = await self._send_msg(
            pool_id=pool_id,
            worker_id=worker_id,
            key=ExecutionManagerProtocolKeys.SEND_FUNCTION.value,
            data=(func_key, func),
        )
        await self._recv_msg(pool_id, msg_id, expect=ExecutionManagerProtocolKeys.UP_SEND_FUNCTION_RESPONSE, close_msg_queue=True)

    async def send_function_to_pool(self, pool_id: str, func_key: str, func: Callable[..., Any]|Callable[..., Awaitable[Any]]) -> None:
        """
        Send a function to all workers in a pool.

        Args:
            pool_id: The ID of the pool to send the function to.
            func_key: The key of the function to send.
            func: The function to send.
        """
        tasks = [asyncio.create_task(self.send_function(pool_id, worker_id, func_key, func)) for worker_id in range(self._pools[pool_id].num_workers)]
        await asyncio.gather(*tasks)

    async def close(self):
        """Close the execution manager and all its pools."""
        # Cancel message receiver tasks FIRST to prevent them from trying to
        # recv from closed pools (which would raise PoolNotStarted)
        errors = []
        for task in self._msg_recv_tasks.values():
            task.cancel()
        for task in self._msg_recv_tasks.values():
            try:
                await task
            except asyncio.CancelledError:
                pass
            except Exception as e:
                errors.append(e)

        # Now close the pools
        for pool in self._pools.values():
            await pool.close()

        self._started = False

        # Propagate any errors from the recv tasks
        if errors:
            raise errors[0]

    @property
    def pools(self) -> list[tuple[str, Type[PoolType]]]:
        """Get list of pool IDs."""
        return [(k, type(v)) for k, v in self._pools.items()]

    def get_num_workers(self, pool_id: str) -> int:
        """Get the number of workers in a pool."""
        return self._pools[pool_id].num_workers

    def get_worker_ids(self, pool_id: str) -> list[str]:
        """Get the list of worker IDs for a pool."""
        return [f"{pool_id}_{i}" for i in range(self._pools[pool_id].num_workers)]

    def get_worker_jobs(self, pool_id: str, worker_id: int) -> list[SubmittedJobInfo]:
        """Get the list of currently submitted jobs for a worker."""
        return list(self._worker_jobs.get((pool_id, worker_id), []))

    async def __aenter__(self) -> "ExecutionManager":
        """Context manager entry - starts the manager."""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """Context manager exit - closes the manager."""
        await self.close()
